[
    {
        "model": "symposion_proposals.proposalsection",
        "pk": 1,
        "fields": {
            "section": 2,
            "start": "2018-05-12T02:43:08Z",
            "end": "2018-05-19T02:43:09Z",
            "closed": false,
            "published": false
        }
    },
    {
        "model": "symposion_proposals.proposalkind",
        "pk": 1,
        "fields": {
            "section": 2,
            "name": "talk",
            "slug": "talk"
        }
    },
    {
        "model": "symposion_proposals.proposalkind",
        "pk": 2,
        "fields": {
            "section": 1,
            "name": "tutorial",
            "slug": "tutorial"
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 4,
        "fields": {
            "kind": 1,
            "title": "defeating sauron with python",
            "description": "we must destroy the dark lord",
            "abstract": "blah",
            "abstract_html": "<p>blah</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-02-19T04:31:33.060Z",
            "speaker": 5,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 7,
        "fields": {
            "kind": 1,
            "title": "How to instantly publish data to the internet with Datasette",
            "description": "Datasette is a new tool for publishing structured data to the internet, as both a browseable web interface and a flexible JSON API. Aimed at newspapers, cultural institutions and local governments, Datasette makes it easy to publish all kinds of data in a much more powerful format than traditional CSVs. This talk will teach you how to use Datasette and explore the philosophy behind the project.",
            "abstract": "https://github.com/simonw/datasette\r\n\r\nDatasette provides an instant, read-only JSON API for any SQLite database. It also provides tools for packaging the database up as a Docker container and instantly deploying that container to a hosting provider.\r\n\r\nThis makes it a powerful tool for sharing interesting data online, in a way that allows users to both explore that data themselves and bulid their own interpretations of the data using the Datasette JSON API.\r\n\r\nIn this session I'll show you how to use Datasette to publish data, and illustrate examples of the exciting things people have already built using the tool. I'll show how Datasette's JSON API can be used to quickly build custom visualizations like https://sf-tree-search.now.sh/\r\n\r\nI'll also talk about the philosophy and design behind Datasette, including how immutable SQLite databases make for a surprisingly scalable solution for serving data on the internet.\r\n\r\nDatasette is built using Python 3 asyncio, and I'll also be discussing some of the asyncio patterns I used to create the tool.",
            "abstract_html": "<p>https://github.com/simonw/datasette</p>\n<p>Datasette provides an instant, read-only JSON API for any SQLite database. It also provides tools for packaging the database up as a Docker container and instantly deploying that container to a hosting provider.</p>\n<p>This makes it a powerful tool for sharing interesting data online, in a way that allows users to both explore that data themselves and bulid their own interpretations of the data using the Datasette JSON API.</p>\n<p>In this session I'll show you how to use Datasette to publish data, and illustrate examples of the exciting things people have already built using the tool. I'll show how Datasette's JSON API can be used to quickly build custom visualizations like https://sf-tree-search.now.sh/</p>\n<p>I'll also talk about the philosophy and design behind Datasette, including how immutable SQLite databases make for a surprisingly scalable solution for serving data on the internet.</p>\n<p>Datasette is built using Python 3 asyncio, and I'll also be discussing some of the asyncio patterns I used to create the tool.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-01T22:14:38.807Z",
            "speaker": 9,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 8,
        "fields": {
            "kind": 1,
            "title": "Make the most out of static typing",
            "description": "While static type checkers like mypy are great for producing more reliable code, subtle tweaks to how we model our data can make static typing far more useful and effective at preventing bugs.",
            "abstract": "This talk dives into some techniques that can be used to make the most out of static typing in Python. It covers some of the subtle tweaks to how we model our data that can improve code quality and reliability, and also touches on some slightly more advanced uses of mypy.",
            "abstract_html": "<p>This talk dives into some techniques that can be used to make the most out of static typing in Python. It covers some of the subtle tweaks to how we model our data that can improve code quality and reliability, and also touches on some slightly more advanced uses of mypy.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-06T05:57:02.167Z",
            "speaker": 10,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 9,
        "fields": {
            "kind": 1,
            "title": "Robot Nitpicks",
            "description": "Ever had a code review that was nothing more than nitpicking?\r\nEver gave one?\r\nNitpicks do not foster good relationships,\r\nbut code consistency is good to have.\r\nLet the robots nitpick for you,\r\nand have interesting discussions about code instead!",
            "abstract": "Nitpicking in code review is often as annoying for the reviewer as it is for the submitter, and tends to cause social friction. Using the modern Python static analysis toolbox -- PyLint, flake8, MyPy and more -- it is possible to leave the nitpicks to automated processes.\r\n\r\nThis frees up human reviewer time for important work, like checking that the code is clear and effective -- while letting the nitpicks not just *be* objective and impartial, but be so in a verifiable way.",
            "abstract_html": "<p>Nitpicking in code review is often as annoying for the reviewer as it is for the submitter, and tends to cause social friction. Using the modern Python static analysis toolbox -- PyLint, flake8, MyPy and more -- it is possible to leave the nitpicks to automated processes.</p>\n<p>This frees up human reviewer time for important work, like checking that the code is clear and effective -- while letting the nitpicks not just <em>be</em> objective and impartial, but be so in a verifiable way.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-10T05:10:22.862Z",
            "speaker": 11,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 10,
        "fields": {
            "kind": 1,
            "title": "Python as a DSL",
            "description": "Python is a great general programming language --\r\nbut it is also a great Domain Specific Language (DSL).\r\nAt least, many systems use Python as their DSL.\r\nWhy?\r\nHow to do it well?\r\nHow to avoid doing it badly?\r\nCome find out!",
            "abstract": "Python Domain Specific Languages are useful in many places. They have been used for build configuration languages (SCons, Pants), Database schema description (Django ORM) and Application configuration (Twisted .tac files).\r\n\r\nSome are universally loved, others universally reviled, and yet others have been met with mixed receptions. The talk will cover the salient features that make Python DSLs good or bad, and how to make a good DSL for your projects.",
            "abstract_html": "<p>Python Domain Specific Languages are useful in many places. They have been used for build configuration languages (SCons, Pants), Database schema description (Django ORM) and Application configuration (Twisted .tac files).</p>\n<p>Some are universally loved, others universally reviled, and yet others have been met with mixed receptions. The talk will cover the salient features that make Python DSLs good or bad, and how to make a good DSL for your projects.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-10T05:32:11.030Z",
            "speaker": 11,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 11,
        "fields": {
            "kind": 1,
            "title": "Dependency Injection and You",
            "description": "Dependency injection, or inversion of control, was super-hot at the\r\nturn of the millenium.\r\nNow that the topic has cooled off, it is time to revisit it:\r\nis it as good as we thought back then?\r\nShould we avoid completely?\r\nOr can we find a good middle ground?",
            "abstract": "Python has several libraries that do dependency injections. From near-copies of Java-based libraries to extremely light-weight ones, the landscape is without obvious winners -- and the default is to not use dependency injection at all.\r\n\r\nThe talk will explore the existing options, the trade-offs between them, and what kind of problems are useful to solve with dependency injection.",
            "abstract_html": "<p>Python has several libraries that do dependency injections. From near-copies of Java-based libraries to extremely light-weight ones, the landscape is without obvious winners -- and the default is to not use dependency injection at all.</p>\n<p>The talk will explore the existing options, the trade-offs between them, and what kind of problems are useful to solve with dependency injection.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-10T05:39:56.573Z",
            "speaker": 11,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 12,
        "fields": {
            "kind": 1,
            "title": "Creating native python extensions with rust language",
            "description": "What is Rust language, what makes rust unique. How to combine python and rust in one project and get benefits from both.",
            "abstract": "Rust is one of the most interesting new language in the industry. It provide memory safety without using garbage collector. Rust is statically type language, compiled to machine code. It offers high speed, safety and allows to write high level code. It very well suited for native extension development and in general is good for c language replacement.",
            "abstract_html": "<p>Rust is one of the most interesting new language in the industry. It provide memory safety without using garbage collector. Rust is statically type language, compiled to machine code. It offers high speed, safety and allows to write high level code. It very well suited for native extension development and in general is good for c language replacement.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-13T00:48:38.867Z",
            "speaker": 12,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 13,
        "fields": {
            "kind": 1,
            "title": "pysecops - a python security tool for devops",
            "description": "A cornerstone of devops is continuous monitoring. While good tools exist for server & application performance monitoring. There is no good open source tool that provides continuous package security updates as your server and codebase change. Until now.\r\n\r\nTalk goes over design decisions and implementation of the tool including hangups and pitfalls encountered along the way.",
            "abstract": "A cornerstone of devops is continuous monitoring. While good tools exist for server & application performance monitoring. There is no good open source tool that provides continuous package security updates as your server and codebase change. Until now.\r\n\r\nTalk goes over design decisions and implementation of the tool including hangups and pitfalls encountered along the way.",
            "abstract_html": "<p>A cornerstone of devops is continuous monitoring. While good tools exist for server &amp; application performance monitoring. There is no good open source tool that provides continuous package security updates as your server and codebase change. Until now.</p>\n<p>Talk goes over design decisions and implementation of the tool including hangups and pitfalls encountered along the way.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-15T04:55:52.435Z",
            "speaker": 13,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 14,
        "fields": {
            "kind": 1,
            "title": "Interaction between SQL Alchemy and Flask",
            "description": "Discussion of both SQL Alchemy and Flask. In this talk, I will create a few scenarios using SQL Alchemy's CORE and other component with Flask, and what these interactions mean for creating databases for web frameworks.",
            "abstract": "Discovery of SQL Alchemy CORE and ORM, and how these two components of SQL alchemy serve as a server for web frameworks, in particular Flask.",
            "abstract_html": "<p>Discovery of SQL Alchemy CORE and ORM, and how these two components of SQL alchemy serve as a server for web frameworks, in particular Flask.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-15T05:31:28.858Z",
            "speaker": 14,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 15,
        "fields": {
            "kind": 1,
            "title": "How to create REST API using AsyncIO library",
            "description": "Build a rest API for python3 application using aiohttp from the AsyncIO library which is an asynchronous http client/server framework.",
            "abstract": "aiohttp is HTTP client/server for python and AsyncIO. It supports both server websockets and client websockets. \r\nThe key part of the aiohttp framework is that it works in an asynchronous manner, it can concurrently handle hundreds of requests per second without too much hassle. In comparison to frameworks such as flask, it\u2019s incredibly performant.",
            "abstract_html": "<p>aiohttp is HTTP client/server for python and AsyncIO. It supports both server websockets and client websockets. \nThe key part of the aiohttp framework is that it works in an asynchronous manner, it can concurrently handle hundreds of requests per second without too much hassle. In comparison to frameworks such as flask, it\u2019s incredibly performant.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-15T08:13:32.439Z",
            "speaker": 15,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 16,
        "fields": {
            "kind": 1,
            "title": "Deploying Python3 application to Kubernetes using  Envoy",
            "description": "This talk will briefly describe how to deploy Python3 FlaskApp in Kubernetes using Envoy to handle round-robining traffic between several Flask apps",
            "abstract": "Microservices are an architectural style in which multiple, independent processes communicate with each other. Organizations are adopting microservices to improve agility and development velocity. \r\nIt\u2019s a pretty promising way to add a lot of flexibility without a lot of pain using tools like Envoy for brokering communications between the various parts of the application.",
            "abstract_html": "<p>Microservices are an architectural style in which multiple, independent processes communicate with each other. Organizations are adopting microservices to improve agility and development velocity. \nIt\u2019s a pretty promising way to add a lot of flexibility without a lot of pain using tools like Envoy for brokering communications between the various parts of the application.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-15T08:32:11.467Z",
            "speaker": 15,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 17,
        "fields": {
            "kind": 1,
            "title": "Nim for Python Programmers",
            "description": "Python is widely lauded for its productivity, minimalistic syntax, standard library feature set. However, there are some areas like speed, distribution, and multicore processing where it lacks a good solution. Nim is a statically typed and high-performance garbage-collected language which builds upon Python\u2019s strengths and addresses someone its weakness in an innovative way.",
            "abstract": "Ever wondered if there existed a language as expressive as Python and as efficient as C/C++? Look no further then. Nim is a statically typed, compiled language with a focus on efficiency. It is versatile and borrows much of its constructs and standard library design from Python https://nim-lang.org",
            "abstract_html": "<p>Ever wondered if there existed a language as expressive as Python and as efficient as C/C++? Look no further then. Nim is a statically typed, compiled language with a focus on efficiency. It is versatile and borrows much of its constructs and standard library design from Python https://nim-lang.org</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-15T17:47:13.718Z",
            "speaker": 16,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 18,
        "fields": {
            "kind": 1,
            "title": "Building Google Assistant Apps with Python",
            "description": "The Google Assistant platform is already available on a majority of Android smart phones and accessible also on iOS and other devices like the Google Home. This talk will demonstrate the process of developing a Google Assistant app using Python. Lastly, the talk will showcase a variety of Python libraries you can integrate to help with natural language processing in your voice app.",
            "abstract": "Google Assistant is a new voice platform that allows you to build apps that are only a spoken keyword away on a majority of smart phones and other devices. This talk will introduce you to Google Assistant apps and how to build them with Python.",
            "abstract_html": "<p>Google Assistant is a new voice platform that allows you to build apps that are only a spoken keyword away on a majority of smart phones and other devices. This talk will introduce you to Google Assistant apps and how to build them with Python.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-15T20:53:31.945Z",
            "speaker": 17,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 19,
        "fields": {
            "kind": 1,
            "title": "Kitchen Nightmares as a Model for Dysfunctional Development Teams",
            "description": "There are many similarities around how a restaurant functions and how a software team functions. The show Kitchen Nightmares showcased dysfunctional restaurants and the techniques chef Gordon Ramsay used to fix them. This talk will show some common pitfalls of software teams and use examples from Kitchen Nightmares to explain how we can remedy these situations.",
            "abstract": "The TV show Kitchen Nightmares showcased dysfunctional restaurants and the process chef Gordon Ramsay used to fix them. This talk will show how we can take the lessons learned from Kitchen Nightmares and apply them to our own dysfunctional software teams.",
            "abstract_html": "<p>The TV show Kitchen Nightmares showcased dysfunctional restaurants and the process chef Gordon Ramsay used to fix them. This talk will show how we can take the lessons learned from Kitchen Nightmares and apply them to our own dysfunctional software teams.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-15T20:59:52.446Z",
            "speaker": 17,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 20,
        "fields": {
            "kind": 1,
            "title": "Embedding Python in a High Performance Event Processing System",
            "description": "We built a high performance event processing system in a language called Pony, then we built an application development API in Python. Getting Pony and Python to work together has been an exciting challenge and we've learned a lot. This talk will cover some of what we've learned, from running an embedded interpreter across multiple threads to making two GC systems work together.",
            "abstract": "This talk is based on my experience building a Python API for a high performance stream processing system that was written in Pony. Getting the two languages to work together was at times challenging, and along the way we ran across a number of problems that were not covered in any of the documentation that we found on the internet. I will talk about what we learned and how we solved these problems, focusing specifically on:\r\n* calling the Python interpreter from different threads\r\n* handling errors when calling Python from C\r\n* managing Python objects outside of Python\r\n* building a Pythonic API",
            "abstract_html": "<p>This talk is based on my experience building a Python API for a high performance stream processing system that was written in Pony. Getting the two languages to work together was at times challenging, and along the way we ran across a number of problems that were not covered in any of the documentation that we found on the internet. I will talk about what we learned and how we solved these problems, focusing specifically on:\n<em> calling the Python interpreter from different threads\n</em> handling errors when calling Python from C\n<em> managing Python objects outside of Python\n</em> building a Pythonic API</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-15T21:46:51.341Z",
            "speaker": 18,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 21,
        "fields": {
            "kind": 1,
            "title": "Probabilistic programming from scratch",
            "description": "Bayesian inference is a mathematical framework for learning from real-world data and probabilistic programming makes it practical. Together we\u2019ll develop a lightweight, pure Python probabilistic programming system from scratch, and we\u2019ll use it to solve real data problems. We\u2019ll also look at how we\u2019d solve those problems with PyMC3.",
            "abstract": "This talk is for anyone who deals with real world data. Such data is always incomplete or imperfect in some way. Bayesian inference is a framework that allows us to draw conclusion from that data. And despite a reputation for mathematical and computational complexity, you don\u2019t need a statistics background to understand Bayes at a conceptual level. \r\n\r\nWe\u2019ll develop that understanding by building a lightweight, pure Python probabilistic programming system from scratch. Our library will use the simple \u201cApproximate Bayesian Computation\u201d algorithm, and we\u2019ll use the code we write to solve two real data problems (an A/B test and the German Tank problem). We\u2019ll also look at how we\u2019d solve those problems with PyMC3, a much more powerful, fully-featured probabilistic programming system.",
            "abstract_html": "<p>This talk is for anyone who deals with real world data. Such data is always incomplete or imperfect in some way. Bayesian inference is a framework that allows us to draw conclusion from that data. And despite a reputation for mathematical and computational complexity, you don\u2019t need a statistics background to understand Bayes at a conceptual level. </p>\n<p>We\u2019ll develop that understanding by building a lightweight, pure Python probabilistic programming system from scratch. Our library will use the simple \u201cApproximate Bayesian Computation\u201d algorithm, and we\u2019ll use the code we write to solve two real data problems (an A/B test and the German Tank problem). We\u2019ll also look at how we\u2019d solve those problems with PyMC3, a much more powerful, fully-featured probabilistic programming system.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-18T04:17:02.106Z",
            "speaker": 19,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 22,
        "fields": {
            "kind": 1,
            "title": "Serverless for data scientists",
            "description": "Working in the cloud means you don\u2019t have to deal with hardware. The goal of \"serverless\" is to also avoid dealing with operating systems. It offers instances that run for the duration of a single function call. These instances have limitations, but a lot of what data scientists do is a perfect fit for this new world! That\u2019s what this talk is about.",
            "abstract": "In this talk we'll first see the basic idea behind serverless and learn how to deploy a very simple web application to AWS Lambda using Zappa. We'll then look in detail at the \"embarrassingly parallel\" problems where serverless really shines for data scientists. In particular we'll take a look at PyWren, an ultra-lightweight alternative to heavy big data distributed systems such as Spark. We'll learn how PyWren uses AWS Lambda as its computational backend to churn through huge analytics tasks. PyWren opens up big data to mere mortal data scientists who don't have the budget or engineering support for a long-lived cluster.",
            "abstract_html": "<p>In this talk we'll first see the basic idea behind serverless and learn how to deploy a very simple web application to AWS Lambda using Zappa. We'll then look in detail at the \"embarrassingly parallel\" problems where serverless really shines for data scientists. In particular we'll take a look at PyWren, an ultra-lightweight alternative to heavy big data distributed systems such as Spark. We'll learn how PyWren uses AWS Lambda as its computational backend to churn through huge analytics tasks. PyWren opens up big data to mere mortal data scientists who don't have the budget or engineering support for a long-lived cluster.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-18T04:31:16.571Z",
            "speaker": 19,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 23,
        "fields": {
            "kind": 1,
            "title": "Robots, Biology and Unsupervised Model Selection",
            "description": "Zymergen combines robotics, software, and biology to improve microbial strains predictably and reliably. Robots can perform hundreds of experiments in parallel, and our analytical automation cleans and processes those data in near real-time. I present an end-to-end approach, in Python, for model selection, with an emphasis on parameter tuning, for unsupervised outlier detection algorithms.",
            "abstract": "At Zymergen we integrate robotics, software and biology to provide predictability and reliability to the process of rapidly improving microbial strains through genetic engineering.  One critical part of this process is rapid, robust and useful processing of data to provide scientists with the information they need to make the next round of changes and decide which strains to promote.   Robots can perform hundreds of experiments in parallel, and our analytical automation cleans and processes those data in near real-time.  A first step is to identify outliers that arise in the data due to multiple opportunities for process failure, and with this comes the challenges of modeling outliers, selecting a model, and tuning parameters for these models.  In Robots, Biology and Unsupervised Model Selection, I present an end-to-end approach, in python, for parameter tuning unsupervised outlier detection algorithms.  This problem is well studied for supervised and even semi-supervised (labels are human evaluation) anomaly and outlier detection algorithms, but there are few resources readily available when it comes to unsupervised algorithms in this arena.",
            "abstract_html": "<p>At Zymergen we integrate robotics, software and biology to provide predictability and reliability to the process of rapidly improving microbial strains through genetic engineering.  One critical part of this process is rapid, robust and useful processing of data to provide scientists with the information they need to make the next round of changes and decide which strains to promote.   Robots can perform hundreds of experiments in parallel, and our analytical automation cleans and processes those data in near real-time.  A first step is to identify outliers that arise in the data due to multiple opportunities for process failure, and with this comes the challenges of modeling outliers, selecting a model, and tuning parameters for these models.  In Robots, Biology and Unsupervised Model Selection, I present an end-to-end approach, in python, for parameter tuning unsupervised outlier detection algorithms.  This problem is well studied for supervised and even semi-supervised (labels are human evaluation) anomaly and outlier detection algorithms, but there are few resources readily available when it comes to unsupervised algorithms in this arena.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-19T22:32:43.763Z",
            "speaker": 20,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 24,
        "fields": {
            "kind": 1,
            "title": "Designing fast and scalable Python MicroServices with django",
            "description": "django, combined with the django REST framework, makes it very easy to build RESTful MicroServices. However, django is perceived to have some overhead making it hard to build fast and scalable MicroServices. This talk shows you how to squeeze the last bit of performance from django.",
            "abstract": "1. Understanding django\u2019s request processing lifecycle (3 minutes) ==========================================================\r\nI will start off by showing the django request processing lifecycle - starting with the WSGI handler to executing Request & Response middlewares emphasizing that all of them are run serially for the request. Understanding django\u2019s request processing lifecycle helps gain key insights. For example blocking the request for a long time inside a middleware function or the API view function will slow down the requests and will negatively impact request throughput. Using too many middleware classes or performing blocking IO or function calls in the request handler should be avoided. \r\n\r\n2. The django ORM layer (5 minutes) \r\n================================ \r\ndjango\u2019s ORM layer is one of it\u2019s greatest strengths. django restframework makes it very easy to expose SQL tables as RESTful resources to clients. However, there are many common mistake that should be avoided. Relying too much on the ORM to run optimal queries is one big issue. In this section, we\u2019ll go over optimizing database access through django\u2019s ORM specifically we\u2019ll cover adding indexes, prefetching or selecting related entities using prefetch_related & select_related, reusing querysets as well as defining custom query sets. Finally, we\u2019ll also see how to discover costs associated with SQL queries using the EXPLAIN statement. \r\n\r\n3. Pagination & Filtering (5 minutes)\r\n================================ \r\nA common and fatal mistake is to pull excessive amount of data (records) from the server. This is usually mitigated by adding pagination and filtering. In this mechanism, the client provides a predicate (e.g. `city=SFO&start=20&pagesize=10`) that flows all the way to the database as part of the SQL query (WHERE `city`=\u201dSFO\u201d LIMIT 10 OFFSET 20). This helps the SQL query planner eliminate unnecessary records. Pagination establishes an upper bound on the amount of data being requested as well the time that the client needs to wait in order to receive a response. The upside of pagination is the client can make multiple concurrent requests to the server. This can reduce the overall time it takes to fetch the entire dataset since each request may be served by different server instances. django\u2019s built-in pagination mechanism ensures that the limit, offset are passed to the SQL layer to avoid pulling excess data from the database. Additionally, if there are parameters that could be filtered on, django filter backend provides a great way to filter out records. This eliminates unnecessary records being pulled in. \r\n\r\n4. Measuring & Profiling performance (5 minutes) ============================================\r\nPremature optimization is the root of all evil in programming (Don Knuth). The process of optimization starts with profiling your code. django debug toolbar is a very useful tool to instrument and measure the performance of your code. We\u2019re specifically interested in the latency in fetching data from the database. This is a combination of individual SQL query performance and number of SQL queries being performed. It may help uncover common mistakes for example `len(queryset)`. Fixing it involves simply changing it to `queryset.count()`. I will go over the django debug toolbar output to show redundant & slow queries. Queries that are run repeatedly in a loop using different `QuerySet` instance every time are also a major cause of slowness. \r\n\r\n5. Caching (3 minutes)\r\n====================\r\nCaching is a powerful tool if used effectively. We\u2019ll go through configuring caching with Redis with django. We\u2019ll cover built-in caching strategies such as view level caching. We\u2019ll talk about finer grained caching at an object or property level. Finally, we\u2019ll talk about cache invalidation and complexity in dealing with cache invalidation. I will briefly talk about using the django signals framework to achieve invalidation using `post_save()`, `post_delete()` signals. \r\n\r\n6. Asynchronous processing (3 minutes)\r\n===================================\r\nSome services require you to make external calls or perform heavy IO that may be deferred and are not essential for the response. For example, pinging an external metrics system. In most cases this can be offloaded to an external worker. This ensures that the request pipeline is not blocked. We will look at celery integration in django that makes it easy to implement this pattern. \r\n\r\n7. Other ideas to explore (1 minute) \r\n=============================== \r\nI will take some time to point to the audience to resources to further learn about performance optimizations such as using HTTP Caching via E-Tags / LastModified headers, Cached Session, Load Shedding. \r\n\r\n8. Questions (5 minutes) \r\n====================== \r\nI will take questions and provide references to the material presented for further reading.",
            "abstract_html": "<ol>\n<li>Understanding django\u2019s request processing lifecycle (3 minutes) ==========================================================\nI will start off by showing the django request processing lifecycle - starting with the WSGI handler to executing Request &amp; Response middlewares emphasizing that all of them are run serially for the request. Understanding django\u2019s request processing lifecycle helps gain key insights. For example blocking the request for a long time inside a middleware function or the API view function will slow down the requests and will negatively impact request throughput. Using too many middleware classes or performing blocking IO or function calls in the request handler should be avoided. </li>\n</ol>\n<h1>2. The django ORM layer (5 minutes)</h1>\n<p>django\u2019s ORM layer is one of it\u2019s greatest strengths. django restframework makes it very easy to expose SQL tables as RESTful resources to clients. However, there are many common mistake that should be avoided. Relying too much on the ORM to run optimal queries is one big issue. In this section, we\u2019ll go over optimizing database access through django\u2019s ORM specifically we\u2019ll cover adding indexes, prefetching or selecting related entities using prefetch_related &amp; select_related, reusing querysets as well as defining custom query sets. Finally, we\u2019ll also see how to discover costs associated with SQL queries using the EXPLAIN statement. </p>\n<h1>3. Pagination &amp; Filtering (5 minutes)</h1>\n<p>A common and fatal mistake is to pull excessive amount of data (records) from the server. This is usually mitigated by adding pagination and filtering. In this mechanism, the client provides a predicate (e.g. <code>city=SFO&amp;start=20&amp;pagesize=10</code>) that flows all the way to the database as part of the SQL query (WHERE <code>city</code>=\u201dSFO\u201d LIMIT 10 OFFSET 20). This helps the SQL query planner eliminate unnecessary records. Pagination establishes an upper bound on the amount of data being requested as well the time that the client needs to wait in order to receive a response. The upside of pagination is the client can make multiple concurrent requests to the server. This can reduce the overall time it takes to fetch the entire dataset since each request may be served by different server instances. django\u2019s built-in pagination mechanism ensures that the limit, offset are passed to the SQL layer to avoid pulling excess data from the database. Additionally, if there are parameters that could be filtered on, django filter backend provides a great way to filter out records. This eliminates unnecessary records being pulled in. </p>\n<ol>\n<li>Measuring &amp; Profiling performance (5 minutes) ============================================\nPremature optimization is the root of all evil in programming (Don Knuth). The process of optimization starts with profiling your code. django debug toolbar is a very useful tool to instrument and measure the performance of your code. We\u2019re specifically interested in the latency in fetching data from the database. This is a combination of individual SQL query performance and number of SQL queries being performed. It may help uncover common mistakes for example <code>len(queryset)</code>. Fixing it involves simply changing it to <code>queryset.count()</code>. I will go over the django debug toolbar output to show redundant &amp; slow queries. Queries that are run repeatedly in a loop using different <code>QuerySet</code> instance every time are also a major cause of slowness. </li>\n</ol>\n<h1>5. Caching (3 minutes)</h1>\n<p>Caching is a powerful tool if used effectively. We\u2019ll go through configuring caching with Redis with django. We\u2019ll cover built-in caching strategies such as view level caching. We\u2019ll talk about finer grained caching at an object or property level. Finally, we\u2019ll talk about cache invalidation and complexity in dealing with cache invalidation. I will briefly talk about using the django signals framework to achieve invalidation using <code>post_save()</code>, <code>post_delete()</code> signals. </p>\n<h1>6. Asynchronous processing (3 minutes)</h1>\n<p>Some services require you to make external calls or perform heavy IO that may be deferred and are not essential for the response. For example, pinging an external metrics system. In most cases this can be offloaded to an external worker. This ensures that the request pipeline is not blocked. We will look at celery integration in django that makes it easy to implement this pattern. </p>\n<h1>7. Other ideas to explore (1 minute)</h1>\n<p>I will take some time to point to the audience to resources to further learn about performance optimizations such as using HTTP Caching via E-Tags / LastModified headers, Cached Session, Load Shedding. </p>\n<h1>8. Questions (5 minutes)</h1>\n<p>I will take questions and provide references to the material presented for further reading.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-20T08:25:52.949Z",
            "speaker": 21,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 25,
        "fields": {
            "kind": 1,
            "title": "OS for AI: Microservices & Serverless Computing Enable the Next Gen of Machine Learning",
            "description": "We\u2019ll look at why Machine Learning is a natural fit for serverless computing, discuss a general architecture for scalable ML, and demonstrate GPU-enabled Serverless Python functions as a core component of the new Operating System for AI.",
            "abstract": "You\u2019ve trained machine learning models on your data, but how do you put them into production? When you have tens of thousands of model versions, each written in any mix of Python ML frameworks (SciKit, Tensorflow, Theano...), running on GPUs and exposed as REST API endpoints, and your users love to chain algorithms and run ensembles in parallel\u2026 how do you maintain a latency less than 20ms on just a few servers?\r\n\r\nAI has been a hot topic lately, with advances being made constantly in what is possible, there has not been as much discussion of the infrastructure and scaling challenges that come with it. At Algorithmia, we\u2019ve built, deployed, and scaled thousands of algorithms and machine learning models, using every kind of framework. We\u2019ve seen many of the challenges faced in this area, and in this talk I\u2019ll share some insights into the problems you\u2019re likely to face, and how to approach solving them.\r\n\r\nIn brief, we\u2019ll examine the need for, and implementations of, a complete \"Operating System for AI\" \u2013 a common interface for different algorithms to be used and combined, and a general architecture for serverless machine learning which is discoverable, versioned, scalable and sharable.",
            "abstract_html": "<p>You\u2019ve trained machine learning models on your data, but how do you put them into production? When you have tens of thousands of model versions, each written in any mix of Python ML frameworks (SciKit, Tensorflow, Theano...), running on GPUs and exposed as REST API endpoints, and your users love to chain algorithms and run ensembles in parallel\u2026 how do you maintain a latency less than 20ms on just a few servers?</p>\n<p>AI has been a hot topic lately, with advances being made constantly in what is possible, there has not been as much discussion of the infrastructure and scaling challenges that come with it. At Algorithmia, we\u2019ve built, deployed, and scaled thousands of algorithms and machine learning models, using every kind of framework. We\u2019ve seen many of the challenges faced in this area, and in this talk I\u2019ll share some insights into the problems you\u2019re likely to face, and how to approach solving them.</p>\n<p>In brief, we\u2019ll examine the need for, and implementations of, a complete \"Operating System for AI\" \u2013 a common interface for different algorithms to be used and combined, and a general architecture for serverless machine learning which is discoverable, versioned, scalable and sharable.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-20T18:35:02.833Z",
            "speaker": 22,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 26,
        "fields": {
            "kind": 1,
            "title": "When Booleans Are Not Enough... State Machines?",
            "description": "Booleans are great to represent a single state, but when it comes to representing multiple states and behaviors, they are far from ideal. This talk aims to identify common cases where booleans are not the right solution, and explores how state machines may be a better approach when designing objects that describe multiple states and behaviors.",
            "abstract": "Usually, we tend to represent objects status with boolean attributes. At first, this seems right and simple enough, but as the code base evolves and gets bigger, status become complex; so do the transitions between them.\r\n\r\nTransition between status can be seen as behaviors or actions. Therefore, we find ourselves implementing rules to enforce behaviors and validating transitions; or even worse, not enforcing or validating anything at all.\r\n\r\nConsidering using a state machine to represent the status of an object may be ideal. As each state on the machine can represent an object status, the transitions between states can represent well defined actions or behaviors that can be performed between status. \r\n\r\nThis talk focuses on how to identify when booleans are not the right type to represent status, and how using a state machine may lead you to a better and cleaner design that can enforce conditions between status by simply relying on the definition of a state machine.",
            "abstract_html": "<p>Usually, we tend to represent objects status with boolean attributes. At first, this seems right and simple enough, but as the code base evolves and gets bigger, status become complex; so do the transitions between them.</p>\n<p>Transition between status can be seen as behaviors or actions. Therefore, we find ourselves implementing rules to enforce behaviors and validating transitions; or even worse, not enforcing or validating anything at all.</p>\n<p>Considering using a state machine to represent the status of an object may be ideal. As each state on the machine can represent an object status, the transitions between states can represent well defined actions or behaviors that can be performed between status. </p>\n<p>This talk focuses on how to identify when booleans are not the right type to represent status, and how using a state machine may lead you to a better and cleaner design that can enforce conditions between status by simply relying on the definition of a state machine.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-21T15:33:08.323Z",
            "speaker": 23,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 27,
        "fields": {
            "kind": 1,
            "title": "CPython, Grumpy, PyPy- When, How, Why",
            "description": "I will talk about the differences between the three implementations. Focusing on use cases in which each of them should be used and a drill-down to how each environment tackles performance (GIL, just-in-time and Goroutines). I will also evaluate performance impact in real life scenario.",
            "abstract": "I will talk about the differences between the three implementations. Focusing on use cases in which each of them should be used and a drill-down to how each environment tackles performance (GIL, just-in-time and Goroutines). I will also evaluate performance impact in real life scenario.",
            "abstract_html": "<p>I will talk about the differences between the three implementations. Focusing on use cases in which each of them should be used and a drill-down to how each environment tackles performance (GIL, just-in-time and Goroutines). I will also evaluate performance impact in real life scenario.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-22T13:45:23.569Z",
            "speaker": 24,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 28,
        "fields": {
            "kind": 1,
            "title": "Production-Ready Python applications",
            "description": "This session is for anyone in the audience who wants to, or does write Python applications that run in production environments. No specific knowledge is required here, however any knowledge of operating a production service would be useful.",
            "abstract": "In 2016, Susan Fowler released the 'Production Ready Microservices' book. This book sets an industry benchmark on explaining how microservices should be conceived, all the way through to documentation. So how does this translate for Python applications? This session will explore how to expertly deploy your Python microservice to production.",
            "abstract_html": "<p>In 2016, Susan Fowler released the 'Production Ready Microservices' book. This book sets an industry benchmark on explaining how microservices should be conceived, all the way through to documentation. So how does this translate for Python applications? This session will explore how to expertly deploy your Python microservice to production.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-22T16:07:46.871Z",
            "speaker": 25,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 30,
        "fields": {
            "kind": 1,
            "title": "Understanding Decorators",
            "description": "This talk is about understanding how decorators work and how to write them. We'll also cover function and scoping basics, the foundation to understanding the more advanced topic of decorators. This talk is geared to beginner and intermediate users of Python: those that have a familiarity with Python syntax but may not fully understand how things in the language work.",
            "abstract": "Decorators are an often-used and powerful feature in Python. They provide the ability to alter function calls without having to subclass or change the function source. Have you wanted to understand how Python decorators do this? In this talk, we'll go beyond simply knowing what and how to use decorators; we'll dive deep into Python functions to understand how decorators work. You will learn about scoping, the *args/**kwargs syntax, and come away with a better understanding of Python altogether. This talk is interactive: we'll have fun learning about functions and walk through writing decorators together.",
            "abstract_html": "<p>Decorators are an often-used and powerful feature in Python. They provide the ability to alter function calls without having to subclass or change the function source. Have you wanted to understand how Python decorators do this? In this talk, we'll go beyond simply knowing what and how to use decorators; we'll dive deep into Python functions to understand how decorators work. You will learn about scoping, the <em>args/</em>*kwargs syntax, and come away with a better understanding of Python altogether. This talk is interactive: we'll have fun learning about functions and walk through writing decorators together.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-26T19:10:21.077Z",
            "speaker": 27,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 31,
        "fields": {
            "kind": 1,
            "title": "Unit Test and How to Write Your Own Mocking Library",
            "description": "This talk covers the important topic of unit testing. We'll cover the theory behind unit test like the rules of a proper unit test and how to employ three different mocking strategies. We'll use unittest, doctest, and pytest. We'll discuss common patterns encountered when writing test code and write our own mocking library.",
            "abstract": "Being able to write unittests is a necessary skill of the professional  programmer. A proper unittest adheres to several principles. Two of these principles are (1) tests should be isolated from each other and (2) they should be fast. Because of this, it is important to know when and how to mock. In this talk, we'll cover the basics first: the principles of unit testing and the three mocking strategies. Then we'll put the theory to practice and show how to write tests in three different frameworks, unittest, doctest and pytest. Finally, we'll discuss the common patterns encountered when writing tests, such as the need to stub out interfaces with known return values. Then we'll write our own mocking library to solve these problems.",
            "abstract_html": "<p>Being able to write unittests is a necessary skill of the professional  programmer. A proper unittest adheres to several principles. Two of these principles are (1) tests should be isolated from each other and (2) they should be fast. Because of this, it is important to know when and how to mock. In this talk, we'll cover the basics first: the principles of unit testing and the three mocking strategies. Then we'll put the theory to practice and show how to write tests in three different frameworks, unittest, doctest and pytest. Finally, we'll discuss the common patterns encountered when writing tests, such as the need to stub out interfaces with known return values. Then we'll write our own mocking library to solve these problems.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-26T20:23:13.103Z",
            "speaker": 27,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 32,
        "fields": {
            "kind": 1,
            "title": "Chaps: A Relative Dir Pants Wrapper",
            "description": "Chaps is a relative dir pants wrapper for the pants build system.  If you're in your working directory, you can use short names for your targets.  Instead of having to cd or reference the full path for the pants target (./pants path/to/target_dir:target_name), you can run chaps binary :target_name. Chaps will call pants with the correct path to compile your target.   Chaps also works with tests.",
            "abstract": "This talk will go over what problem we're trying to solve, how we solved it, what chaps does/how it does it, supported pants commands, features and limitations of chaps, and availability to the community for contribution and use.",
            "abstract_html": "<p>This talk will go over what problem we're trying to solve, how we solved it, what chaps does/how it does it, supported pants commands, features and limitations of chaps, and availability to the community for contribution and use.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-27T17:27:52.837Z",
            "speaker": 28,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 33,
        "fields": {
            "kind": 1,
            "title": "Docs as Code",
            "description": "Documentation is the primary user interface for APIs, Python packages, and tools of all sorts. Yet docs are often after thoughts and second-class citizens.\r\n\r\nDocumentation is an engineering product. It should be treated like one. This talk will discuss how to apply standard development processes to documentation, and why doing so is important.",
            "abstract": "Documentation is the primary user interface for APIs, Python packages, SDKs, and tools of all sorts. And even most GUI-based applications and platforms require documentation of some sort. Yet docs are often after thoughts and second-class citizens.\r\n\r\nDocumentation is a product \u2013 an engineering product. It should be treated like one. This talk will discuss how to apply standard development processes like version control, automated testing, and continuous integration to documentation, and why doing so is important. \r\n\r\nA Python-based toolchain currently in use on a large open source documentation project will provide concrete examples.",
            "abstract_html": "<p>Documentation is the primary user interface for APIs, Python packages, SDKs, and tools of all sorts. And even most GUI-based applications and platforms require documentation of some sort. Yet docs are often after thoughts and second-class citizens.</p>\n<p>Documentation is a product \u2013 an engineering product. It should be treated like one. This talk will discuss how to apply standard development processes like version control, automated testing, and continuous integration to documentation, and why doing so is important. </p>\n<p>A Python-based toolchain currently in use on a large open source documentation project will provide concrete examples.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-27T19:04:06.585Z",
            "speaker": 29,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 34,
        "fields": {
            "kind": 1,
            "title": "Pluggable Libs Through Design Patterns",
            "description": "In this talk I'll will use real world open source libraries to walk the audience through 3 popular Design Patterns: Adapter, Strategy and Collection Pipeline. Those libraries heavily rely on patterns to allow flexibility and pluggability. A great example of this is Python Social Auth (PSA) which will be used to guide the presentation and provide examples on good use of patterns.",
            "abstract": "Design Patterns are a standardized way to talk about certain code architectures. They've been first formally introduced through the book \"Design Patterns: Elements of Reusable Object-Oriented Software\" and we are still discovering new ones. They are such a good way to architecture code that even Python comes with some of them, such as the iterator, built into the language.\r\n\r\nIn this talk I'll will walk the audience through 3 popular design patterns: Adapter, Strategy and Collection Pipeline. Some of the best examples of use cases for those are open source libraries. Those libraries heavily rely on patterns to allow flexibility and pluggability. A very good example of this if the Python Social Auth (PSA) project. PSA allows applications to integrate with a huge number of web authentication service providers. It works with frameworks such as Django, Flask, Tornado and Pyramid and knows how to handle OAuth1, OAuth2, OpenID and SAML (and its slight customizations through each provider). This kind of flexibility is only possible because PSA has design patterns in its core.\r\n\r\nBefore observing patterns in the libraries, attendees will be presented to real world situations being tackled with poorly designed software. This will give context before we move on to understand how we can use a pattern to improve the quality of it and a give a more detailed explanation on how it works. This will lead to examples of tools that implement that pattern and an exploration of the architecture and the source code.\r\n\r\nOutline:\r\n\r\n+ Brief Introduction to Design Patterns (~ 5 min)\r\n    - History\r\n    - \"Design Patterns: Elements of Reusable Object-Oriented Software\"\r\n    - State of the art\r\n    - Python's built-in design patterns\r\n    - Dynamic Typing and why Python makes it easy to use design patterns\r\n    - Frameworks and Design Patterns\r\n    - Open source and Design Patterns\r\n+ The Adapter pattern (~ 5 min)\r\n    - A poorly designed solution to making requests to APIs\r\n    - A more sensible solution to making requests to APIs\r\n    - What is the Adapter pattern and how it works\r\n    - How [Tapioca](https://github.com/vintasoftware/tapioca-wrapper) learns how to handle requests to RESTful APIs\r\n- Introduction to [Python Social Auth (PSA)](https://python-social-auth.readthedocs.io/en/latest/)\r\n- The evolution from Django specific to a generic social authentication framework\r\n- How PSA adapts to any social login\r\n+ The Strategy pattern (~ 5 min)\r\n    - A poorly designed solution to handling users in different payment plans\r\n    - A more sensible solution to handling users in different payment plans\r\n    - What is the Strategy pattern and how it works\r\n    - Other use cases: handling multiple payment methods, publishing to social networks\r\n    - How PSA adapts to any Python web framework\r\n+ The Pipeline pattern (~ 5 min)\r\n    - A poorly designed solution to scraping web pages\r\n    - A more sensible solution to scraping web pages\r\n    - What is the Pipeline pattern and how it works\r\n    - How [Scrapy](https://scrapy.org/) uses pipelines\r\n    - Improving the multiple payment plans problem with pipelines\r\n    - How PSA adapts to your application\r\n+ \"What next?\" (~ 5 min)\r\n    - How open source benefits from design patterns\r\n    - Learn the design patterns as a framework for communication not for overengineering or bragging about how smart you are\r\n    - SOLID, SoC, KISS, YAGNI, SSOT\r\n- Read open source code",
            "abstract_html": "<p>Design Patterns are a standardized way to talk about certain code architectures. They've been first formally introduced through the book \"Design Patterns: Elements of Reusable Object-Oriented Software\" and we are still discovering new ones. They are such a good way to architecture code that even Python comes with some of them, such as the iterator, built into the language.</p>\n<p>In this talk I'll will walk the audience through 3 popular design patterns: Adapter, Strategy and Collection Pipeline. Some of the best examples of use cases for those are open source libraries. Those libraries heavily rely on patterns to allow flexibility and pluggability. A very good example of this if the Python Social Auth (PSA) project. PSA allows applications to integrate with a huge number of web authentication service providers. It works with frameworks such as Django, Flask, Tornado and Pyramid and knows how to handle OAuth1, OAuth2, OpenID and SAML (and its slight customizations through each provider). This kind of flexibility is only possible because PSA has design patterns in its core.</p>\n<p>Before observing patterns in the libraries, attendees will be presented to real world situations being tackled with poorly designed software. This will give context before we move on to understand how we can use a pattern to improve the quality of it and a give a more detailed explanation on how it works. This will lead to examples of tools that implement that pattern and an exploration of the architecture and the source code.</p>\n<p>Outline:</p>\n<ul>\n<li>Brief Introduction to Design Patterns (~ 5 min)<ul>\n<li>History</li>\n<li>\"Design Patterns: Elements of Reusable Object-Oriented Software\"</li>\n<li>State of the art</li>\n<li>Python's built-in design patterns</li>\n<li>Dynamic Typing and why Python makes it easy to use design patterns</li>\n<li>Frameworks and Design Patterns</li>\n<li>Open source and Design Patterns</li>\n</ul>\n</li>\n<li>The Adapter pattern (~ 5 min)<ul>\n<li>A poorly designed solution to making requests to APIs</li>\n<li>A more sensible solution to making requests to APIs</li>\n<li>What is the Adapter pattern and how it works</li>\n<li>How <a href=\"https://github.com/vintasoftware/tapioca-wrapper\">Tapioca</a> learns how to handle requests to RESTful APIs</li>\n</ul>\n</li>\n<li>Introduction to <a href=\"https://python-social-auth.readthedocs.io/en/latest/\">Python Social Auth (PSA)</a></li>\n<li>The evolution from Django specific to a generic social authentication framework</li>\n<li>How PSA adapts to any social login</li>\n<li>The Strategy pattern (~ 5 min)<ul>\n<li>A poorly designed solution to handling users in different payment plans</li>\n<li>A more sensible solution to handling users in different payment plans</li>\n<li>What is the Strategy pattern and how it works</li>\n<li>Other use cases: handling multiple payment methods, publishing to social networks</li>\n<li>How PSA adapts to any Python web framework</li>\n</ul>\n</li>\n<li>The Pipeline pattern (~ 5 min)<ul>\n<li>A poorly designed solution to scraping web pages</li>\n<li>A more sensible solution to scraping web pages</li>\n<li>What is the Pipeline pattern and how it works</li>\n<li>How <a href=\"https://scrapy.org/\">Scrapy</a> uses pipelines</li>\n<li>Improving the multiple payment plans problem with pipelines</li>\n<li>How PSA adapts to your application</li>\n</ul>\n</li>\n<li>\"What next?\" (~ 5 min)<ul>\n<li>How open source benefits from design patterns</li>\n<li>Learn the design patterns as a framework for communication not for overengineering or bragging about how smart you are</li>\n<li>SOLID, SoC, KISS, YAGNI, SSOT</li>\n</ul>\n</li>\n<li>Read open source code</li>\n</ul>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-28T21:20:50.846Z",
            "speaker": 30,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 35,
        "fields": {
            "kind": 1,
            "title": "Parse NBA Statistics with Openpyxl",
            "description": "Read NBA statistics from a Microsoft Excel sheet using the Openpyxl library. How will we know which statistics to look for and return? Text a number two players and a basketball statistic (like total points) and then the SMS response will look up the statistics of the corresponding players.",
            "abstract": "Data stored in Excel spreadsheets can be hard to read with anything other than Excel and it\u2019s especially tough to compare two specific datasets within all that data. One possible solution? Let Python do the dirty work of finding the information for us! We'll read NBA statistics with the Openpyxl library and we'll also get the audience involved.",
            "abstract_html": "<p>Data stored in Excel spreadsheets can be hard to read with anything other than Excel and it\u2019s especially tough to compare two specific datasets within all that data. One possible solution? Let Python do the dirty work of finding the information for us! We'll read NBA statistics with the Openpyxl library and we'll also get the audience involved.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-29T00:22:45.225Z",
            "speaker": 31,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 36,
        "fields": {
            "kind": 1,
            "title": "Why you need to know the internals of list and tuple?",
            "description": "Data structures are widely used in CPython but the internal details of them are not common knowledge. In this talk, we will discuss the internals of CPython\u2019s list and tuple with examples. We will also learn about the space and time complexity of the list and tuple methods, and finally the appropriate usage of these methods.",
            "abstract": "Data structures are widely used in CPython but the internal details of them are not common knowledge. In this talk, we will discuss the internals of CPython\u2019s list and tuple with examples. We will also learn about the space and time complexity of the list and tuple methods, and finally the appropriate usage of these methods. Understanding internals will help in choosing the right data structure for a particular task and also with choosing the right method for a given data structure. \r\n\r\nThis talk is for everybody who is currently using CPython\u2019s data structures such as lists and tuples and would like to learn about the internals.  The attendees need to have intermediate knowledge of Python. \r\n\r\nAt the end of the talk, the attendees will have a clear knowledge of the internal working of CPython\u2019s list and tuple and can use them appropriately and efficiently.",
            "abstract_html": "<p>Data structures are widely used in CPython but the internal details of them are not common knowledge. In this talk, we will discuss the internals of CPython\u2019s list and tuple with examples. We will also learn about the space and time complexity of the list and tuple methods, and finally the appropriate usage of these methods. Understanding internals will help in choosing the right data structure for a particular task and also with choosing the right method for a given data structure. </p>\n<p>This talk is for everybody who is currently using CPython\u2019s data structures such as lists and tuples and would like to learn about the internals.  The attendees need to have intermediate knowledge of Python. </p>\n<p>At the end of the talk, the attendees will have a clear knowledge of the internal working of CPython\u2019s list and tuple and can use them appropriately and efficiently.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-29T03:43:53.130Z",
            "speaker": 32,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 37,
        "fields": {
            "kind": 1,
            "title": "Clean up CFP module of PyBay.com",
            "description": "This is a test to see if we are getting phone numbers, talk length and new themes in the backend of our system",
            "abstract": "people will die if it's not fixed!",
            "abstract_html": "<p>people will die if it's not fixed!</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-29T16:57:06.712Z",
            "speaker": 6,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 38,
        "fields": {
            "kind": 1,
            "title": "1 + 1 = 1 or Record Deduplication with Python",
            "description": "What to do when you must find duplicate or related records in one or more datasets and you don't have unique identifiers, like the SSN for US citizens? The answer is to use Data Deduplication techniques and look for matches by cleaning and comparing attributes in a fuzzy way. In this talk, you'll learn with Python examples how to do this without needing any expert Data Science knowledge.",
            "abstract": "Record Deduplication, or more generally, Record Linkage is the task of finding which records refer to the same entity, like a person or a company. It's used mainly when there isn't a unique identifier in records like Social Security Number for US citizens. This means one can't trivially find duplicate records in a single dataset, neither easily link records from different datasets. Without an identifier, record linkage looks for matches by cleaning and comparing record attributes in a fuzzy way. Imagine you have two datasets with information about people, but without any unique identifier in the records. You have to compare attributes like name, date of birth, and address in a smart way to find which records from the two datasets refer to the same person. A similar approach must be used to dedupe records in a single dataset, so Record Deduplication is a kind of Record Linkage.\r\n\r\nThere are a number of important applications of data deduplication in government and business. For example, by deduping records from Census data, the Australian government was able to find there were 250,000 fewer people in the country than they previously thought. This reduction impacted the estimations of government agencies and even caused the revision economical projections. Similarly, businesses can use record linkage techniques to enrich their customers' data with publicly available datasets.\r\n\r\nIn this talk, you'll learn with Python examples the main concepts of Record Deduplication, what kinds of problems can be solved, what's the most common workflow for the process, what algorithms are involved, and which tools and libraries you can use. Although some of the discussed concepts are related to data mining, any intermediate-level Python developer will be able to learn the basics of how to dedupe data using Python.",
            "abstract_html": "<p>Record Deduplication, or more generally, Record Linkage is the task of finding which records refer to the same entity, like a person or a company. It's used mainly when there isn't a unique identifier in records like Social Security Number for US citizens. This means one can't trivially find duplicate records in a single dataset, neither easily link records from different datasets. Without an identifier, record linkage looks for matches by cleaning and comparing record attributes in a fuzzy way. Imagine you have two datasets with information about people, but without any unique identifier in the records. You have to compare attributes like name, date of birth, and address in a smart way to find which records from the two datasets refer to the same person. A similar approach must be used to dedupe records in a single dataset, so Record Deduplication is a kind of Record Linkage.</p>\n<p>There are a number of important applications of data deduplication in government and business. For example, by deduping records from Census data, the Australian government was able to find there were 250,000 fewer people in the country than they previously thought. This reduction impacted the estimations of government agencies and even caused the revision economical projections. Similarly, businesses can use record linkage techniques to enrich their customers' data with publicly available datasets.</p>\n<p>In this talk, you'll learn with Python examples the main concepts of Record Deduplication, what kinds of problems can be solved, what's the most common workflow for the process, what algorithms are involved, and which tools and libraries you can use. Although some of the discussed concepts are related to data mining, any intermediate-level Python developer will be able to learn the basics of how to dedupe data using Python.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-29T17:31:24.695Z",
            "speaker": 33,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 39,
        "fields": {
            "kind": 1,
            "title": "Programming Your Own Software Process",
            "description": "Software development processes vary a lot, some are amazing ways to be productive, others are very inconvenient for the programmer. Regardless of how you see yours, perhaps the way to create a really good one isn't to use and out-of-the-box solution. It's to jump out of the box. This talk will focus on how to assemble, iterate and then code your software process to achieve awesome results!",
            "abstract": "* What is Modern Agile is and why it came to be? (3min)\r\n** Joshua Kerievsky talk and Modern Agile definition\r\n\t\tQuick reference to his talk and the idea of creating your own software process instead of using a whole-package, quotes from the manifesto creators that a lot of the things associated with agile actually hinder developers work to deliver faster.\r\n** Where to start with your software project and The four principles of modern agile\r\nDescription of the four principles and my experience as to where to start with them:\r\nFocusing on security and delivering value constantly is a good first step; \r\nGuarantee productivity and good work then start growing when problems appear;\r\nSolve problems by making people awesome then iterate faster and faster by coding it down and allowing people to change it whenever a better solution appears.\r\n\r\n* Make Safety a Prerequisite (8min)\r\n** Code ownership, PRs and linters\r\nConcept of code ownership and how it evolves from no code ownership, to personal code ownership to the ideal distributed code ownership. Exemplifying it\u2019s benefits and how PRs and automated tests, logging, alerts and [impersonate](https://pypi.python.org/pypi/django-impersonate) help with this in our projects. Finishing with how linters can help keep alive what was learned.\r\n** Errors are most of the time, systemic, not someone's fault\r\n\t[Sentry](https://sentry.io/for/python/) is a very good tool to focus efforts on solving crisis, not blaming individuals. And this understanding is very important, people should be safe knowing that if they find something it was because your development process had a failure, not because someone did something wrong.\r\n\r\n* Deliver Value Continuously (5min)\r\n** Feature toggle and safer deploys\r\nHow not to disrupt your git flow with feature toggle. How they give you more productivity and allow to catch bugs faster. Our experience at Vinta and how they are used at Spotify to smooth the process of deploying important features.\r\n** CI and automated tests\r\n\tAutomated tests and continuous deployment are great to guarantee quality on code, lots of tools available can integrate smoothly with your python projects like [CircleCI](https://circleci.com). These tools can also be included in your boilerplate for easy setup in new projects and to provide automated tests from day one.\r\n\r\n* Make People Awesome (6min)\r\n** UX concerns, front-end heatmapping and Backend metrics\r\n\tUsers want to feel good using products we develop, but to interview users is expensive and time consuming. Some tools allow us to identify that something is wrong on the user side, which allows our teams to act proactively with the client bringing news of a feature underperforming before the users complain. Hotjar is great for front-end analysis with Heatmaps and mixpanel is a solution for frontend and backend events. Both of them integrate with Python/Django/Flask with a simple piece of code. \r\n** Collaborator's tools for growth and goals\r\n\tWeekly meetings are common practice, but can be conducted to optimize learning through the company. This section is about how they can be tuned to that end and how to make it smooth so that employees share knowledge between themselves. There are several different tools for that, I\u2019ll show one called [django-knowledge-share](https://github.com/vintasoftware/django-knowledge-share) that publishes on twitter whatever someone learns something and writes it on Slack.\r\n\r\n* Experiment & Learn Rapidly (3min)\r\n** Playbook and bottom-up process innovation\r\n\tIf you write down the way everything runs in your company, people don\u2019t need to ask around and, if they find a way that works best, it should be easier to spread around that knowledge and keep it safe without filling the developers day with bureaucracy. A playbook on Git, such as the ones from Vinta and Basecamp, and a little freedom to project teams can go a long way into solidifying something but still leaving room for improvement.",
            "abstract_html": "<ul>\n<li>\n<p>What is Modern Agile is and why it came to be? (3min)\n<strong> Joshua Kerievsky talk and Modern Agile definition\n        Quick reference to his talk and the idea of creating your own software process instead of using a whole-package, quotes from the manifesto creators that a lot of the things associated with agile actually hinder developers work to deliver faster.\n</strong> Where to start with your software project and The four principles of modern agile\nDescription of the four principles and my experience as to where to start with them:\nFocusing on security and delivering value constantly is a good first step; \nGuarantee productivity and good work then start growing when problems appear;\nSolve problems by making people awesome then iterate faster and faster by coding it down and allowing people to change it whenever a better solution appears.</p>\n</li>\n<li>\n<p>Make Safety a Prerequisite (8min)\n<strong> Code ownership, PRs and linters\nConcept of code ownership and how it evolves from no code ownership, to personal code ownership to the ideal distributed code ownership. Exemplifying it\u2019s benefits and how PRs and automated tests, logging, alerts and <a href=\"https://pypi.python.org/pypi/django-impersonate\">impersonate</a> help with this in our projects. Finishing with how linters can help keep alive what was learned.\n</strong> Errors are most of the time, systemic, not someone's fault\n    <a href=\"https://sentry.io/for/python/\">Sentry</a> is a very good tool to focus efforts on solving crisis, not blaming individuals. And this understanding is very important, people should be safe knowing that if they find something it was because your development process had a failure, not because someone did something wrong.</p>\n</li>\n<li>\n<p>Deliver Value Continuously (5min)\n<strong> Feature toggle and safer deploys\nHow not to disrupt your git flow with feature toggle. How they give you more productivity and allow to catch bugs faster. Our experience at Vinta and how they are used at Spotify to smooth the process of deploying important features.\n</strong> CI and automated tests\n    Automated tests and continuous deployment are great to guarantee quality on code, lots of tools available can integrate smoothly with your python projects like <a href=\"https://circleci.com\">CircleCI</a>. These tools can also be included in your boilerplate for easy setup in new projects and to provide automated tests from day one.</p>\n</li>\n<li>\n<p>Make People Awesome (6min)\n<strong> UX concerns, front-end heatmapping and Backend metrics\n    Users want to feel good using products we develop, but to interview users is expensive and time consuming. Some tools allow us to identify that something is wrong on the user side, which allows our teams to act proactively with the client bringing news of a feature underperforming before the users complain. Hotjar is great for front-end analysis with Heatmaps and mixpanel is a solution for frontend and backend events. Both of them integrate with Python/Django/Flask with a simple piece of code. \n</strong> Collaborator's tools for growth and goals\n    Weekly meetings are common practice, but can be conducted to optimize learning through the company. This section is about how they can be tuned to that end and how to make it smooth so that employees share knowledge between themselves. There are several different tools for that, I\u2019ll show one called <a href=\"https://github.com/vintasoftware/django-knowledge-share\">django-knowledge-share</a> that publishes on twitter whatever someone learns something and writes it on Slack.</p>\n</li>\n<li>\n<p>Experiment &amp; Learn Rapidly (3min)\n** Playbook and bottom-up process innovation\n    If you write down the way everything runs in your company, people don\u2019t need to ask around and, if they find a way that works best, it should be easier to spread around that knowledge and keep it safe without filling the developers day with bureaucracy. A playbook on Git, such as the ones from Vinta and Basecamp, and a little freedom to project teams can go a long way into solidifying something but still leaving room for improvement.</p>\n</li>\n</ul>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-29T18:13:15.087Z",
            "speaker": 34,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 40,
        "fields": {
            "kind": 1,
            "title": "Programming Your Own Software Process",
            "description": "Software development processes vary a lot, some are amazing ways to be productive, others are very inconvenient for the programmer. Regardless of how you see yours, perhaps the way to create a really good one isn't to use and out-of-the-box solution. It's to jump out of the box. This talk will focus on how to assemble, iterate and then code your software process to achieve awesome results!",
            "abstract": "* What is Modern Agile is and why it came to be? (6min)\r\n** Joshua Kerievsky talk and Modern Agile definition\r\n\t\tQuick reference to his talk and the idea of creating your own software process instead of using a whole-package, quotes from the manifesto creators that a lot of the things associated with agile actually hinder developers work to deliver faster.\r\n** Where to start with your software project and The four principles of modern agile\r\nDescription of the four principles and my experience as to where to start with them:\r\nFocusing on security and delivering value constantly is a good first step; \r\nGuarantee productivity and good work then start growing when problems appear;\r\nSolve problems by making people awesome then iterate faster and faster by coding it down and allowing people to change it whenever a better solution appears.\r\n\r\n* Make Safety a Prerequisite (12min)\r\n** Code ownership, PRs and linters\r\nConcept of code ownership and how it evolves from no code ownership, to personal code ownership to the ideal distributed code ownership. Exemplifying it\u2019s benefits and how PRs and automated tests, logging, alerts and [impersonate](https://pypi.python.org/pypi/django-impersonate) help with this in our projects. Finishing with how linters can help keep alive what was learned.\r\n** Errors are most of the time, systemic, not someone's fault\r\n\t[Sentry](https://sentry.io/for/python/) is a very good tool to focus efforts on solving crisis, not blaming individuals. And this understanding is very important, people should be safe knowing that if they find something it was because your development process had a failure, not because someone did something wrong.\r\n\r\n* Deliver Value Continuously (9min)\r\n** Feature toggle and safer deploys\r\nHow not to disrupt your git flow with feature toggle. How they give you more productivity and allow to catch bugs faster. Our experience at Vinta and how they are used at Spotify to smooth the process of deploying important features.\r\n** CI and automated tests\r\n\tAutomated tests and continuous deployment are great to guarantee quality on code, lots of tools available can integrate smoothly with your python projects like [CircleCI](https://circleci.com). These tools can also be included in your boilerplate for easy setup in new projects and to provide automated tests from day one.\r\n\r\n* Make People Awesome (9min)\r\n** UX concerns, front-end heatmapping and Backend metrics\r\n\tUsers want to feel good using products we develop, but to interview users is expensive and time consuming. Some tools allow us to identify that something is wrong on the user side, which allows our teams to act proactively with the client bringing news of a feature underperforming before the users complain. Hotjar is great for front-end analysis with Heatmaps and mixpanel is a solution for frontend and backend events. Both of them integrate with Python/Django/Flask with a simple piece of code. \r\n** Collaborator's tools for growth and goals\r\n\tWeekly meetings are common practice, but can be conducted to optimize learning through the company. This section is about how they can be tuned to that end and how to make it smooth so that employees share knowledge between themselves. There are several different tools for that, I\u2019ll show one called [django-knowledge-share](https://github.com/vintasoftware/django-knowledge-share) that publishes on twitter whatever someone learns something and writes it on Slack.\r\n\r\n* Experiment & Learn Rapidly (5min)\r\n** Playbook and bottom-up process innovation\r\n\tIf you write down the way everything runs in your company, people don\u2019t need to ask around and, if they find a way that works best, it should be easier to spread around that knowledge and keep it safe without filling the developers day with bureaucracy. A playbook on Git, such as the ones from Vinta and Basecamp, and a little freedom to project teams can go a long way into solidifying something but still leaving room for improvement.",
            "abstract_html": "<ul>\n<li>\n<p>What is Modern Agile is and why it came to be? (6min)\n<strong> Joshua Kerievsky talk and Modern Agile definition\n        Quick reference to his talk and the idea of creating your own software process instead of using a whole-package, quotes from the manifesto creators that a lot of the things associated with agile actually hinder developers work to deliver faster.\n</strong> Where to start with your software project and The four principles of modern agile\nDescription of the four principles and my experience as to where to start with them:\nFocusing on security and delivering value constantly is a good first step; \nGuarantee productivity and good work then start growing when problems appear;\nSolve problems by making people awesome then iterate faster and faster by coding it down and allowing people to change it whenever a better solution appears.</p>\n</li>\n<li>\n<p>Make Safety a Prerequisite (12min)\n<strong> Code ownership, PRs and linters\nConcept of code ownership and how it evolves from no code ownership, to personal code ownership to the ideal distributed code ownership. Exemplifying it\u2019s benefits and how PRs and automated tests, logging, alerts and <a href=\"https://pypi.python.org/pypi/django-impersonate\">impersonate</a> help with this in our projects. Finishing with how linters can help keep alive what was learned.\n</strong> Errors are most of the time, systemic, not someone's fault\n    <a href=\"https://sentry.io/for/python/\">Sentry</a> is a very good tool to focus efforts on solving crisis, not blaming individuals. And this understanding is very important, people should be safe knowing that if they find something it was because your development process had a failure, not because someone did something wrong.</p>\n</li>\n<li>\n<p>Deliver Value Continuously (9min)\n<strong> Feature toggle and safer deploys\nHow not to disrupt your git flow with feature toggle. How they give you more productivity and allow to catch bugs faster. Our experience at Vinta and how they are used at Spotify to smooth the process of deploying important features.\n</strong> CI and automated tests\n    Automated tests and continuous deployment are great to guarantee quality on code, lots of tools available can integrate smoothly with your python projects like <a href=\"https://circleci.com\">CircleCI</a>. These tools can also be included in your boilerplate for easy setup in new projects and to provide automated tests from day one.</p>\n</li>\n<li>\n<p>Make People Awesome (9min)\n<strong> UX concerns, front-end heatmapping and Backend metrics\n    Users want to feel good using products we develop, but to interview users is expensive and time consuming. Some tools allow us to identify that something is wrong on the user side, which allows our teams to act proactively with the client bringing news of a feature underperforming before the users complain. Hotjar is great for front-end analysis with Heatmaps and mixpanel is a solution for frontend and backend events. Both of them integrate with Python/Django/Flask with a simple piece of code. \n</strong> Collaborator's tools for growth and goals\n    Weekly meetings are common practice, but can be conducted to optimize learning through the company. This section is about how they can be tuned to that end and how to make it smooth so that employees share knowledge between themselves. There are several different tools for that, I\u2019ll show one called <a href=\"https://github.com/vintasoftware/django-knowledge-share\">django-knowledge-share</a> that publishes on twitter whatever someone learns something and writes it on Slack.</p>\n</li>\n<li>\n<p>Experiment &amp; Learn Rapidly (5min)\n** Playbook and bottom-up process innovation\n    If you write down the way everything runs in your company, people don\u2019t need to ask around and, if they find a way that works best, it should be easier to spread around that knowledge and keep it safe without filling the developers day with bureaucracy. A playbook on Git, such as the ones from Vinta and Basecamp, and a little freedom to project teams can go a long way into solidifying something but still leaving room for improvement.</p>\n</li>\n</ul>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-29T18:15:35.010Z",
            "speaker": 34,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 42,
        "fields": {
            "kind": 1,
            "title": "Pull Requests: Merging good practices into your project",
            "description": "On average, developers spend 45% of their time fixing bugs and technical debt, when they could be developing new features, had those bugs been caught during code review. Badly reviewed code can result in huge risks and unpredictable behavior. The attendees will learn tips, tools, processes and recommended practices from my experience and from big players like Django, Facebook, Mozilla, etc.",
            "abstract": "Although known by most, Pull Requests are often not dealt with in the most effective way. Believe it or not, there are teams that don\u2019t review code at all! People may assume that a senior developer is experienced enough to not make any mistakes, or that merely changing those 3 lines of code couldn\u2019t possibly do any harm to the system. In these cases, it\u2019s not uncommon to skip the code review in order to cut some time. Unreviewed (or badly reviewed) code can be extremely dangerous, resulting in huge risks and unpredictable behavior.\r\n\r\nA survey says that, on average, developers spend 45% of their time fixing bugs and technical debt, when they could be developing new features instead. Defining simple guideline files, adopting certain behaviors and setting up repository configurations are steps that can increase manyfold the code review performance (in both time and quality). Using review tools both on server (e.g. Heroku Review Apps) and locally (e.g. linters) can also greatly increase the process\u2019 speed. Creating templates and checklists ensures no step is overlooked or forgotten. The list goes on, but enough spoilers for now. The attendees will learn specific tips, tools, processes and recommended practices that were compiled from research and real-life use cases (both from my experience and from big players like Django, Facebook, Instagram, Mozilla, etc), along with some survey data that demonstrates why reviewing code is important.",
            "abstract_html": "<p>Although known by most, Pull Requests are often not dealt with in the most effective way. Believe it or not, there are teams that don\u2019t review code at all! People may assume that a senior developer is experienced enough to not make any mistakes, or that merely changing those 3 lines of code couldn\u2019t possibly do any harm to the system. In these cases, it\u2019s not uncommon to skip the code review in order to cut some time. Unreviewed (or badly reviewed) code can be extremely dangerous, resulting in huge risks and unpredictable behavior.</p>\n<p>A survey says that, on average, developers spend 45% of their time fixing bugs and technical debt, when they could be developing new features instead. Defining simple guideline files, adopting certain behaviors and setting up repository configurations are steps that can increase manyfold the code review performance (in both time and quality). Using review tools both on server (e.g. Heroku Review Apps) and locally (e.g. linters) can also greatly increase the process\u2019 speed. Creating templates and checklists ensures no step is overlooked or forgotten. The list goes on, but enough spoilers for now. The attendees will learn specific tips, tools, processes and recommended practices that were compiled from research and real-life use cases (both from my experience and from big players like Django, Facebook, Instagram, Mozilla, etc), along with some survey data that demonstrates why reviewing code is important.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-29T20:54:57.847Z",
            "speaker": 36,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 43,
        "fields": {
            "kind": 1,
            "title": "You don\u2019t need deep learning to NLP",
            "description": "Learn about NLP without the deep learning hype. This talk will introduce natural language processing concepts and provide simple examples with useful python libraries. You will learn how to manipulate text, analyze words, parse strings, and search documents.",
            "abstract": "Deep learning gets all the hype, but there are many other ways to do natural language processing that are often simpler, faster, and just as effective. Python has some of the best libraries out there for analyzing, clustering, classifying, manipulating, and parsing strings and text. We will cover some of these libraries with simple examples, while introducing core concepts for natural language processing.",
            "abstract_html": "<p>Deep learning gets all the hype, but there are many other ways to do natural language processing that are often simpler, faster, and just as effective. Python has some of the best libraries out there for analyzing, clustering, classifying, manipulating, and parsing strings and text. We will cover some of these libraries with simple examples, while introducing core concepts for natural language processing.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-29T21:45:25.885Z",
            "speaker": 37,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 44,
        "fields": {
            "kind": 1,
            "title": "Notebooks with matplotlib to Make Non-Technical Clients Excited",
            "description": "Presenting computational work to clinicians is never easy, in a world of excel and powerpoint figures, I wanted to find a way to dynamically showcase some of the tools I was creating and their results. Jupyter Notebooks was my savior, and I want to share how it changed the way I do clinical research, and convey those results.",
            "abstract": "My clinical research presentations involved plugging matlab results into powerpoint and excel to create figures for clinicians and other stake holders to review results. I wanted to find a way to better reach my target audience, and have the ability to iteratively walk through the work flow of the experiments and toolsets. Enter Jupyter Notebooks, and matplot lib. I converted everything I was doing in matlab to python (thanks numpy and pandas) and have been off to the races since. \r\n\r\nI have noticed a strong uptick in participation from stakeholders since they are able to follow the research at an atomic level, and having the ability to annotate with markdown along the way only increases this buy in. \r\n\r\nPlus now I have the ability to organize my research into individual notebooks, and can easily export reports for funding agencies looking for project updates, and new research proposals.",
            "abstract_html": "<p>My clinical research presentations involved plugging matlab results into powerpoint and excel to create figures for clinicians and other stake holders to review results. I wanted to find a way to better reach my target audience, and have the ability to iteratively walk through the work flow of the experiments and toolsets. Enter Jupyter Notebooks, and matplot lib. I converted everything I was doing in matlab to python (thanks numpy and pandas) and have been off to the races since. </p>\n<p>I have noticed a strong uptick in participation from stakeholders since they are able to follow the research at an atomic level, and having the ability to annotate with markdown along the way only increases this buy in. </p>\n<p>Plus now I have the ability to organize my research into individual notebooks, and can easily export reports for funding agencies looking for project updates, and new research proposals.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-03-29T23:19:24.361Z",
            "speaker": 38,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 46,
        "fields": {
            "kind": 1,
            "title": "Machine Learning for Hotels",
            "description": "At Treebo hotels, we want to transform the budget segment hotels by providing best experience for our guests at the lowest price possible. We are a tech company at the core and build various products that solve our problems and reduce costs in different ways.",
            "abstract": "There are different places we use machine learning models:\r\n* Revenue Management System and Pricing\r\n* Quality of Hotels\r\n* Stay analysis\r\n\r\n## Revenue Management System\r\n* Dynamic pricing\r\n* Scale\r\n\r\n### Why can\u2019t we use something off the shelf?\r\n\r\nWe have given this a shot and explored available products in the market and most of them don\u2019t fit our requirement because:\r\n\r\n* Most of the systems provide very basic and provide analysis on 1 or 2 dimensional data \r\n* Are mostly rule based which often breaks our complex requirement\r\n\r\n### How we use machine learning to price our hotels?\r\n\r\nWe build very high dimensional features based on various signals like historical data, competitor data (reviews, rating etc), airline prices, seasonality information - events, holidays etc.\r\n\r\n## Quality of Hotels\r\nAfter price, the next important thing for a consumer in this space is quality of stay. We have our field agents using our quality management app that collect granular information of various issues during the quality audits. With these data points, we have a  predictive maintenance system to identify various issues that might crop up and hamper guest experience.\r\n\r\n## Stay Analysis\r\nTo analyse our guest experience, we use feedback from various channels and run different NLP techniques to understand common pain points and best experiences of guests at various properties.",
            "abstract_html": "<p>There are different places we use machine learning models:\n<em> Revenue Management System and Pricing\n</em> Quality of Hotels\n* Stay analysis</p>\n<h2>Revenue Management System</h2>\n<ul>\n<li>Dynamic pricing</li>\n<li>Scale</li>\n</ul>\n<h3>Why can\u2019t we use something off the shelf?</h3>\n<p>We have given this a shot and explored available products in the market and most of them don\u2019t fit our requirement because:</p>\n<ul>\n<li>Most of the systems provide very basic and provide analysis on 1 or 2 dimensional data </li>\n<li>Are mostly rule based which often breaks our complex requirement</li>\n</ul>\n<h3>How we use machine learning to price our hotels?</h3>\n<p>We build very high dimensional features based on various signals like historical data, competitor data (reviews, rating etc), airline prices, seasonality information - events, holidays etc.</p>\n<h2>Quality of Hotels</h2>\n<p>After price, the next important thing for a consumer in this space is quality of stay. We have our field agents using our quality management app that collect granular information of various issues during the quality audits. With these data points, we have a  predictive maintenance system to identify various issues that might crop up and hamper guest experience.</p>\n<h2>Stay Analysis</h2>\n<p>To analyse our guest experience, we use feedback from various channels and run different NLP techniques to understand common pain points and best experiences of guests at various properties.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-01T06:00:08.984Z",
            "speaker": 40,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 47,
        "fields": {
            "kind": 1,
            "title": "Lessons from Building an Enterprise SaaS Startup on Python",
            "description": "So you want to build an enterprise app using Python? There are a few particular enterprise concerns to consider, such as security, integrations, and the speed and flexibility of meeting customer requirements. In this talk, we\u2019ll share our lessons learned.",
            "abstract": "PlusPlus is an enterprise learning platform. It enables engineers to share knowledge at scale. Current customers include Airbnb, Netflix, and Salesforce. \r\n\r\nIn this talk, we\u2019ll explore the particular challenges of using Python for enterprise SaaS applications. We\u2019ll focus on enterprise security issues such as SSO, integrations with other systems like Workday and Google Apps, and speed and flexibility of iterations to meet specific customer needs without painting yourself to the wall.",
            "abstract_html": "<p>PlusPlus is an enterprise learning platform. It enables engineers to share knowledge at scale. Current customers include Airbnb, Netflix, and Salesforce. </p>\n<p>In this talk, we\u2019ll explore the particular challenges of using Python for enterprise SaaS applications. We\u2019ll focus on enterprise security issues such as SSO, integrations with other systems like Workday and Google Apps, and speed and flexibility of iterations to meet specific customer needs without painting yourself to the wall.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-04T01:14:34.263Z",
            "speaker": 41,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 48,
        "fields": {
            "kind": 1,
            "title": "Introduction to Deep Learning and TensorFlow",
            "description": "An introduction to Deep Learning (DL) concepts, including back propagation, activation functions, and CNNs, followed by an intro to Keras and TensorFlow.",
            "abstract": "A fast-paced introduction to Deep Learning (DL) concepts, starting with aspects of deep neural networks, such as back propagation, activation functions, and an overview of CNNs. Next, a quick introduction to TensorFlow and Tensorboard, and then some code samples with TensorFlow.\r\n\r\nFor best results, familiarity with basic vectors and matrices, inner (aka \"dot\") products of vectors, the notion of a derivative, and rudimentary Python is strongly recommended. If time permits, we'll look at the Universal Approximation Theorem.",
            "abstract_html": "<p>A fast-paced introduction to Deep Learning (DL) concepts, starting with aspects of deep neural networks, such as back propagation, activation functions, and an overview of CNNs. Next, a quick introduction to TensorFlow and Tensorboard, and then some code samples with TensorFlow.</p>\n<p>For best results, familiarity with basic vectors and matrices, inner (aka \"dot\") products of vectors, the notion of a derivative, and rudimentary Python is strongly recommended. If time permits, we'll look at the Universal Approximation Theorem.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-04T03:25:12.529Z",
            "speaker": 42,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 49,
        "fields": {
            "kind": 1,
            "title": "Down with pipeline debt! Introducing Great Expectations",
            "description": "Great Expectations is a framework that helps teams save time and promote analytic integrity with a new twist on automated testing: pipeline tests. Pipeline tests are applied to data (instead of code) and at batch time (instead of compile or deploy time).\r\n\r\nBy making it easy to bring pipelines under test, Great Expectations fosters discipline, confidence, and acceleration in data teams.",
            "abstract": "Data science and engineering have been missing out on one of the biggest productivity boosters in modern software development: automated testing. Without automated tests, data pipelines often become deep stacks of unverified assumptions. Mysterious (and sometimes embarrassing) bugs crop up more and more frequently, and resolving them requires painstaking exploration of upstream data, often leading to frustrating negotiations about data specs across teams.\r\n\r\nGreat Expectations is an open source Python framework for bringing data pipelines and products under test. Like assertions in traditional Python unit tests, expectations provide a flexible, declarative language for describing expected behavior. Unlike traditional unit tests, Great Expectations applies expectations to data instead of code. Great Expectations makes it easy to set up your testing framework early, capture findings while they\u2019re still fresh, and systematically validate new data against them.",
            "abstract_html": "<p>Data science and engineering have been missing out on one of the biggest productivity boosters in modern software development: automated testing. Without automated tests, data pipelines often become deep stacks of unverified assumptions. Mysterious (and sometimes embarrassing) bugs crop up more and more frequently, and resolving them requires painstaking exploration of upstream data, often leading to frustrating negotiations about data specs across teams.</p>\n<p>Great Expectations is an open source Python framework for bringing data pipelines and products under test. Like assertions in traditional Python unit tests, expectations provide a flexible, declarative language for describing expected behavior. Unlike traditional unit tests, Great Expectations applies expectations to data instead of code. Great Expectations makes it easy to set up your testing framework early, capture findings while they\u2019re still fresh, and systematically validate new data against them.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-04T04:14:33.270Z",
            "speaker": 43,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 50,
        "fields": {
            "kind": 1,
            "title": "Why database normalization matters in Django",
            "description": "To maintain a Django application, developers need to update the database design by adding and removing fields and tables. But it's easy to introduce changes that lead to data redundancy and inconsistencies because of database normalization violations. This talk shows Django-focused examples of how normalization helps with integrity, along with libraries to help when normalization isn't feasible.",
            "abstract": "Database normalization mandates how one should (re)structure a database in accordance with several \"normal forms\", i.e., sets of criteria that a database schema must follow. Normalization is a very useful process that helps to reduce data redundancy and improve data integrity. In fact, some normal forms were studied and proposed by the very creator of relational databases, E. F. Codd. Django developers, like any other developer that uses relational databases, should understand and apply first, second, third normal forms, among others. However, often developers don't know exactly how to do it due to the excess of formalism in normal forms definitions. This talk aims to remediate that by showing Django-focused examples of the most important normal forms, along with libraries and patterns to help when normalization isn't feasible.",
            "abstract_html": "<p>Database normalization mandates how one should (re)structure a database in accordance with several \"normal forms\", i.e., sets of criteria that a database schema must follow. Normalization is a very useful process that helps to reduce data redundancy and improve data integrity. In fact, some normal forms were studied and proposed by the very creator of relational databases, E. F. Codd. Django developers, like any other developer that uses relational databases, should understand and apply first, second, third normal forms, among others. However, often developers don't know exactly how to do it due to the excess of formalism in normal forms definitions. This talk aims to remediate that by showing Django-focused examples of the most important normal forms, along with libraries and patterns to help when normalization isn't feasible.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-04T19:01:49.355Z",
            "speaker": 33,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 53,
        "fields": {
            "kind": 1,
            "title": "Tasks: you gotta know how to run 'em, you gotta know how to safe  'em",
            "description": "How long is it acceptable to keep users waiting for a response? The ideal answer is: the minimum their internet connection allows. This means server process time should be close as possible to zero. In this talk you will learn what are async tasks, what they can do and what are the good practices.",
            "abstract": "Web developers often find themselves in situations where server processing takes longer than a user would accept. One very common situation is when sending emails. Although simple and relatively quick task, it requires the communication with an external service. In this situation, it's not possible to foresee how long that service will take to answer. Not to mention the many unexpected situations that can arise such as errors and bugs. The solution to this problem is to delegate long lasting tasks while responding quickly to the user. This is the point where we need async tasks. There are some tools available that can assist in this job in the Python ecosystem, Celery being the most popular. This talk will cover most of the basics on how an async task queue works and dive further on the best practices and how to handle errors, test and monitor those.\r\n\r\nRundown:\r\n\r\n- Setting the context (~2 min)\r\n- The architecture (~5 min)\r\n    - Brokers\r\n    - Workers\r\n- Use cases (~1 min)\r\n    - External calls\r\n    - Long computations\r\n    - Data caching\r\n- Tools available (~1 min)\r\n- Best practices (~16 min)\r\n    - Idempotency & Atomicity\r\n    - Error handling\r\n    - Monitoring\r\n    - Logging\r\n    - Tests and Debugging",
            "abstract_html": "<p>Web developers often find themselves in situations where server processing takes longer than a user would accept. One very common situation is when sending emails. Although simple and relatively quick task, it requires the communication with an external service. In this situation, it's not possible to foresee how long that service will take to answer. Not to mention the many unexpected situations that can arise such as errors and bugs. The solution to this problem is to delegate long lasting tasks while responding quickly to the user. This is the point where we need async tasks. There are some tools available that can assist in this job in the Python ecosystem, Celery being the most popular. This talk will cover most of the basics on how an async task queue works and dive further on the best practices and how to handle errors, test and monitor those.</p>\n<p>Rundown:</p>\n<ul>\n<li>Setting the context (~2 min)</li>\n<li>The architecture (~5 min)<ul>\n<li>Brokers</li>\n<li>Workers</li>\n</ul>\n</li>\n<li>Use cases (~1 min)<ul>\n<li>External calls</li>\n<li>Long computations</li>\n<li>Data caching</li>\n</ul>\n</li>\n<li>Tools available (~1 min)</li>\n<li>Best practices (~16 min)<ul>\n<li>Idempotency &amp; Atomicity</li>\n<li>Error handling</li>\n<li>Monitoring</li>\n<li>Logging</li>\n<li>Tests and Debugging</li>\n</ul>\n</li>\n</ul>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-05T21:16:24.713Z",
            "speaker": 30,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 54,
        "fields": {
            "kind": 1,
            "title": "No API, Web-Scrape Sigh",
            "description": "You would like to gather data on a project you are working on. However, there is no public API available to you. You want the information, and see it on generated tables in the website you are looking at. We see how we can get that information...now.",
            "abstract": "How to get the information you need with a bit of Data Munging with Beautiful Soup Python Library.",
            "abstract_html": "<p>How to get the information you need with a bit of Data Munging with Beautiful Soup Python Library.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-06T19:13:17.296Z",
            "speaker": 44,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 55,
        "fields": {
            "kind": 1,
            "title": "Testing C Libraries in Python with CFFI",
            "description": "Help bridge the testing gap between C/C++ developers and Python QA engineers, by using CFFI to wrap C libraries into testable Python packages. Find out how easy it can be to wrap C libraries for use in Python, and learn some of the pitfalls to avoid along the way.",
            "abstract": "In many industries, developers sometimes work on performance-critical library code exclusively in C/C++. This presents a problem when QA engineers only comfortable in Python are assigned to test the results. However, these days it is relatively easy to wrap C libraries in Python using CFFI. In addition to covering how to get started with CFFI, we will talk about how to work with a library that uses advanced data structures in its parameter list or requires dynamic memory to be allocated. Finally, we will focus on the differences between C-style APIs and Pythonic APIs, and why that distinction should be preserved when creating CFFI-wrapped Python packages.",
            "abstract_html": "<p>In many industries, developers sometimes work on performance-critical library code exclusively in C/C++. This presents a problem when QA engineers only comfortable in Python are assigned to test the results. However, these days it is relatively easy to wrap C libraries in Python using CFFI. In addition to covering how to get started with CFFI, we will talk about how to work with a library that uses advanced data structures in its parameter list or requires dynamic memory to be allocated. Finally, we will focus on the differences between C-style APIs and Pythonic APIs, and why that distinction should be preserved when creating CFFI-wrapped Python packages.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-07T00:26:38.803Z",
            "speaker": 45,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 56,
        "fields": {
            "kind": 1,
            "title": "Gradient Descent, Demystified",
            "description": "The audience and I walk through a live coding practicum (in a RISE Jupyter Notebook slideshow) in which I implement an initial gradient descent algorithm for logistic and linear regression, demonstrating the flexibility of the optimization technique and the decidedly un-scary code required to get our prototype up-and-running.",
            "abstract": "Gradient descent (GD) is a fundamental optimization algorithm that sounds much scarier than it is. Many users of Scikit-learn et al. can apply GD through these tools, but do not `grok` what GD is really doing. Other more engineering-oriented practitioners are put off entirely by the seeming complexity. I walk through a live coding practicum (in a RISE Jupyter Notebook slideshow) in which I implement an initial gradient descent algorithm for logistic and linear regression, demonstrating the flexibility of the optimization technique and the decidedly un-scary code required to get our prototype up-and-running. I compare the results of our hand coded algorithm to those generated by Scikit-learn (and the closed-form normal equation, for linear regression) and show equality. \r\n\r\nThe focus of this talk is on the practicum of implementation one\u2019s own GD algorithm, though I review the most important mathematical and theoretical components of GD to ground the practicum for attendees. Mathematical review touches on the nature of gradients, what they are, how they relate to derivates, and how they enable iterative optimization over a parameter space. This talk does not include a formal derivation of, eg, the loss functions used in linear and logistic regression, nor does it require mastery of calculus. Attendees will leave the talk with a better understanding of iterative optimization and a template of their own for implementing GD in Python, should they feel this would enrich their understanding.\r\n\r\nThe live-coding setup has two major advantages over static slides + disquisition. First, it is more engaging to participants. Second, it reinforces the central tenant of the talk: that GD is much much less intimidating than it seems, and indeed is implementable in quite few lines of code.\r\n\r\nMany tutorials on gradient descent (1) make only a cursory nod towards the underlying math, or (2) start with something tangentially related to \u2014 and far removed from \u2014 the algorithm, eg, an MLE derivation of the logistic regression loss function. But the fundamental iterative nature of the algorithm is intuitive and unfussy: by starting with this iterative intuition before fleshing out *how* one arrives at this mechanism participants should be able to understand the mathematical components of the algorithm in broader context, rather than in a vacuum.\r\n\r\nPracticum participants are aware that Iterative solutions are often computationally expensive. I review, also, closed-form solutions (the normal equation) to linear regression and explain the benefits of GD over the seemingly more straight-forward closed-form model. Perhaps the main reason that gradient descent remains relevant in industry is its flexibility: the second largest point of emphasis (after the compactness and elegance of the algorithm) is the clear parallel between the GD implementation for linear and logistic regression. \r\n\r\nSeeing is believing. A final benefit of live-coding GD is that we can compare our hand coded algorithm with the implementation users know and love from Scikit-learn. Scitkit-learn source code can be inscrutable to those note well-versed in object oriented python: by showing (rough) algorithmic equivalence participants may better convince themselves that directly inspecting Scikit source is within reach.",
            "abstract_html": "<p>Gradient descent (GD) is a fundamental optimization algorithm that sounds much scarier than it is. Many users of Scikit-learn et al. can apply GD through these tools, but do not <code>grok</code> what GD is really doing. Other more engineering-oriented practitioners are put off entirely by the seeming complexity. I walk through a live coding practicum (in a RISE Jupyter Notebook slideshow) in which I implement an initial gradient descent algorithm for logistic and linear regression, demonstrating the flexibility of the optimization technique and the decidedly un-scary code required to get our prototype up-and-running. I compare the results of our hand coded algorithm to those generated by Scikit-learn (and the closed-form normal equation, for linear regression) and show equality. </p>\n<p>The focus of this talk is on the practicum of implementation one\u2019s own GD algorithm, though I review the most important mathematical and theoretical components of GD to ground the practicum for attendees. Mathematical review touches on the nature of gradients, what they are, how they relate to derivates, and how they enable iterative optimization over a parameter space. This talk does not include a formal derivation of, eg, the loss functions used in linear and logistic regression, nor does it require mastery of calculus. Attendees will leave the talk with a better understanding of iterative optimization and a template of their own for implementing GD in Python, should they feel this would enrich their understanding.</p>\n<p>The live-coding setup has two major advantages over static slides + disquisition. First, it is more engaging to participants. Second, it reinforces the central tenant of the talk: that GD is much much less intimidating than it seems, and indeed is implementable in quite few lines of code.</p>\n<p>Many tutorials on gradient descent (1) make only a cursory nod towards the underlying math, or (2) start with something tangentially related to \u2014 and far removed from \u2014 the algorithm, eg, an MLE derivation of the logistic regression loss function. But the fundamental iterative nature of the algorithm is intuitive and unfussy: by starting with this iterative intuition before fleshing out <em>how</em> one arrives at this mechanism participants should be able to understand the mathematical components of the algorithm in broader context, rather than in a vacuum.</p>\n<p>Practicum participants are aware that Iterative solutions are often computationally expensive. I review, also, closed-form solutions (the normal equation) to linear regression and explain the benefits of GD over the seemingly more straight-forward closed-form model. Perhaps the main reason that gradient descent remains relevant in industry is its flexibility: the second largest point of emphasis (after the compactness and elegance of the algorithm) is the clear parallel between the GD implementation for linear and logistic regression. </p>\n<p>Seeing is believing. A final benefit of live-coding GD is that we can compare our hand coded algorithm with the implementation users know and love from Scikit-learn. Scitkit-learn source code can be inscrutable to those note well-versed in object oriented python: by showing (rough) algorithmic equivalence participants may better convince themselves that directly inspecting Scikit source is within reach.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-07T19:17:02.509Z",
            "speaker": 46,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 57,
        "fields": {
            "kind": 1,
            "title": "Taming Irreversibility with Feature Flags",
            "description": "Feature flags is a technique to make easy to toggle on/off a feature in production without touching the code base. If well implemented it can improve a lot the git flow and give the developers some peace of mind. This talk is going to show what are the main concerns on implementing feature flags in your projects and the best practices.",
            "abstract": "Continuous delivery has gained a lot of popularity in software development teams lately. Nowadays we have a lot of tools available to make this practice easier to implement, things like test tools, linters, continuous integration services, continuous deployment hooks etc.\r\n\r\nOne of the main gains from those tools is the ability for shorter delivery cycles, which makes deployments more of a day to day activity than a scary one-time event. This mindset change helps teams build better and more robust software at the same time that provides a safer environment [and peace of mind]. But short cycles are no silver bullet. For them to properly be implemented no branch should live too long away from master and this means that incomplete features should go into production from time to time. They also do not guarantee bug-free software or that features correctly satisfy user needs.\r\n\r\nFeature flags is a design that comes not only to help buggy or incomplete features to be easily deactivated, but also to allow us to test if our features are really improving the user experience. This improves the development process by allowing developers to sync the code more often (even with incomplete features), and to rapidly react to production bugs, reducing the team\u2019s stress in crisis.",
            "abstract_html": "<p>Continuous delivery has gained a lot of popularity in software development teams lately. Nowadays we have a lot of tools available to make this practice easier to implement, things like test tools, linters, continuous integration services, continuous deployment hooks etc.</p>\n<p>One of the main gains from those tools is the ability for shorter delivery cycles, which makes deployments more of a day to day activity than a scary one-time event. This mindset change helps teams build better and more robust software at the same time that provides a safer environment [and peace of mind]. But short cycles are no silver bullet. For them to properly be implemented no branch should live too long away from master and this means that incomplete features should go into production from time to time. They also do not guarantee bug-free software or that features correctly satisfy user needs.</p>\n<p>Feature flags is a design that comes not only to help buggy or incomplete features to be easily deactivated, but also to allow us to test if our features are really improving the user experience. This improves the development process by allowing developers to sync the code more often (even with incomplete features), and to rapidly react to production bugs, reducing the team\u2019s stress in crisis.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-07T21:05:12.280Z",
            "speaker": 47,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 58,
        "fields": {
            "kind": 1,
            "title": "Journey to the center of the Python",
            "description": "What happens when you type print('hello')? Is it magic? Gremlins? Let's find out!",
            "abstract": "We've probably all used Python at some point or the other. But how does Python work? We take a journey through common Python code and how they get executed.",
            "abstract_html": "<p>We've probably all used Python at some point or the other. But how does Python work? We take a journey through common Python code and how they get executed.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-08T07:22:45.703Z",
            "speaker": 48,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 59,
        "fields": {
            "kind": 1,
            "title": "The grass is always greener",
            "description": "We might all agree that Python is pretty good. Let's look elsewhere to see how it could be even better.",
            "abstract": "After a long time of staying with one set of tools, one becomes accustomed to its warts and faults. Thus, we grow to neglect our most used tools and suffer for it.\r\n\r\nLet's look at what others are doing, and see if we can take inspiration from it.",
            "abstract_html": "<p>After a long time of staying with one set of tools, one becomes accustomed to its warts and faults. Thus, we grow to neglect our most used tools and suffer for it.</p>\n<p>Let's look at what others are doing, and see if we can take inspiration from it.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-08T07:22:49.750Z",
            "speaker": 48,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 60,
        "fields": {
            "kind": 1,
            "title": "git inexplicable terminology explained",
            "description": "Or how to use git and know what you're doing\u2014AT THE SAME TIME!",
            "abstract": "Git has become the tool of choice for many projects to version their projects with, and yet it is a major source of confusion for developers.\r\n\r\nLet's clarify git commands by going over how it works so you can speak its language.",
            "abstract_html": "<p>Git has become the tool of choice for many projects to version their projects with, and yet it is a major source of confusion for developers.</p>\n<p>Let's clarify git commands by going over how it works so you can speak its language.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-08T07:22:54.610Z",
            "speaker": 48,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 61,
        "fields": {
            "kind": 1,
            "title": "Deprecating the state machine: building conversational AI with the Rasa stack",
            "description": "Rasa NLU & Rasa Core are the leading open source libraries for building machine learning-based chatbots and voice assistants. In this live-coding workshop you will learn the fundamentals of conversational AI and how to build your own using the Rasa Stack.",
            "abstract": "There's a large body of research on machine learning-based dialogue, but most voice and chat systems in production are still implemented using a state machine and a set of rules.\r\n\r\nRasa NLU & Rasa Core are the leading open source libraries for building machine learning-based chatbots and voice assistants. \r\n\r\nIn this workshop we will live-code a useful, engaging conversational AI bot based entirely on machine learning. We will start with language understanding, bootstrapping from very little annotated training data. You will then go on to build up your bot's ability to handle increasingly complex dialogues through supervised and interactive learning.",
            "abstract_html": "<p>There's a large body of research on machine learning-based dialogue, but most voice and chat systems in production are still implemented using a state machine and a set of rules.</p>\n<p>Rasa NLU &amp; Rasa Core are the leading open source libraries for building machine learning-based chatbots and voice assistants. </p>\n<p>In this workshop we will live-code a useful, engaging conversational AI bot based entirely on machine learning. We will start with language understanding, bootstrapping from very little annotated training data. You will then go on to build up your bot's ability to handle increasingly complex dialogues through supervised and interactive learning.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-08T16:27:14.066Z",
            "speaker": 49,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 62,
        "fields": {
            "kind": 1,
            "title": "Finding Vulnerabilities for Free: The Magic of Static Analysis",
            "description": "Are you a website or application developer? Interested about security or compilers?\r\n\r\nIn this talk we will walk through how to automatically find vulnerabilities in web applications with static analysis. Most of what will be presented here is based on a tool called PyT already open-source on GitHub. This talk will lightly cover topics such as web application security and data-flow analysis.",
            "abstract": "Many vulnerability classes in web applications share the same pattern of something coming from a 'source' (HTTP request) and eventually getting put in a 'sink' (SQL query), through the power of data-flow analysis, we can easily find them. This talk will walk through the architecture, techniques and past evaluations of an open-source security static analysis tool available at at https://github.com/python-security/pyt We will also talk about alternative approaches and more advanced techniques for reducing false-positives.",
            "abstract_html": "<p>Many vulnerability classes in web applications share the same pattern of something coming from a 'source' (HTTP request) and eventually getting put in a 'sink' (SQL query), through the power of data-flow analysis, we can easily find them. This talk will walk through the architecture, techniques and past evaluations of an open-source security static analysis tool available at at https://github.com/python-security/pyt We will also talk about alternative approaches and more advanced techniques for reducing false-positives.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-08T21:04:06.424Z",
            "speaker": 50,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 63,
        "fields": {
            "kind": 1,
            "title": "De Novo Design of Molecules with Deep Learning",
            "description": "Finding molecules with desirable properties is important with a variety of applications from drug discovery in pharma to agriculture. Methods such as density functional theory have long done a reasonable job helping scientist design molecules, however recently DL and its ability to learn complex generating distributions is showing it could also also be a strong tool for molecule design.",
            "abstract": "Deep learning in the form of convolutional neural nets and recurrent neural nets have made huge inroads in image process and natural language processing, respectively. However, recently deep learning has been making more and more progress in \"hard\" sciences. In this talk I'll review advances in one area of hard science: chemistry. And in particular, how deep learning can be used to help design and create molecules of interest.\r\n\r\nFirst we'll review some of the types of models that are showing up in the literature: sequence based models potentially with reinforcement learning to guild the sequence generation and models that approach the the problem using graph based techniques such as a graph convolutional network. Once there is a generative model for molecule creation, the next task is to tune the generation \u2014 search the molecule space \u2014 for molecules with certain desirable properties, e.g. the binding affinity of a molecule to a receptor (how ibuprofen works). Finally, we'll review some ancillary topics such as do a survey of available datasets for model training and evaluation, some feature creation methods for preparing molecules for DL models, and useful python libraries for molecule design.",
            "abstract_html": "<p>Deep learning in the form of convolutional neural nets and recurrent neural nets have made huge inroads in image process and natural language processing, respectively. However, recently deep learning has been making more and more progress in \"hard\" sciences. In this talk I'll review advances in one area of hard science: chemistry. And in particular, how deep learning can be used to help design and create molecules of interest.</p>\n<p>First we'll review some of the types of models that are showing up in the literature: sequence based models potentially with reinforcement learning to guild the sequence generation and models that approach the the problem using graph based techniques such as a graph convolutional network. Once there is a generative model for molecule creation, the next task is to tune the generation \u2014 search the molecule space \u2014 for molecules with certain desirable properties, e.g. the binding affinity of a molecule to a receptor (how ibuprofen works). Finally, we'll review some ancillary topics such as do a survey of available datasets for model training and evaluation, some feature creation methods for preparing molecules for DL models, and useful python libraries for molecule design.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-08T22:53:24.332Z",
            "speaker": 51,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 64,
        "fields": {
            "kind": 1,
            "title": "Consumption is Fractal: Open Source Sustainability",
            "description": "We all rely on open-source for just about everything in tech, but the past few years have shown just how precarious most of the ecosystem is. More people need to know about how to both support projects they rely on and run their own projects more sustainably.",
            "abstract": "While open-source sustainability has been a common topic in the hallways tracks and dev channels of major projects for years, the Heartbleed vulnerability in 2014 catapulted the topic into the spotlight. Since then we've seen many discussions about how open-source should be supported for the long term, but rarely much action beyond a few token donations and some long Twitter threads. In this talk we'll look at an overview of what kinds of projects are out there, how sustainability works in real terms, and what techniques have worked or not worked over the years.",
            "abstract_html": "<p>While open-source sustainability has been a common topic in the hallways tracks and dev channels of major projects for years, the Heartbleed vulnerability in 2014 catapulted the topic into the spotlight. Since then we've seen many discussions about how open-source should be supported for the long term, but rarely much action beyond a few token donations and some long Twitter threads. In this talk we'll look at an overview of what kinds of projects are out there, how sustainability works in real terms, and what techniques have worked or not worked over the years.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-09T02:15:49.729Z",
            "speaker": 52,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 65,
        "fields": {
            "kind": 1,
            "title": "Docker for Data Scientists: Simplify your workflow and avoid pitfalls",
            "description": "These days, DevOps folks live and breath containers, especially Docker and related technologies. As a Data Scientist, you may have heard about Docker, but are less interested in investing the time to become an expert, since it is not core to your job. In this talk, you will get just enough Docker knowledge to improve your data science workflow and avoid common pitfalls.",
            "abstract": "The day-to-day concerns of Data Scientists and DevOps Engineers can be very different. In this talk, we\u2019ll show how Docker, a technology from the DevOps world, can improve the lives of Data Scientists. In particular, Docker can dramatically simplify the configuration of your programming/analysis environment, facilitate sharing your work with colleagues, and lead to reproducible workflows and results. On the other hand, there are pitfalls with Docker that you will want to avoid.\r\n\r\nWe will cover container and Docker concepts, running Docker, using Docker with GPUs, and best practices for data science with Docker. You will see concrete examples of how to use Docker in Python-focused environments, including interactive REPL and script development using Anaconda Python and scikit-learn, web environments such as Jupyter and TensorBoard, and Nvidia\u2019s DIGITS. Along the way, we will point out common pitfalls to avoid and better approaches.\r\n\r\nThis talk is aimed at intermediate to advanced Data Scientists. Some familiarity with a Linux/Mac command line is assumed.",
            "abstract_html": "<p>The day-to-day concerns of Data Scientists and DevOps Engineers can be very different. In this talk, we\u2019ll show how Docker, a technology from the DevOps world, can improve the lives of Data Scientists. In particular, Docker can dramatically simplify the configuration of your programming/analysis environment, facilitate sharing your work with colleagues, and lead to reproducible workflows and results. On the other hand, there are pitfalls with Docker that you will want to avoid.</p>\n<p>We will cover container and Docker concepts, running Docker, using Docker with GPUs, and best practices for data science with Docker. You will see concrete examples of how to use Docker in Python-focused environments, including interactive REPL and script development using Anaconda Python and scikit-learn, web environments such as Jupyter and TensorBoard, and Nvidia\u2019s DIGITS. Along the way, we will point out common pitfalls to avoid and better approaches.</p>\n<p>This talk is aimed at intermediate to advanced Data Scientists. Some familiarity with a Linux/Mac command line is assumed.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-09T04:51:23.124Z",
            "speaker": 53,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 66,
        "fields": {
            "kind": 1,
            "title": "Zebras and Lasers: A crash course on barcodes with Python",
            "description": "Factory automation and logistics are industries full of software and full of technologies that seem simple but aren't. This talk is a Python-filled deep dive into the surprisingly complex world of the humble barcode, a true workhorse at the interface between software and the physical world. There will be shipping labels, sidewalk mosaics and even DNA!",
            "abstract": "Because software is nothing but electrons in a box, writing code that deals with tangible objects in the physical world is tricky. To make things easier to identify by software, we have sprinkled our world with machine-readable artifacts like RFID tags and magstripes. The most popular such tag is the barcode, clocking in at an estimated 5 billion barcode reads per day!\r\n\r\nDespite being the ubiquitous interface between the worlds of hardware and software, barcodes rarely get much attention from engineers on either side of software/hardware divide. Only few barcode connoisseurs geek out about the nuanced benefits of the many different barcode symbologies and hundreds of industry-specific data encodings. This talk is a guided tour through the wonderful world of barcodes, curated especially for Pythonistas.\r\n\r\nI will start with a live demo of how to read a barcode with nothing but pen, paper, and a few list comprehensions, followed by a brief barcoding theory 101. After we\u2019ve seen the nuts and bolts, I will go over a brief survey of the packages and tools for generating and reading barcodes that are available to Python developers. We\u2019ll finish with a few application examples, ranging from interesting to mischievous.\r\n\r\nWarning: There might be lasers (but probably no zebras).",
            "abstract_html": "<p>Because software is nothing but electrons in a box, writing code that deals with tangible objects in the physical world is tricky. To make things easier to identify by software, we have sprinkled our world with machine-readable artifacts like RFID tags and magstripes. The most popular such tag is the barcode, clocking in at an estimated 5 billion barcode reads per day!</p>\n<p>Despite being the ubiquitous interface between the worlds of hardware and software, barcodes rarely get much attention from engineers on either side of software/hardware divide. Only few barcode connoisseurs geek out about the nuanced benefits of the many different barcode symbologies and hundreds of industry-specific data encodings. This talk is a guided tour through the wonderful world of barcodes, curated especially for Pythonistas.</p>\n<p>I will start with a live demo of how to read a barcode with nothing but pen, paper, and a few list comprehensions, followed by a brief barcoding theory 101. After we\u2019ve seen the nuts and bolts, I will go over a brief survey of the packages and tools for generating and reading barcodes that are available to Python developers. We\u2019ll finish with a few application examples, ranging from interesting to mischievous.</p>\n<p>Warning: There might be lasers (but probably no zebras).</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-09T06:58:07.707Z",
            "speaker": 54,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 67,
        "fields": {
            "kind": 1,
            "title": "Everything you need to know about time series forecasting",
            "description": "The analysis of time series data can be essential when making strategic decisions under uncertainty, it being a fundamental part within and outside of the field of computer science. This presentation discusses the tradeoff between statistical models (represented by the ARIMA model) and neural network-based techniques. It also demonstrates how to apply these techniques to a real problem.",
            "abstract": "When making strategic decisions under uncertainty, we all make forecasts. In situations where time and money are directly related especially. The analysis of time series data can be essential in achieving good results, it being a fundamental part within and outside of the field of computer science. In fact, time series are everywhere. They surround the unspoken mysteries of our existence, from forecasting the amount of rain that pours onto a river per year, to the big stock markets, to weekly company sales, just to name a few.\r\n\r\nThis presentation discusses the tradeoff between statistical models and neural network-based techniques, the later receiving a lot of attention in the data science community in the past few years. This talk also demonstrates how to apply them to a real problem: How to forecast a high-risk asset, which  price can unpredictably increase or decrease over a short period of time, that can also be influenced by a wide range of factors. In other words, what\u2019s gonna be the bitcoin's price? Lastly, this piece compares the advantages and disadvantages of these methods.",
            "abstract_html": "<p>When making strategic decisions under uncertainty, we all make forecasts. In situations where time and money are directly related especially. The analysis of time series data can be essential in achieving good results, it being a fundamental part within and outside of the field of computer science. In fact, time series are everywhere. They surround the unspoken mysteries of our existence, from forecasting the amount of rain that pours onto a river per year, to the big stock markets, to weekly company sales, just to name a few.</p>\n<p>This presentation discusses the tradeoff between statistical models and neural network-based techniques, the later receiving a lot of attention in the data science community in the past few years. This talk also demonstrates how to apply them to a real problem: How to forecast a high-risk asset, which  price can unpredictably increase or decrease over a short period of time, that can also be influenced by a wide range of factors. In other words, what\u2019s gonna be the bitcoin's price? Lastly, this piece compares the advantages and disadvantages of these methods.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-09T14:00:59.084Z",
            "speaker": 55,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 69,
        "fields": {
            "kind": 1,
            "title": "Diving into Production Issues at Scale",
            "description": "Using a real production war story, this talk will highlight some of the thoughts, techniques, and approaches to troubleshooting production python at scale.",
            "abstract": "Problems with single hosts are challenging enough. Scaling up to hundreds or thousands of running hosts only multiplies the problems. However, troubleshooting and remediating production issues at scale can also be much easier to deal with than issues on smaller installations.\r\n\r\nServices written in python can be more apt to encounter certain problems and lend themselves to certain solutions as well. In this talk, we will explore a real production issue or two around a python application to highlight some sound techniques and approaches to handling services at scale.\r\n\r\nWhile working through the narrative of the problem, we will explore some specific how-tos from a simple level, such as reading logs, to more complicated things pertaining the overall state of the runtime environment.",
            "abstract_html": "<p>Problems with single hosts are challenging enough. Scaling up to hundreds or thousands of running hosts only multiplies the problems. However, troubleshooting and remediating production issues at scale can also be much easier to deal with than issues on smaller installations.</p>\n<p>Services written in python can be more apt to encounter certain problems and lend themselves to certain solutions as well. In this talk, we will explore a real production issue or two around a python application to highlight some sound techniques and approaches to handling services at scale.</p>\n<p>While working through the narrative of the problem, we will explore some specific how-tos from a simple level, such as reading logs, to more complicated things pertaining the overall state of the runtime environment.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-09T17:39:50.291Z",
            "speaker": 57,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 70,
        "fields": {
            "kind": 1,
            "title": "GPU Dataframe Python API for accelerating Deep Learning Pipeline",
            "description": "We\u2019ll explore seamless analytics on GPUs by extracting data from MapD core, preprocess in pygdf, train, and predict with CNTK all by keeping the data in the GPU buffer. With Apache Arrow, an efficient data interchange is created by MapD and pygdf to leverage ML and deep learning tools. Performing analysis on GPU eliminates the latency implications caused by copying data onto host memory.",
            "abstract": "Use of the humble GPU has spiked over the past couple years as machine learning and data analytics workloads have been optimized to take advantage of the GPU\u2019s parallelism and memory bandwidth. Even though these operations (the steps of the Machine Learning Pipeline) could all be run on the same GPUs, they were typically isolated, and much slower than they needed to be, because data was serialized and deserialized between the steps over PCIe.\r\n\r\nThat inefficiency was addressed by the formation of the GPU Open Analytics Initiative (GOAI http://gpuopenanalytics.com/), an industry standard founded by MapD, Anaconda, and H2O.ai. MapD and Anaconda, another GOAi founding member, are involved in development of pythonic clients such as pymapd (interface to MapD's SQL engine supporting DBAPI 2.0), pygdf (Python interface to access and manipulate the GPU Dataframe) along with our core platform modules MapD Core SQL engine and MapD Immerse, visual analytics tool.",
            "abstract_html": "<p>Use of the humble GPU has spiked over the past couple years as machine learning and data analytics workloads have been optimized to take advantage of the GPU\u2019s parallelism and memory bandwidth. Even though these operations (the steps of the Machine Learning Pipeline) could all be run on the same GPUs, they were typically isolated, and much slower than they needed to be, because data was serialized and deserialized between the steps over PCIe.</p>\n<p>That inefficiency was addressed by the formation of the GPU Open Analytics Initiative (GOAI http://gpuopenanalytics.com/), an industry standard founded by MapD, Anaconda, and H2O.ai. MapD and Anaconda, another GOAi founding member, are involved in development of pythonic clients such as pymapd (interface to MapD's SQL engine supporting DBAPI 2.0), pygdf (Python interface to access and manipulate the GPU Dataframe) along with our core platform modules MapD Core SQL engine and MapD Immerse, visual analytics tool.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-09T17:40:08.949Z",
            "speaker": 58,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 72,
        "fields": {
            "kind": 1,
            "title": "Unlocking the power of Django Admin",
            "description": "Django Admin is a commonly used library yet most Djangonauts don't know about its full potential. In this talk, we explore the hidden features in Django Admin to build a simple CRM clone.",
            "abstract": "Django Admin is a commonly used library yet most Djangonauts don't know about its full potential. In this talk, we explore the hidden features in Django Admin such as actions, filtering, and essential plugins from the broader django admin ecosystem. We then use that knowledge to build a simple CRM that performs lead tracking, reporting & analytics on deals, and basic task management.",
            "abstract_html": "<p>Django Admin is a commonly used library yet most Djangonauts don't know about its full potential. In this talk, we explore the hidden features in Django Admin such as actions, filtering, and essential plugins from the broader django admin ecosystem. We then use that knowledge to build a simple CRM that performs lead tracking, reporting &amp; analytics on deals, and basic task management.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-09T21:44:17.614Z",
            "speaker": 60,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 73,
        "fields": {
            "kind": 1,
            "title": "The Alliance: an employer-employee relationship of trust and allignment",
            "description": "We will present the basic concepts behind \"The Alliance\" framework, the problem it's trying to solve and the proposed changes you should apply to run your business under this more realistic framework.\r\n\r\nThe Alliance not only works for mature companies like LinkedIn or Microsoft, we've built our company from the ground up using it and we think you should too.",
            "abstract": "The employer-employee relationship is broken, and managers face a seemingly impossible dilemma: the old model of guaranteed long-term employment no longer works in a business environment defined by continuous change, but neither does a system in which every employee acts like a free agent.\r\n\r\n\r\nThe employer-employee relationship is broken, and managers face a seemingly impossible dilemma: the old model of guaranteed long-term employment no longer works in a business environment defined by continuous change, but neither does a system in which every employee acts like a free agent.\r\n\r\nThe solution? Stop thinking of employees as either family or as free agents. Think of them instead as allies.\r\n\r\nIn this talk, I present you with the basic concepts behind The Alliance, the problem it's trying to solve and the proposed changes you should apply to run your business under this more realistic framework.",
            "abstract_html": "<p>The employer-employee relationship is broken, and managers face a seemingly impossible dilemma: the old model of guaranteed long-term employment no longer works in a business environment defined by continuous change, but neither does a system in which every employee acts like a free agent.</p>\n<p>The employer-employee relationship is broken, and managers face a seemingly impossible dilemma: the old model of guaranteed long-term employment no longer works in a business environment defined by continuous change, but neither does a system in which every employee acts like a free agent.</p>\n<p>The solution? Stop thinking of employees as either family or as free agents. Think of them instead as allies.</p>\n<p>In this talk, I present you with the basic concepts behind The Alliance, the problem it's trying to solve and the proposed changes you should apply to run your business under this more realistic framework.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-09T21:49:11.634Z",
            "speaker": 61,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 74,
        "fields": {
            "kind": 1,
            "title": "Site Reliability Engineering: Taking DevOps to the next level",
            "description": "In this talk, we cover the main problems affecting organizations trying to move faster in an environment of increased operational complexity, the different approaches that exist to address them and why we believe Google's SRE model is the most mature and reproducible option out there.",
            "abstract": "With the relentless progression towards the cloud, and the recent explosion of Kubernetes, Docker and other serverless strategies, operational complexity for the average company has gone through the roof.\r\n\r\nMost organizations have found themselves stuck in a place where their team's skillsets and culture have become a blocker to adopt best practices and move faster.\r\n\r\nIn this talk, we cover the main problems affecting organizations under these conditions, the different approaches that exist to address them and why we believe Google's SRE model is the most mature and reproducible option out there.",
            "abstract_html": "<p>With the relentless progression towards the cloud, and the recent explosion of Kubernetes, Docker and other serverless strategies, operational complexity for the average company has gone through the roof.</p>\n<p>Most organizations have found themselves stuck in a place where their team's skillsets and culture have become a blocker to adopt best practices and move faster.</p>\n<p>In this talk, we cover the main problems affecting organizations under these conditions, the different approaches that exist to address them and why we believe Google's SRE model is the most mature and reproducible option out there.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-09T21:57:24.652Z",
            "speaker": 61,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 75,
        "fields": {
            "kind": 1,
            "title": "Datmo Snapshots: Open Source Protocol for AI Model Tracking and Reproducibility in Python",
            "description": "While traditional version control works great for software project management, data science, AI is an entirely different beast altogether! Learn how we used Python to build the Datmo open model versioning protocol (Datmo Snapshots) to solve for current AI/ML developer experience pain points in tracking code, data, environments, metrics, and configurations all in one place.",
            "abstract": "While traditional version control works great for software project management, data science, AI is an entirely different beast altogether! Learn how we used Python to build the Datmo open model versioning protocol (Datmo Snapshots) to solve for current AI/ML developer experience pain points in tracking code, data, environments, metrics, and configurations all in one place. Whether you\u2019re a developer who\u2019s experienced the common struggles of trying to work with AI/ML models, or a DevOps engineer looking to learn lessons about how we use Python to wrangle all of these moving parts for productionalizing AI/ML, this talk is for you!\r\n\r\nCome to learn that you\u2019re not alone in dealing with all of these major problems, and that the result of the work we do at Datmo is to help make your life as an engineer in the AI space a little bit easier. After seeing the open Datmo protocol, we hope others can walk away with ideas of how to leverage Snapshots as a fundamental building block which they can build upon within their own AI or ML pipelines for model tracking, versioning, and deployment.",
            "abstract_html": "<p>While traditional version control works great for software project management, data science, AI is an entirely different beast altogether! Learn how we used Python to build the Datmo open model versioning protocol (Datmo Snapshots) to solve for current AI/ML developer experience pain points in tracking code, data, environments, metrics, and configurations all in one place. Whether you\u2019re a developer who\u2019s experienced the common struggles of trying to work with AI/ML models, or a DevOps engineer looking to learn lessons about how we use Python to wrangle all of these moving parts for productionalizing AI/ML, this talk is for you!</p>\n<p>Come to learn that you\u2019re not alone in dealing with all of these major problems, and that the result of the work we do at Datmo is to help make your life as an engineer in the AI space a little bit easier. After seeing the open Datmo protocol, we hope others can walk away with ideas of how to leverage Snapshots as a fundamental building block which they can build upon within their own AI or ML pipelines for model tracking, versioning, and deployment.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-09T23:36:28.053Z",
            "speaker": 62,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 76,
        "fields": {
            "kind": 1,
            "title": "Getting started with Deep Learning: Using Keras & Numpy to detect voice disorders",
            "description": "This talk is for Python programmers who want to learn how to use Keras to get started with deep learning. The audience should expect to learn what deep learning is, develop an intuitive understanding of how it works, and learn how to avoid some common mistakes. All of this is done via a recurring example, using utterance data to determine whether a medical patient might have a voice disorder.",
            "abstract": "Deep learning is a useful tool for problems in computer vision, natural language processing, and medicine. While it might seem difficult to get started in deep learning, Python libraries, such as Keras make deep learning quite accessible. In this talk, we will discuss what deep learning is, introduce NumPy and Keras, and discuss common mistakes and debugging strategies. Throughout the talk, we will return to an example project in the medical domain, which used deep learning on vocal data to determine whether a patient has a voice disorder called vocal hyperfunction.",
            "abstract_html": "<p>Deep learning is a useful tool for problems in computer vision, natural language processing, and medicine. While it might seem difficult to get started in deep learning, Python libraries, such as Keras make deep learning quite accessible. In this talk, we will discuss what deep learning is, introduce NumPy and Keras, and discuss common mistakes and debugging strategies. Throughout the talk, we will return to an example project in the medical domain, which used deep learning on vocal data to determine whether a patient has a voice disorder called vocal hyperfunction.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-09T23:37:42.918Z",
            "speaker": 63,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 77,
        "fields": {
            "kind": 1,
            "title": "Tools to manage large Python codebases",
            "description": "In this talk, you will learn about a variety of open source projects that will help you maintain & scale Python codebases. We will dive deep into best practices surrounding these tools and how you can set them up in new & existing Python codebases.",
            "abstract": "Managing a large Python codebase can be difficult. Without the right tools setup, many projects find it difficult to maintain a high level of code quality with few bugs.\r\n\r\nIn this talk, you will learn about a variety of open source projects that will help you maintain & scale Python codebases. Tools discussed include isort, flake8, pip-tools, Coverage.py, pre-commit, python-dotenv & more.",
            "abstract_html": "<p>Managing a large Python codebase can be difficult. Without the right tools setup, many projects find it difficult to maintain a high level of code quality with few bugs.</p>\n<p>In this talk, you will learn about a variety of open source projects that will help you maintain &amp; scale Python codebases. Tools discussed include isort, flake8, pip-tools, Coverage.py, pre-commit, python-dotenv &amp; more.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T00:57:41.828Z",
            "speaker": 64,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 78,
        "fields": {
            "kind": 1,
            "title": "Bootstrapping a Visual Search Engine",
            "description": "With the rise of deep learning, computer vision tasks have become more accessible for developers. Building a custom visual search engine creates interesting possibilities for new features. Learn to train and deploy a visual search engine to enable visual similarity search. Implementing it from scratch enables domain-specific features and customizability that third party solutions lack.",
            "abstract": "# Intro\r\nBuilding a visual search engine allows users to discover items they may never be exposed to. It enables features such as using images to query visually similar items on an application.\r\n\r\n# Developing a Model\r\nThe field of computer vision has exploded with recent developments in deep convolutional neural networks. These models can embed input images into lower dimensional spaces, transforming them into fixed-size vectors. This is useful because all images reside in a unified vector-space where their relative positions hold significance. With deep learning, visual similarity models are taught to minimize the distance between similar images and separate dissimilar images. This process is called metric learning.\r\n\r\nPython packages like Keras and Tensorflow make it simple to structure and train a model. The training data needed are pairs of similar images and pairs or dissimilar images. This could be as easy as labeling various images of the same item as similar and randomly sampling images as dissimilar. Use Keras to implement a siamese neural network that takes two images as input and embeds each into the vector-space. If the image pair is labeled as similar, the model will adjust itself to push their vectors closer together. Depending on the training set size and GPU hardware, a visual similarity model could be ready within a few hours or days.\r\n\r\n# Indexing Imagery for Search\r\nK-nearest-neighbor search is the method for finding similar images in the vector-space. Each searchable image must be embedded into the vector-space by pushing it through the trained model. When all vectors are generated, create a fast knn data-structure with them to enable real-time vector-search. Many Python packages exists that create binary tree structures or small-world networks for knn search. Once that structure is created, it can be pickled and stored for later production use.\r\n\r\n# Deploying to a Microservice\r\nModels trained on Keras (with a Tensorflow backend) can be deployed to production systems using Tensorflow Serving. TF Serving is a gRPC server that allows a client to run the model on images as if the model were running on the client locally. There are many reasons to run the model outside the microservice such as performance and independent scaling.\r\n\r\nFlask is a useful tool to build a microservice that ties together all the necessary components. Upon startup, the microservice will load the pickled index into memory so it can use the index for knn search. Url routes are easily creatable and support POST data like images. Upon image upload, the microservice will generate the image\u2019s vector via connection to TF Serving and use that vector for a knn search on the in-memory index. The results of the search are the most similar images in the index.\r\n\r\n# Outline\r\n- Intro (3 minutes)\r\n    - Who am I\r\n    - Use cases for visual search\r\n- Developing a Model (6 minutes)\r\n    - Using Keras, create a model to identify when two images are similar\r\n    - Gather training data to fine-tune model\r\n- Indexing Imagery for Search (4 minutes)\r\n    - Use trained model to produce a vector for any image\r\n    - Create fast k-nearest-neighbor structure with image vectors\r\n    - Pickle those structures and save them as the search indexes\r\n- Deploying to Web Service (7 minutes)\r\n    - Compile Tensorflow Serving to load model on gRPC service\r\n    - Run Flask microservice to load search indexes in-memory\r\n    - On query image, use gRPC to generate image vector and do nearest neighbor search\r\n- Questions (5 minutes)",
            "abstract_html": "<h1>Intro</h1>\n<p>Building a visual search engine allows users to discover items they may never be exposed to. It enables features such as using images to query visually similar items on an application.</p>\n<h1>Developing a Model</h1>\n<p>The field of computer vision has exploded with recent developments in deep convolutional neural networks. These models can embed input images into lower dimensional spaces, transforming them into fixed-size vectors. This is useful because all images reside in a unified vector-space where their relative positions hold significance. With deep learning, visual similarity models are taught to minimize the distance between similar images and separate dissimilar images. This process is called metric learning.</p>\n<p>Python packages like Keras and Tensorflow make it simple to structure and train a model. The training data needed are pairs of similar images and pairs or dissimilar images. This could be as easy as labeling various images of the same item as similar and randomly sampling images as dissimilar. Use Keras to implement a siamese neural network that takes two images as input and embeds each into the vector-space. If the image pair is labeled as similar, the model will adjust itself to push their vectors closer together. Depending on the training set size and GPU hardware, a visual similarity model could be ready within a few hours or days.</p>\n<h1>Indexing Imagery for Search</h1>\n<p>K-nearest-neighbor search is the method for finding similar images in the vector-space. Each searchable image must be embedded into the vector-space by pushing it through the trained model. When all vectors are generated, create a fast knn data-structure with them to enable real-time vector-search. Many Python packages exists that create binary tree structures or small-world networks for knn search. Once that structure is created, it can be pickled and stored for later production use.</p>\n<h1>Deploying to a Microservice</h1>\n<p>Models trained on Keras (with a Tensorflow backend) can be deployed to production systems using Tensorflow Serving. TF Serving is a gRPC server that allows a client to run the model on images as if the model were running on the client locally. There are many reasons to run the model outside the microservice such as performance and independent scaling.</p>\n<p>Flask is a useful tool to build a microservice that ties together all the necessary components. Upon startup, the microservice will load the pickled index into memory so it can use the index for knn search. Url routes are easily creatable and support POST data like images. Upon image upload, the microservice will generate the image\u2019s vector via connection to TF Serving and use that vector for a knn search on the in-memory index. The results of the search are the most similar images in the index.</p>\n<h1>Outline</h1>\n<ul>\n<li>Intro (3 minutes)<ul>\n<li>Who am I</li>\n<li>Use cases for visual search</li>\n</ul>\n</li>\n<li>Developing a Model (6 minutes)<ul>\n<li>Using Keras, create a model to identify when two images are similar</li>\n<li>Gather training data to fine-tune model</li>\n</ul>\n</li>\n<li>Indexing Imagery for Search (4 minutes)<ul>\n<li>Use trained model to produce a vector for any image</li>\n<li>Create fast k-nearest-neighbor structure with image vectors</li>\n<li>Pickle those structures and save them as the search indexes</li>\n</ul>\n</li>\n<li>Deploying to Web Service (7 minutes)<ul>\n<li>Compile Tensorflow Serving to load model on gRPC service</li>\n<li>Run Flask microservice to load search indexes in-memory</li>\n<li>On query image, use gRPC to generate image vector and do nearest neighbor search</li>\n</ul>\n</li>\n<li>Questions (5 minutes)</li>\n</ul>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T05:07:57.438Z",
            "speaker": 65,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 79,
        "fields": {
            "kind": 1,
            "title": "Modern Data Science for Patent Analytics",
            "description": "Presenting Elementary IP's high-performance patent analytics tool based on modern Data Science and Deep Learning neural networks",
            "abstract": "Most government patent offices and commercial patent vendors continue to offer dated tools for patent analytics. We present our high-performance Elementary IP patent analytics tool based on modern Data Science and Deep Learning neural networks. We will share case studies for a range of usage scenarios - semantic word graphs, similarity search, document classification, semantic clustering, entity normalization and patent claims analysis. A brief interactive demo will also be presented.",
            "abstract_html": "<p>Most government patent offices and commercial patent vendors continue to offer dated tools for patent analytics. We present our high-performance Elementary IP patent analytics tool based on modern Data Science and Deep Learning neural networks. We will share case studies for a range of usage scenarios - semantic word graphs, similarity search, document classification, semantic clustering, entity normalization and patent claims analysis. A brief interactive demo will also be presented.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T05:16:19.986Z",
            "speaker": 66,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 80,
        "fields": {
            "kind": 1,
            "title": "Factory Automation with Python",
            "description": "Modern-day factories are full of sensors, robots, and complicated workflows. Python turns out to be surprisingly versatile in this setting, whether for prototyping a single conveyor belt or taming a building full of machinery. This talk explains how to use Python for interfacing with two common industrial automation devices: a barcode scanner and a Programmable Logic Controller (PLC).",
            "abstract": "Modern factories are full of sensors and robots. Python turns out to be surprisingly versatile in this setting, whether for prototyping a single conveyor belt or for taming a building full of robots.\r\n\r\nThis talk explains how to use Python for interfacing with machinery using two common industrial automation devices: a barcode scanner and a programmable logic controller (PLC). Python beginners will find out how even a few lines of straightforward code are enough to make things move and create meaningful automated behavior. Advanced Pythonistas will get a sense of how their skills might apply in a domain where Python, and modern software development in general, are still far from ubiquitous.\r\n\r\nAs we go along, I will demonstrate these ideas by live coding control software for an on-stage \u201cmini factory\u201d that scans barcodes and sorts chewing gum packages into bins based on the color of their packaging.",
            "abstract_html": "<p>Modern factories are full of sensors and robots. Python turns out to be surprisingly versatile in this setting, whether for prototyping a single conveyor belt or for taming a building full of robots.</p>\n<p>This talk explains how to use Python for interfacing with machinery using two common industrial automation devices: a barcode scanner and a programmable logic controller (PLC). Python beginners will find out how even a few lines of straightforward code are enough to make things move and create meaningful automated behavior. Advanced Pythonistas will get a sense of how their skills might apply in a domain where Python, and modern software development in general, are still far from ubiquitous.</p>\n<p>As we go along, I will demonstrate these ideas by live coding control software for an on-stage \u201cmini factory\u201d that scans barcodes and sorts chewing gum packages into bins based on the color of their packaging.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T06:53:14.438Z",
            "speaker": 54,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 81,
        "fields": {
            "kind": 1,
            "title": "Object classification, detection and Segmentation",
            "description": "A walkthrough of different image models and how they build upon one another. In the talk we'll cover different deep learning models that are used for classifying images, detecting different objects in them and finally how the image is segmented into entities. Covered models include Resnet, Inception, SSD, Retina Net, Faster RCNN, Mask RCNN etc.",
            "abstract": "Deep learning has been tremendously successful in the image processing and computer vision world. The ease of model development with libraries such as Tensorflow, Keras etc has made it easy to build and train larger and more complicated Neural Networks. Few key tasks in Computer Vision include the ability to classify images, detect objects within them and then finally segment the image into different items.\r\n\r\nIn this talk, we'll start with covering the basics of Convolutional Neural Networks. Then we'll dive into some of the models used for image classification such as Resnet and Inception. Next, we'll cover the object detection space like SSD, Faster RCNN and Retina Net and understand how they build upon our learnings from image classification. At the end we'll see how these models later inspire the state of the art segmentation models.\r\n\r\nAt the end of the talk the attendee would have an understanding of various state of the art models that are used in Computer Vision today.",
            "abstract_html": "<p>Deep learning has been tremendously successful in the image processing and computer vision world. The ease of model development with libraries such as Tensorflow, Keras etc has made it easy to build and train larger and more complicated Neural Networks. Few key tasks in Computer Vision include the ability to classify images, detect objects within them and then finally segment the image into different items.</p>\n<p>In this talk, we'll start with covering the basics of Convolutional Neural Networks. Then we'll dive into some of the models used for image classification such as Resnet and Inception. Next, we'll cover the object detection space like SSD, Faster RCNN and Retina Net and understand how they build upon our learnings from image classification. At the end we'll see how these models later inspire the state of the art segmentation models.</p>\n<p>At the end of the talk the attendee would have an understanding of various state of the art models that are used in Computer Vision today.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T07:17:51.295Z",
            "speaker": 67,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 82,
        "fields": {
            "kind": 1,
            "title": "Recent advances in Deep Learning and Tensorflow",
            "description": "A whirlwind tour of all the latest developments in the world ranging from applications in healthcare, natural language processing, reinforcement learning and machine learning infrastructure.",
            "abstract": "Deep learning has been tremendously successful in variety of fields and in such a rapidly growing field it can be hard to keep up with the latest advances in the field. In this talk, we'll try to address that give you a quick overview of what is going on in the field by talking about latest results on different applications of deep learning. Topics covered could include Tensorflow, Retina scans, Cloud TPUs, Meta learning etc.",
            "abstract_html": "<p>Deep learning has been tremendously successful in variety of fields and in such a rapidly growing field it can be hard to keep up with the latest advances in the field. In this talk, we'll try to address that give you a quick overview of what is going on in the field by talking about latest results on different applications of deep learning. Topics covered could include Tensorflow, Retina scans, Cloud TPUs, Meta learning etc.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T07:28:07.640Z",
            "speaker": 67,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 83,
        "fields": {
            "kind": 1,
            "title": "Recent advances in Deep Learning and Tensorflow",
            "description": "A whirlwind tour of all the latest developments in the world ranging from applications in healthcare, natural language processing, reinforcement learning and machine learning infrastructure.",
            "abstract": "Deep learning has been tremendously successful in variety of fields and in such a rapidly growing field it can be hard to keep up with the latest advances in the field. In this talk, we'll try to address that give you a quick overview of what is going on in the field by talking about latest results on different applications of deep learning. Topics covered could include Tensorflow, Retina scans, Cloud TPUs, Meta learning etc.",
            "abstract_html": "<p>Deep learning has been tremendously successful in variety of fields and in such a rapidly growing field it can be hard to keep up with the latest advances in the field. In this talk, we'll try to address that give you a quick overview of what is going on in the field by talking about latest results on different applications of deep learning. Topics covered could include Tensorflow, Retina scans, Cloud TPUs, Meta learning etc.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T07:28:08.023Z",
            "speaker": 67,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 84,
        "fields": {
            "kind": 1,
            "title": "Gathering Related Functionality: Patterns for Clean API Design",
            "description": "When designing libraries, you want something that \"just works\" but also has an obvious escape hatch to handle corner cases. This talk covers several patterns for cleanly organizing related and overlapping functionality that satisfies both humans and static analysis tools.",
            "abstract": "What do you do when you have to choose between designing your function for one of two common use cases?\r\n\r\nHow about when the same logical operations (say, multiplication or concatenation) need to have different implementations depending on the type of the arguments they are applied to?\r\n\r\nThese kinds of questions can be vexing when trying to design a clean, well-scoped API.\r\n\r\nThis talk will cover several strategies for grouping related functionality in a way that presents a logically clean interface to both humans and static analysis tools like type checkers and document generators.\r\n\r\nThis talk covers:\r\n    - Alternate constructors with @classmethod\r\n    - Namespacing functions under a class with @staticmethod\r\n    - Dispatch by type\r\n    - A new convention for namespacing functions: ``variants``",
            "abstract_html": "<p>What do you do when you have to choose between designing your function for one of two common use cases?</p>\n<p>How about when the same logical operations (say, multiplication or concatenation) need to have different implementations depending on the type of the arguments they are applied to?</p>\n<p>These kinds of questions can be vexing when trying to design a clean, well-scoped API.</p>\n<p>This talk will cover several strategies for grouping related functionality in a way that presents a logically clean interface to both humans and static analysis tools like type checkers and document generators.</p>\n<p>This talk covers:\n    - Alternate constructors with @classmethod\n    - Namespacing functions under a class with @staticmethod\n    - Dispatch by type\n    - A new convention for namespacing functions: <code>variants</code></p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T16:21:13.083Z",
            "speaker": 68,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 85,
        "fields": {
            "kind": 1,
            "title": "Contributing to Open Source: A Guide",
            "description": "Contributing to open source can be a rewarding experience, but many people don't know how to contribute, or find the process confusing or frustrating. This talk tries to bridge both the technical and social gap between new contributors and maintainers to help the listener have a positive open source experience.",
            "abstract": "Open source software powers a huge amount of the technology you use every day. Maintainers often exhort the public for contributions, but many developers find the process complicated or don't think that they have any skills they can contribute.\r\n\r\nThis talk will cover:\r\n   - The different ways *you* can contribute to open source projects\r\n   - The technical process of using GitHub for Issues and Pull Requests\r\n   - How to maximize the chances that your contributions will be accepted\r\n\r\nIf you think it sounds cool to be able to make a meaningful contribution to software that touches millions of lives, but never thought you could - this talk is for you. If some open source software you use has a bug in it that just never seems to get fixed - this talk is for you. Even if you don't see the appeal of contributing to open source projects and just want to learn why you might want to do so - this talk is for you.",
            "abstract_html": "<p>Open source software powers a huge amount of the technology you use every day. Maintainers often exhort the public for contributions, but many developers find the process complicated or don't think that they have any skills they can contribute.</p>\n<p>This talk will cover:\n   - The different ways <em>you</em> can contribute to open source projects\n   - The technical process of using GitHub for Issues and Pull Requests\n   - How to maximize the chances that your contributions will be accepted</p>\n<p>If you think it sounds cool to be able to make a meaningful contribution to software that touches millions of lives, but never thought you could - this talk is for you. If some open source software you use has a bug in it that just never seems to get fixed - this talk is for you. Even if you don't see the appeal of contributing to open source projects and just want to learn why you might want to do so - this talk is for you.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T16:40:07.596Z",
            "speaker": 68,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 86,
        "fields": {
            "kind": 1,
            "title": "Data Wrangling Basics for the Every Day Scientist",
            "description": "I'll go over the basics of data cleaning, wrangling, and visualization for the every day (\"small-data\") scientist. This is aimed to be a template for non-programmers who are interested in data analysis but don't plan on doing big data. I'll cover the process of transforming raw data into a clean and usable dataset. Then cover some of the most common tools to analyze and visualize data in python.",
            "abstract": "In this talk I aim to provide a template for a basic data analysis workflow/pipeline for non-programmers. \r\nI will start with an example of a raw data set (no labels) and transform it using an accompanying variable dictionary into something readable.\r\nI will then cover the cleaning of data: dropping nulls, filling in values, extracting features, transforming dtypes, using datetime.\r\nWe will not go too into the details of each, the point of this talk is to provide a general sense of the options one has in python and the language so those interested can later research further in their own time.\r\nOnce we have a readable and usable data set I will go over some of the most common ways one analyzes data including basic descriptive statistics, slicing, indexing, grouping, etc.\r\nI will finish with some of the most common easy-to-use powerful visualization libraries: plotly (using cufflinks), altair, seaborn.",
            "abstract_html": "<p>In this talk I aim to provide a template for a basic data analysis workflow/pipeline for non-programmers. \nI will start with an example of a raw data set (no labels) and transform it using an accompanying variable dictionary into something readable.\nI will then cover the cleaning of data: dropping nulls, filling in values, extracting features, transforming dtypes, using datetime.\nWe will not go too into the details of each, the point of this talk is to provide a general sense of the options one has in python and the language so those interested can later research further in their own time.\nOnce we have a readable and usable data set I will go over some of the most common ways one analyzes data including basic descriptive statistics, slicing, indexing, grouping, etc.\nI will finish with some of the most common easy-to-use powerful visualization libraries: plotly (using cufflinks), altair, seaborn.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T17:27:24.276Z",
            "speaker": 69,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 87,
        "fields": {
            "kind": 1,
            "title": "Snake Farming: How to work with multiple versions of Python on the same machine",
            "description": "There are many situations when you need to have two or more versions of Python installed next to each other. While several tools exist to help with this, the process can be befuddling to beginners and even advanced Pythonistas. This talk compares your options, from package managers to Docker to compiling from source, with a few source code excursions to catch a glimps of how all this it works.",
            "abstract": "There are many reasons why you might need to have two or more versions of cPython installed next to each other: Projects with conflicting Python requirements, while upgrading a codebase from one version to another, or for testing your code for compatibility. This talk is a summary of what I learned from a weeklong investigation that started on Stackoverflow and ended deep in the source code of cPython itself.\r\n\r\n\r\nThe tools this talk covers fall into three categories: First, I will go over mini tutorials for virtualenv, pyenv, pipenv and how they all fit together. Next, brew and apt will serve as examples for package managers and what you can and can't do with them. And third, we will look at Docker and how you can use it as your local Python development environment without knowing too much about Docker.\r\n\r\nAlong the way I will show code snippets, PEPs, excerpts from mailing list discussions and other bits and pieces that illustrate how these tools work. Some of the questions this talk answers are: What is the correct hashbang format to use? What does the maintainer of a brew package actually do? And what does that have to do with how Python finds pip-installed packages? And how do I compile Python myself if I don't want to deal with any of this!?",
            "abstract_html": "<p>There are many reasons why you might need to have two or more versions of cPython installed next to each other: Projects with conflicting Python requirements, while upgrading a codebase from one version to another, or for testing your code for compatibility. This talk is a summary of what I learned from a weeklong investigation that started on Stackoverflow and ended deep in the source code of cPython itself.</p>\n<p>The tools this talk covers fall into three categories: First, I will go over mini tutorials for virtualenv, pyenv, pipenv and how they all fit together. Next, brew and apt will serve as examples for package managers and what you can and can't do with them. And third, we will look at Docker and how you can use it as your local Python development environment without knowing too much about Docker.</p>\n<p>Along the way I will show code snippets, PEPs, excerpts from mailing list discussions and other bits and pieces that illustrate how these tools work. Some of the questions this talk answers are: What is the correct hashbang format to use? What does the maintainer of a brew package actually do? And what does that have to do with how Python finds pip-installed packages? And how do I compile Python myself if I don't want to deal with any of this!?</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T17:48:37.505Z",
            "speaker": 54,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 88,
        "fields": {
            "kind": 1,
            "title": "Django Channels and Websockets in Production!",
            "description": "Recently we helped launch a React Native mobile application that is a platform for college students to buy and sell from each other. Django and DRF are our usual tools of choice, but for these delivery user flows real-time updates via websockets seemed like the best UX we could offer. We started with a Django Channels spike and now it\u2019s a large piece of our application that is out in the wild.",
            "abstract": "The Technology\r\nAs web technologies have progressed, many backend frameworks that can take advantage of asynchronous code have integrated websockets. This has gone hand and hand with users who now often expect near real-time communication with the applications they use daily. Andrew Godwin started work on bring websockets into Django with is work on Channels in 2015. With the release of Channels 2.0 this year it\u2019s ready for prime time!\r\n\r\nThe Problem\r\nDjango Channels, even though it was been worked on for a few years is still relatively new in the Django world. Even moreso we were using Channels 2.0, which had literally just come out a week prior to us diving head first into Django + websockets. I personally learn best from seeing fully fleshed out examples and an ample amount of tutorials. In this case, we had to rely on just the documentation and go it alone for our use case, which is a bit different than your standard chatroom websocket example.\r\n\r\nThe Talk\r\nFrom this talk you\u2019ll get working code samples that show you a fleshed out Django Channels implementation that is live in production. We\u2019ll cover some first-time pitfalls that you should watch out for (that we fell into). You should walk away with an understanding of how you may go about integrating Django Channels into your application and where you would start with several helpful resources that you can follow up with after the talk. Specifically one of the largest areas that you\u2019ll get a jump start in is actually deploying and running infrastructure that will support Django Channels.",
            "abstract_html": "<p>The Technology\nAs web technologies have progressed, many backend frameworks that can take advantage of asynchronous code have integrated websockets. This has gone hand and hand with users who now often expect near real-time communication with the applications they use daily. Andrew Godwin started work on bring websockets into Django with is work on Channels in 2015. With the release of Channels 2.0 this year it\u2019s ready for prime time!</p>\n<p>The Problem\nDjango Channels, even though it was been worked on for a few years is still relatively new in the Django world. Even moreso we were using Channels 2.0, which had literally just come out a week prior to us diving head first into Django + websockets. I personally learn best from seeing fully fleshed out examples and an ample amount of tutorials. In this case, we had to rely on just the documentation and go it alone for our use case, which is a bit different than your standard chatroom websocket example.</p>\n<p>The Talk\nFrom this talk you\u2019ll get working code samples that show you a fleshed out Django Channels implementation that is live in production. We\u2019ll cover some first-time pitfalls that you should watch out for (that we fell into). You should walk away with an understanding of how you may go about integrating Django Channels into your application and where you would start with several helpful resources that you can follow up with after the talk. Specifically one of the largest areas that you\u2019ll get a jump start in is actually deploying and running infrastructure that will support Django Channels.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T18:11:01.161Z",
            "speaker": 70,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 89,
        "fields": {
            "kind": 1,
            "title": "Python for Social Sciences",
            "description": "In this talk I will go over some of the resources available to social scientists in the python data ecosystem: scientific libraries for the statistical models used, visualization tools for engaging and effective representations of the results, and other tools for collecting, cleaning, and wrangling data from various sources (PDFs, websites). Will provide an example atendees can recreate later.",
            "abstract": "n this talk I aim to provide a set of examples of how to achieve various tasks commonly found in a social scientist's analysis workflow:\r\n\t1. collecting data - scrapping websites and PDFs (Selenium, Scrapy, pdftables) [if time permits a quick example on using APIs like censuspy - will be included in the accompanying notebook] \r\n\t2. cleaning and tidying up data - pandas, using datetime, etc. [quick example on pd.read_stata() pros and cons]\r\n\t3. analysis - utilizing scientific python libraries (SciPy, Numpy, Statsmodels) and some STATA commands equivalents (i.e. linear/logistic regression)\r\n\t4. visualizing the results - using plotly (cufflinks), seaborn, altair to easily create beautiful and engaging visualizations. \r\n\r\n\r\nThe atendee will learn:\r\n\t1. to scrape data from websites and PDFs\r\n\t2. some of the most common scientific python libraries and their uses (NumPy, SciPy, Statsmodels) and how to replicate whatever they were doing in STATA in Python\r\n\t3. how to visualize their results and best practices for engaging (short mention on how to create palettes for those in institutions with rigorous style guidelines) (libraries of interest: Altair, Plotly, Seaborn)\r\n\t4. about resources available (for data, inspiration, ideas, practice).\r\n\r\nThe goal is for the atendee to leave with a range of resources and the basic language and skill to further explore by themselves the options the python data science ecosystem has. \r\nThese include resources for data, inspiration and practice, visualization best practices, and libraries in python to explore and master for one's unique challenges.",
            "abstract_html": "<p>n this talk I aim to provide a set of examples of how to achieve various tasks commonly found in a social scientist's analysis workflow:\n    1. collecting data - scrapping websites and PDFs (Selenium, Scrapy, pdftables) [if time permits a quick example on using APIs like censuspy - will be included in the accompanying notebook] \n    2. cleaning and tidying up data - pandas, using datetime, etc. [quick example on pd.read_stata() pros and cons]\n    3. analysis - utilizing scientific python libraries (SciPy, Numpy, Statsmodels) and some STATA commands equivalents (i.e. linear/logistic regression)\n    4. visualizing the results - using plotly (cufflinks), seaborn, altair to easily create beautiful and engaging visualizations. </p>\n<p>The atendee will learn:\n    1. to scrape data from websites and PDFs\n    2. some of the most common scientific python libraries and their uses (NumPy, SciPy, Statsmodels) and how to replicate whatever they were doing in STATA in Python\n    3. how to visualize their results and best practices for engaging (short mention on how to create palettes for those in institutions with rigorous style guidelines) (libraries of interest: Altair, Plotly, Seaborn)\n    4. about resources available (for data, inspiration, ideas, practice).</p>\n<p>The goal is for the atendee to leave with a range of resources and the basic language and skill to further explore by themselves the options the python data science ecosystem has. \nThese include resources for data, inspiration and practice, visualization best practices, and libraries in python to explore and master for one's unique challenges.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T18:37:49.600Z",
            "speaker": 69,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 90,
        "fields": {
            "kind": 1,
            "title": "Simple and Fast Messaging with Asyncio NATS",
            "description": "NATS is an open source, high performance messaging server that has simplicity and reliability as its core values.  It has a very minimalistic design with few features with a straightforward plain-text protocol that makes writing clients for it easier to implement. In this talk, we will cover the internals and design decisions of the Asyncio client for NATS to get the best performance.",
            "abstract": "The NATS project offers a simple and lightweight solution for low latency pub/sub communication between services that is increasing recently in popularity and also now a hosted project under the Cloud Native Computing Foundation.\r\n\r\nFor the Python3 implementation of the NATS client, Asyncio was chosen the beginning as it fits very well the async nature of the protocol, and in this talk we will share what has worked the best for the NATS team in its usage of Asyncio in order to have a performant client.\r\nIf you are considering to use Asyncio or NATS on your next project and want to gain better familiarity or interested in performance then this talk is for you.",
            "abstract_html": "<p>The NATS project offers a simple and lightweight solution for low latency pub/sub communication between services that is increasing recently in popularity and also now a hosted project under the Cloud Native Computing Foundation.</p>\n<p>For the Python3 implementation of the NATS client, Asyncio was chosen the beginning as it fits very well the async nature of the protocol, and in this talk we will share what has worked the best for the NATS team in its usage of Asyncio in order to have a performant client.\nIf you are considering to use Asyncio or NATS on your next project and want to gain better familiarity or interested in performance then this talk is for you.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T18:38:39.243Z",
            "speaker": 71,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 91,
        "fields": {
            "kind": 1,
            "title": "Functional programming with python builtins",
            "description": "A walk through list python's list and dictionaries comprehension, lambda, and reduce to perform Functional programming and make a data API",
            "abstract": "Functional programming is growing in popularity but you don't need to learn Haskell for FP to improve your code.  Come to this talk to see FP taken a little to far.",
            "abstract_html": "<p>Functional programming is growing in popularity but you don't need to learn Haskell for FP to improve your code.  Come to this talk to see FP taken a little to far.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T18:53:08.064Z",
            "speaker": 72,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 92,
        "fields": {
            "kind": 1,
            "title": "First steps to transition from SQL to pandas",
            "description": "For developers and analysts using SQL, transitioning to Python to make calculations may initially seem daunting. In my talk, I will discuss different options for data munging before diving into an introduction of how to leverage pandas to \u2018translate\u2019 certain SQL functions, starting with simple aggregations and joins to finally go deeper into window functions and rolling averages.",
            "abstract": "This talk will discuss my experiences of trying different options when wanting to use both SQL and pandas: the SQL Jupyter extension, Python SQL module, connecting to a database through Python and finally, \u2018translating\u2019 all calculations into pandas. I will touch on advantages and disadvantages of all of these methods and I will then dive deeper into slicing and dicing pandas DataFrames, performing joins, unions, aggregations and more advanced calculations such as window functions and rolling averages. All of the calculations shown will use pandas, one of the most common data science libraries.",
            "abstract_html": "<p>This talk will discuss my experiences of trying different options when wanting to use both SQL and pandas: the SQL Jupyter extension, Python SQL module, connecting to a database through Python and finally, \u2018translating\u2019 all calculations into pandas. I will touch on advantages and disadvantages of all of these methods and I will then dive deeper into slicing and dicing pandas DataFrames, performing joins, unions, aggregations and more advanced calculations such as window functions and rolling averages. All of the calculations shown will use pandas, one of the most common data science libraries.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T19:45:30.455Z",
            "speaker": 73,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 93,
        "fields": {
            "kind": 1,
            "title": "Blockchain & Python",
            "description": "Understand basic blockchain concept and what prototype implementation in Python;  and introduce the importance of consensus, and how it is typically done (PoW, PoS, etc.) ; demo SmartContracts in Python (via NEO chain, not ETH chain).",
            "abstract": "Illustrate core concept of blockchains, and what a POC will look like in pure python code.  Of course we won't have time to go for full fledged implementation, but we can cover various consensus algorithms, like the one used in Bitcoin.  Concept of SmartContracts will be demoed, which is the core of success story of Ethereum,  but we will use NEO chain for demo purpose as it has direct Python support, and it has way higher transaction capacity.",
            "abstract_html": "<p>Illustrate core concept of blockchains, and what a POC will look like in pure python code.  Of course we won't have time to go for full fledged implementation, but we can cover various consensus algorithms, like the one used in Bitcoin.  Concept of SmartContracts will be demoed, which is the core of success story of Ethereum,  but we will use NEO chain for demo purpose as it has direct Python support, and it has way higher transaction capacity.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T20:19:56.449Z",
            "speaker": 74,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 94,
        "fields": {
            "kind": 1,
            "title": "Critical Incidents: a guide for developers",
            "description": "As developers we know there's no such thing as bug-free software. Critical incidents are inevitable and being prepared is key. How would you handle, for example, a database outage if it happened now?  What can you do to prepare your team? How to assemble a recovery plan? To answer these questions, I will provide a step-by-step guide, starting from the bug discovery until the incident postmortem.",
            "abstract": "As developers we know (too well) there's no such thing as bug-free software. Whether you work in a small or big production software, critical incidents are inevitable and being prepared is key. How would you handle, for example, a database outage if it happened now? Or a critical bug that is affecting half of your users?\r\n\r\nNavigating in crisis mode is never easy, but having a great company culture, a defined incident process and a recovery plan gives you guidance and mitigates damage. In this talk, I will share some success cases, such as GitLab database outage recovery, and my personal experience as a tech lead overcoming an incident in an subscription system built with Django.\r\n\r\nWhat can you do to prepare your team? When should you enter crisis mode? How to assemble a recovery plan? To answer these and other questions, I will provide a step-by-step guide, from an Modern Agile perspective, starting with the bug discovery, and handling the client's expectations, through the data recovery, until your crisis postmortem.",
            "abstract_html": "<p>As developers we know (too well) there's no such thing as bug-free software. Whether you work in a small or big production software, critical incidents are inevitable and being prepared is key. How would you handle, for example, a database outage if it happened now? Or a critical bug that is affecting half of your users?</p>\n<p>Navigating in crisis mode is never easy, but having a great company culture, a defined incident process and a recovery plan gives you guidance and mitigates damage. In this talk, I will share some success cases, such as GitLab database outage recovery, and my personal experience as a tech lead overcoming an incident in an subscription system built with Django.</p>\n<p>What can you do to prepare your team? When should you enter crisis mode? How to assemble a recovery plan? To answer these and other questions, I will provide a step-by-step guide, from an Modern Agile perspective, starting with the bug discovery, and handling the client's expectations, through the data recovery, until your crisis postmortem.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T22:46:33.738Z",
            "speaker": 75,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 95,
        "fields": {
            "kind": 1,
            "title": "Distributed computing with DAT and python",
            "description": "Dat is a developing technology, enabling peer-to-peer access to data and used by scientists to share datasets, by developers to build peer-to-peer websites and offline editable maps. There is a big infrastructure being developed around DAT protocol which makes distributed web a step closer.",
            "abstract": "In this talk, we will look into the DAT protocol and it's design principles. We will go through currently available infrastructure and also will learn how to make your sites available via DAT and how to use it in your python code. We will go through standard applications - distributed log, using a distributed filesystem, syncing files, accessing distributed web resources.",
            "abstract_html": "<p>In this talk, we will look into the DAT protocol and it's design principles. We will go through currently available infrastructure and also will learn how to make your sites available via DAT and how to use it in your python code. We will go through standard applications - distributed log, using a distributed filesystem, syncing files, accessing distributed web resources.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T23:01:08.904Z",
            "speaker": 76,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 96,
        "fields": {
            "kind": 1,
            "title": "Modern C extensions: Why, How, and the Future",
            "description": "C extensions are a great way to speed up Python code, but are difficult to manage. Learn how to make your life easier and your Python code faster",
            "abstract": "Python is a wonderful programming language. However, at times Python programs can be limited by computational speed. C extension modules can provide a means of providing faster computations without re-writing absolutely everything in another language. C extensions are also a great way to interoperate with existing C and C++ libraries. In this talk we will delve into how to make writing C extensions easier and more maintainable, as well as discuss when it is best to use them.",
            "abstract_html": "<p>Python is a wonderful programming language. However, at times Python programs can be limited by computational speed. C extension modules can provide a means of providing faster computations without re-writing absolutely everything in another language. C extensions are also a great way to interoperate with existing C and C++ libraries. In this talk we will delve into how to make writing C extensions easier and more maintainable, as well as discuss when it is best to use them.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T23:27:24.393Z",
            "speaker": 77,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 97,
        "fields": {
            "kind": 1,
            "title": "Halibut: A drop in compatible, faster CPython",
            "description": "Halibut speeds up CPython through replacing the core interpreter loop with a faster implementation, all without breaking compatibility with existing code.",
            "abstract": "Halibut will be a C extension implementing a register bytecode virtual machine to execute Python code while remaining fully compatible with the CPython runtime. Register based VMs have been shown to perform better than stack based VMs due to less dispatching (a hard to predict branch) and less memory chatter due to not needing to push to/pop from the stack. Halibut will work via a two step process: first, existing *.pyc files are re-compiled to replace the bytecode. Second, the Halibut VM executes the register based bytecode using the PEP 523 frame evaluation API and the existing C APIs used by CPython's own interpreter loop.",
            "abstract_html": "<p>Halibut will be a C extension implementing a register bytecode virtual machine to execute Python code while remaining fully compatible with the CPython runtime. Register based VMs have been shown to perform better than stack based VMs due to less dispatching (a hard to predict branch) and less memory chatter due to not needing to push to/pop from the stack. Halibut will work via a two step process: first, existing *.pyc files are re-compiled to replace the bytecode. Second, the Halibut VM executes the register based bytecode using the PEP 523 frame evaluation API and the existing C APIs used by CPython's own interpreter loop.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T23:43:17.952Z",
            "speaker": 77,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 98,
        "fields": {
            "kind": 1,
            "title": "Face Value: Scaling Usable CLIs in Python",
            "description": "The year is 2018, and the console is more important than ever. Commands like git, docker, pip, zfs, systemctl, and countless others, along with their dozens of subcommands, control everything around us. Come learn about the challenges and solutions available for building and scaling these CLI applications in Python!",
            "abstract": "Python's history as a command-line tool spans millenia. And as long as people have been using Python for scripts and other command-line applications, people have struggled with the libraries and frameworks available. From getopt to optparse to argparse to docopt to click, the debate has flared and smoldered in code.\r\n\r\nWe'll look at a brief history of the command-line, as it applies to us today. We'll follow that with an analysis of a few case studies of CLI design, with a focus on successful Python entries like pip, conda, and hg, before moving into an in-depth look at specific Python patterns. And because building CLI tools can be addictive once you get started, we'll cover a set of best practices to keep those interfaces beautiful.",
            "abstract_html": "<p>Python's history as a command-line tool spans millenia. And as long as people have been using Python for scripts and other command-line applications, people have struggled with the libraries and frameworks available. From getopt to optparse to argparse to docopt to click, the debate has flared and smoldered in code.</p>\n<p>We'll look at a brief history of the command-line, as it applies to us today. We'll follow that with an analysis of a few case studies of CLI design, with a focus on successful Python entries like pip, conda, and hg, before moving into an in-depth look at specific Python patterns. And because building CLI tools can be addictive once you get started, we'll cover a set of best practices to keep those interfaces beautiful.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-10T23:45:07.711Z",
            "speaker": 78,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 99,
        "fields": {
            "kind": 1,
            "title": "Manage IoT Devices With MicroPython And Smartphones",
            "description": "Remember the days where tinkering with computers and electronics gain you the label of geek? Not anymore! \r\n\r\nThanks to Micropython, kids today are quickly learning how to do projects entire companies took years of research and development. In this talk, we will learn how to program and manage IoT devices with our smartphones just using Python.",
            "abstract": "As technology becomes more mainstream and accepted, Python has been playing a major role in simplifying and making it more accessible. A breakthrough in technology education came in 2013 when Damien George created Micropython for microcontrollers, (low cost, resource constrained microcomputers on a chip) that spawned projects like the BBC Microbit, Adafruit's CircuitPython, and Micropython for the wildly popular ESP8266 microcontroller.\r\n\r\nIn this talk, we will learn how to easily program IoT (Internet of Things) devices using Micropython to manage their resources. General Purpose Input/Output pins (GPIOs) allow these little devices to sense the world around and interact with it, for example, controlling buttons, switches, colorful LED strips, light, humidity and temperature sensors.  From there, we will enter the world of IoT wireless communication using Micropython's MQTT (Message Queue Telemetry Transport) to manage our IoT device and publish light intensity readings.\r\n\r\nWe'll wrap-up the talk with a practical implementation of a light dimmer, using QPython and Kivy to build a beautiful user interface that runs on any Android smartphone.",
            "abstract_html": "<p>As technology becomes more mainstream and accepted, Python has been playing a major role in simplifying and making it more accessible. A breakthrough in technology education came in 2013 when Damien George created Micropython for microcontrollers, (low cost, resource constrained microcomputers on a chip) that spawned projects like the BBC Microbit, Adafruit's CircuitPython, and Micropython for the wildly popular ESP8266 microcontroller.</p>\n<p>In this talk, we will learn how to easily program IoT (Internet of Things) devices using Micropython to manage their resources. General Purpose Input/Output pins (GPIOs) allow these little devices to sense the world around and interact with it, for example, controlling buttons, switches, colorful LED strips, light, humidity and temperature sensors.  From there, we will enter the world of IoT wireless communication using Micropython's MQTT (Message Queue Telemetry Transport) to manage our IoT device and publish light intensity readings.</p>\n<p>We'll wrap-up the talk with a practical implementation of a light dimmer, using QPython and Kivy to build a beautiful user interface that runs on any Android smartphone.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T00:09:05.396Z",
            "speaker": 79,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 100,
        "fields": {
            "kind": 1,
            "title": "How to Read Python You Didn't Write: Ramping up in a new Software Engineering role",
            "description": "Have you ever looked at someone else\u2019s code and thought, \u201cWill I ever understand how this works?\u201d This talk covers strategies for contributing to an existing codebase sooner, with more context, and with greater confidence. Students and early-career engineers who haven\u2019t yet worked with a codebase will learn how to ramp up on the job without getting overwhelmed.",
            "abstract": "Have you ever looked at someone else\u2019s code and thought, \u201cWill I ever understand how this works?\u201d You aren't alone! This is exactly how I felt when I started my first job as a Python software engineer. \r\n\r\nI created a plan for exploring my team's code, which I now call \"The 6 Ds of Becoming Familiar With A Codebase\". The \"Ds\" are Description, Diagrams, Debugger, Dialogue, Docstrings and Documentation. \r\n\r\nJoin me as I show you how to implement each of these 6 learning strategies while doing a live \"first-look\" of a Django app with lots of modules, classes and methods!",
            "abstract_html": "<p>Have you ever looked at someone else\u2019s code and thought, \u201cWill I ever understand how this works?\u201d You aren't alone! This is exactly how I felt when I started my first job as a Python software engineer. </p>\n<p>I created a plan for exploring my team's code, which I now call \"The 6 Ds of Becoming Familiar With A Codebase\". The \"Ds\" are Description, Diagrams, Debugger, Dialogue, Docstrings and Documentation. </p>\n<p>Join me as I show you how to implement each of these 6 learning strategies while doing a live \"first-look\" of a Django app with lots of modules, classes and methods!</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T00:27:38.233Z",
            "speaker": 80,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 101,
        "fields": {
            "kind": 1,
            "title": "Automating AWS with Lambda and Python",
            "description": "Reducing costs and enhancing compliance in AWS with Serverless, and the Boto3 library.",
            "abstract": "Python has been a hugely important part of AWS with the Boto libraries.  In this talk we will show how with Boto3 and \"Modern Python\" we can automate compliance, and reduce cost by using CloudWatch, and Lambda to orchestrate the management of our AWS services efficiently.",
            "abstract_html": "<p>Python has been a hugely important part of AWS with the Boto libraries.  In this talk we will show how with Boto3 and \"Modern Python\" we can automate compliance, and reduce cost by using CloudWatch, and Lambda to orchestrate the management of our AWS services efficiently.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T00:33:09.464Z",
            "speaker": 81,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 102,
        "fields": {
            "kind": 1,
            "title": "PyMongo: Working with Python and MongoDB",
            "description": "MongoDB is one of the main NoSQL databases (It was named a leader among NoSQL vendors in the Forrester Wave\u2122 of 2016). It is commonly used for Internet of Things, Mobile and Real-Time Analytics in companies like Expedia, EA, eBay and Adobe. The goal of this talk is to go through the concepts of MongoDB and show how you can use them in Python with PyMongo.",
            "abstract": "MongoDB is a free and open-source NoSQL database that uses JSON-like documents as its main content and is focused on scalability and flexibility. According to iDatalabs, MongoDB is used by 24.434 companies (https://idatalabs.com/tech/products/mongodb), which gives it 4.44% of the market share. In addition, according to its own website (https://www.mongodb.com/what-is-mongodb), MongoDB has more than 30,000,000 downloads and it is used in 35,000 plus GitHub repositories.\r\n\r\nMongoDB aims to make databases more flexible. To do that, it works with JSON-like documents that do not need to follow a certain schema (data can vary from document to document) and can be easily translated to objects in the application code. It also allows Ad-hoc queries, indexing, and real-time aggregation. Another problem that MongoDB solves is scalability. It uses a distributed database cluster as its core and it is pretty simple to add or remove instances as you need.\r\n\r\nTo make use of all of that, we have a Python library called PyMongo. It is basically a toolkit to work with MongoDB in Python and it is the recommended way to do that. PyMongo is open-source (https://github.com/mongodb/mongo-python-driver) and it has a great community (https://api.mongodb.com/python/current/contributors.html) and user group (https://groups.google.com/forum/#!forum/mongodb-user) behind it.\r\n\r\nThe goal of this talk is to cover the concepts of MongoDB and explain how it works, when we should use it and how to make use of the PyMongo lib to work with MongoDB in Python projects. It will also include some examples (personal and some particular use cases) to make the talk more dynamic and interesting.",
            "abstract_html": "<p>MongoDB is a free and open-source NoSQL database that uses JSON-like documents as its main content and is focused on scalability and flexibility. According to iDatalabs, MongoDB is used by 24.434 companies (https://idatalabs.com/tech/products/mongodb), which gives it 4.44% of the market share. In addition, according to its own website (https://www.mongodb.com/what-is-mongodb), MongoDB has more than 30,000,000 downloads and it is used in 35,000 plus GitHub repositories.</p>\n<p>MongoDB aims to make databases more flexible. To do that, it works with JSON-like documents that do not need to follow a certain schema (data can vary from document to document) and can be easily translated to objects in the application code. It also allows Ad-hoc queries, indexing, and real-time aggregation. Another problem that MongoDB solves is scalability. It uses a distributed database cluster as its core and it is pretty simple to add or remove instances as you need.</p>\n<p>To make use of all of that, we have a Python library called PyMongo. It is basically a toolkit to work with MongoDB in Python and it is the recommended way to do that. PyMongo is open-source (https://github.com/mongodb/mongo-python-driver) and it has a great community (https://api.mongodb.com/python/current/contributors.html) and user group (https://groups.google.com/forum/#!forum/mongodb-user) behind it.</p>\n<p>The goal of this talk is to cover the concepts of MongoDB and explain how it works, when we should use it and how to make use of the PyMongo lib to work with MongoDB in Python projects. It will also include some examples (personal and some particular use cases) to make the talk more dynamic and interesting.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T01:46:40.608Z",
            "speaker": 82,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 103,
        "fields": {
            "kind": 1,
            "title": "High-Performance Python Microservice Communication",
            "description": "Good design of the communication layer is critical to implementing a high-performance microservice. Serialization and Deserialization in a binary format such as GRPC might be faster than a text format such as JSON however, you might be sacrificing access to human readable messages often useful for debugging.",
            "abstract": "Python Microservices are becoming increasingly popular for large scale web applications. We\u2019ll start by talking about some of the trade-offs of different microservice communication designs. Afterwards we\u2019ll dive into code regarding specific configurations including RPC and traditional REST.",
            "abstract_html": "<p>Python Microservices are becoming increasingly popular for large scale web applications. We\u2019ll start by talking about some of the trade-offs of different microservice communication designs. Afterwards we\u2019ll dive into code regarding specific configurations including RPC and traditional REST.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T02:17:11.520Z",
            "speaker": 83,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 104,
        "fields": {
            "kind": 1,
            "title": "Building Building ML",
            "description": "At TEECOM, we are using a number of Python-based platforms to apply machine learning and statistical analysis to challenges in the Architecture, Engineering, and Construction (AEC) industry. These include design automation and numerical analysis of completed projects. These applications let us solve problems more quickly and gain insight into the issues facing us, our clients, and collaborators.",
            "abstract": "Architects and engineers have been reducing dimensionality for centuries\u2014exchanging information about the three dimensional physical world through two dimensional drawings. This suggests that there is a subset of the problems in the AEC space that unique applications of contemporary deep neural network-based computer vision algorithms may be used to solve. Additionally, the digital models that carry most of the information in AEC industry are a fertile ground for the application of less flashy statistical techniques. This can be applied trivially with packages from Python\u2019s rich ecosystem like Numpy, MXNet, Matplotlib, and others.",
            "abstract_html": "<p>Architects and engineers have been reducing dimensionality for centuries\u2014exchanging information about the three dimensional physical world through two dimensional drawings. This suggests that there is a subset of the problems in the AEC space that unique applications of contemporary deep neural network-based computer vision algorithms may be used to solve. Additionally, the digital models that carry most of the information in AEC industry are a fertile ground for the application of less flashy statistical techniques. This can be applied trivially with packages from Python\u2019s rich ecosystem like Numpy, MXNet, Matplotlib, and others.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T02:54:20.685Z",
            "speaker": 84,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 105,
        "fields": {
            "kind": 1,
            "title": "Machine Learning at Twitter: Twitter meets Tensorflow",
            "description": "Twitter is a company with massive amounts of data. Thus, it is no wonder that the company applies machine learning in various ways: from Timeline Ranking to Ads. This talk will be focused on our most recent ML platform, which is built on top of (Python) Tensorflow. We will give you an idea of how we are doing ML at Twitter\u2019s scale and what our platform provides on top of Tensorflow.",
            "abstract": "Machine Learning has allowed Twitter to drive engagement, promote healthier conversations, and deliver catered advertisements. Over the past year, we have been working on a new chapter of ML at Twitter by migrating our machine learning platform from Lua Torch to (Python) Tensorflow. This talk will be mainly focusing on the Machine Learning framework we have been developing on top of Tensorflow. More especially, how it allows for teams inside the company to run their models in production at Twitter\u2019s scale.\r\nWe plan to discuss:\r\n    - Our migration from Lua Torch to Tensorflow\r\n    - The additions of our framework on top of Tensorflow\r\n    - How we productionalize our model",
            "abstract_html": "<p>Machine Learning has allowed Twitter to drive engagement, promote healthier conversations, and deliver catered advertisements. Over the past year, we have been working on a new chapter of ML at Twitter by migrating our machine learning platform from Lua Torch to (Python) Tensorflow. This talk will be mainly focusing on the Machine Learning framework we have been developing on top of Tensorflow. More especially, how it allows for teams inside the company to run their models in production at Twitter\u2019s scale.\nWe plan to discuss:\n    - Our migration from Lua Torch to Tensorflow\n    - The additions of our framework on top of Tensorflow\n    - How we productionalize our model</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T02:57:32.686Z",
            "speaker": 85,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 106,
        "fields": {
            "kind": 1,
            "title": "Unsupervised Self-Learning",
            "description": "Looking to level up your skills as a programmer? Come learn about habits and mindsets you can use to continue to improve your programming skills. This talk focuses on how one can improve their coding skills in the absence of a mentor.",
            "abstract": "I'm a self taught developer and have had the opportunity to work alongside great engineers. My journey in programming included spending time at the Recurse Center in NYC where I got to work with amazing programmers who kept pushing the boundaries of their skills. Along the way I've taken notes on how engineers increase their skills and grow. In this talk I want to share the habits and mindsets great programmers use to take their skills to the next level. This talk covers code reading, code copying, tackling side projects, and engaging with open source.",
            "abstract_html": "<p>I'm a self taught developer and have had the opportunity to work alongside great engineers. My journey in programming included spending time at the Recurse Center in NYC where I got to work with amazing programmers who kept pushing the boundaries of their skills. Along the way I've taken notes on how engineers increase their skills and grow. In this talk I want to share the habits and mindsets great programmers use to take their skills to the next level. This talk covers code reading, code copying, tackling side projects, and engaging with open source.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T03:18:21.838Z",
            "speaker": 86,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 107,
        "fields": {
            "kind": 1,
            "title": "Cooperative context switching in the cloud, or how to make a multi-tenant microservice",
            "description": "How to convert an on-prem library to a multitenant application. This talk will demystify cooperative co-routines with a practical real world example.",
            "abstract": "How do you provide a multi-tenant cloud service when each customer has a significantly different context for each request? On premise applications only have to worry about one customer. When you move to the cloud, now you have to handle requests from many customers, using the same architecture & API. This talk will cover how we leveraged the gevent library, cooperative co-routines, and a microservice architecture to create an efficient solution for multi-tenant context dependent request handling.",
            "abstract_html": "<p>How do you provide a multi-tenant cloud service when each customer has a significantly different context for each request? On premise applications only have to worry about one customer. When you move to the cloud, now you have to handle requests from many customers, using the same architecture &amp; API. This talk will cover how we leveraged the gevent library, cooperative co-routines, and a microservice architecture to create an efficient solution for multi-tenant context dependent request handling.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T05:03:32.811Z",
            "speaker": 87,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 108,
        "fields": {
            "kind": 1,
            "title": "Creating correlated log data with Python",
            "description": "Python is great for solving problems. This talk will discuss how Python can be leveraged to create a correlated log library.",
            "abstract": "Every product demo is only as good as its data. Data is easy enough to produce, but what happens when you need both correlated and noisy data for a demo? In order to demonstrate that value in an engaging way, a haystack is needed as much as needles. In order to tell a cohesive story, those needles need to be correlated. This analogy is starting to break down, so join me as share my experience in using Python (and other tools!) to build a dataset for an NLP product that is both correlated and noisy.",
            "abstract_html": "<p>Every product demo is only as good as its data. Data is easy enough to produce, but what happens when you need both correlated and noisy data for a demo? In order to demonstrate that value in an engaging way, a haystack is needed as much as needles. In order to tell a cohesive story, those needles need to be correlated. This analogy is starting to break down, so join me as share my experience in using Python (and other tools!) to build a dataset for an NLP product that is both correlated and noisy.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T05:21:14.136Z",
            "speaker": 88,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 109,
        "fields": {
            "kind": 1,
            "title": "Introducing Managed Key Cache",
            "description": "A library that extends Django's ORM, allowing for performant caching of multi-model queries.",
            "abstract": "Udemy uses Django in production.  Using Django's ORM, it is easy to join against multiple models; however, it is very difficult to cache and clear those results correctly.  We developed a library that integrates with the Django ORM and transparently breaks those joins into separate queries utilizing the keys for each model.  This removed most of our hardest-hit database queries.\r\nA working knowledge of Django and it's object relational model is expected.",
            "abstract_html": "<p>Udemy uses Django in production.  Using Django's ORM, it is easy to join against multiple models; however, it is very difficult to cache and clear those results correctly.  We developed a library that integrates with the Django ORM and transparently breaks those joins into separate queries utilizing the keys for each model.  This removed most of our hardest-hit database queries.\nA working knowledge of Django and it's object relational model is expected.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T05:27:41.945Z",
            "speaker": 89,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 110,
        "fields": {
            "kind": 1,
            "title": "Detecting offensive messages using Deep Learning: A micro-service based approach",
            "description": "What are you doing to control abusive content on your platform? Can your current solution tell the difference between \"f\\*\\*king awesome\" and \"f\\*\\*king loser\"? Can it detect racist and sexist content?\r\n\r\nYou will learn how to build a deep learning based solution and deploy it as a micro-service.",
            "abstract": "This talk focuses on :\r\n\r\n- The shortcomings / common pitfalls of current approaches in terms of:\r\n  - Functionality\r\n  - Scalibility\r\n- Handling the ambiguities in defining offensive content\r\n- Using a Deep Learning based approach to detect offensive content\r\n- Production-izing the solution",
            "abstract_html": "<p>This talk focuses on :</p>\n<ul>\n<li>The shortcomings / common pitfalls of current approaches in terms of:</li>\n<li>Functionality</li>\n<li>Scalibility</li>\n<li>Handling the ambiguities in defining offensive content</li>\n<li>Using a Deep Learning based approach to detect offensive content</li>\n<li>Production-izing the solution</li>\n</ul>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T05:33:26.264Z",
            "speaker": 90,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 111,
        "fields": {
            "kind": 1,
            "title": "The Good, the Bad and the Ugly: Reconciling Open Source with Continuous Integration",
            "description": "Review of the various issues with using open-source packages in a continuous-integration corporate environment. We'll look at how can we debug these issues, resolve them, and contribute back to the community.",
            "abstract": "The authors of open source Python packages often make assumptions that are perfectly suited for their own environment and completely acceptable for open-source development. Unfortunately, some packages become less friendly for use in a corporate environment where every build has to pass through a continuous integration pipeline. The builds must be repeatable, automated, consistent, executed on dedicated build hosts, deployed in a specific manner, and isolated from external world.\r\n\r\nWe are going to review the lessons learned, trade-offs and workarounds applied to get some important widely used Python packages working correctly in a corporate environment. Come and see the issues that can arise with pip, setuptools, wheel, pbr, pex, Sphinx, flake8, numpy, scipy, ..., and how to resolve them. Learn how to be a good citizen and contribute your changes in a respectful manner back to the original authors providing the benefit for everyone else.",
            "abstract_html": "<p>The authors of open source Python packages often make assumptions that are perfectly suited for their own environment and completely acceptable for open-source development. Unfortunately, some packages become less friendly for use in a corporate environment where every build has to pass through a continuous integration pipeline. The builds must be repeatable, automated, consistent, executed on dedicated build hosts, deployed in a specific manner, and isolated from external world.</p>\n<p>We are going to review the lessons learned, trade-offs and workarounds applied to get some important widely used Python packages working correctly in a corporate environment. Come and see the issues that can arise with pip, setuptools, wheel, pbr, pex, Sphinx, flake8, numpy, scipy, ..., and how to resolve them. Learn how to be a good citizen and contribute your changes in a respectful manner back to the original authors providing the benefit for everyone else.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T07:01:17.835Z",
            "speaker": 91,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 112,
        "fields": {
            "kind": 1,
            "title": "Python and the Holy Grail",
            "description": "Python is oft lauded for it's simplicity and syntax and also rightly for it's portability. Other languages have long claimed to be 'write once, run anywhere' but as you'll see in this talk, Python  challenges all comers to take the crown as the best language for cross platform development.",
            "abstract": "When you're as flexible as a Python, you tend to end up in places where people might not expect. \r\n\r\nPython is oft lauded for it's simplicity and syntax and also rightly for it's portability. Other languages have long claimed to be 'write once, run anywhere' but as you'll see in this talk, Python  challenges all comers to take the crown as the best language for cross platform development.\r\n\r\nWe'll be discussing the trade offs you can expect to make, as well as the bench marking that helps us decide where and when to use Python and equally importantly, when not to.\r\n\r\n# Outline  \r\n\r\n### 1. Python Where?\r\n\r\nThis introduction will outline _some_ of the places that you can use Python today, ranked inversely by maturity (all judges decisions are final and made at our discretion!). These include Mobile Applications, IoT, Embedded Devices, Desktop GUIs, Server Side and CLIs.\r\n\r\nRun time: 5 mins\r\n\r\n\r\n### 2. Code Reuse Case Study\r\n\r\nHere we will dive straight into a case study in which a draft RFC (cavage-http-signatures-09) is implemented for use on five different operating systems (Linux, Mac, Windows, Android & Bare Metal) in different interpreters (CPython, PyPy and MicroPython) on completely different platforms (Server, Desktop, Mobile and IoT). I will highlight why we chose to aim for code reuse rather than simply compatible implementations.\r\n\r\nThis section will look at some of the code from this implementation and look at specific decisions which were made to improve code reusability across platforms.\r\n\r\nRun time: 10 mins\r\n\r\n### 3. Code Reuse Gotchyas and Lessons Learned\r\n\r\nWe're going to be highlighting some lessons learned from the experience including:\r\n* You live or die by your test suite\r\n* Automate ALL the testing\r\n* Document inconsistent behavior whenever you see it\r\n* Get comfortable with the official docs, take note of the small things\r\n* Code reuse is great, but test compatability with other implementations as well\r\n\r\nRun time: 8 mins\r\n\r\n\r\n### 5. Conclusion and Questions\r\n\r\nThis is the section where I leave the audience with parting thoughts, take rounds and rounds of applause, a few questions and fin.\r\n\r\nOn a more serious note; this is where I present the resources where audience members can find more about the main topics, talk slides, example code and contact details. This is also where I reiterate why you should choose to double down on Python across your all your platforms. The main takeaway should be that the developer productivity gains outweigh the costs of being a bit unique.\r\n\r\nRun time: 2 mins",
            "abstract_html": "<p>When you're as flexible as a Python, you tend to end up in places where people might not expect. </p>\n<p>Python is oft lauded for it's simplicity and syntax and also rightly for it's portability. Other languages have long claimed to be 'write once, run anywhere' but as you'll see in this talk, Python  challenges all comers to take the crown as the best language for cross platform development.</p>\n<p>We'll be discussing the trade offs you can expect to make, as well as the bench marking that helps us decide where and when to use Python and equally importantly, when not to.</p>\n<h1>Outline</h1>\n<h3>1. Python Where?</h3>\n<p>This introduction will outline <em>some</em> of the places that you can use Python today, ranked inversely by maturity (all judges decisions are final and made at our discretion!). These include Mobile Applications, IoT, Embedded Devices, Desktop GUIs, Server Side and CLIs.</p>\n<p>Run time: 5 mins</p>\n<h3>2. Code Reuse Case Study</h3>\n<p>Here we will dive straight into a case study in which a draft RFC (cavage-http-signatures-09) is implemented for use on five different operating systems (Linux, Mac, Windows, Android &amp; Bare Metal) in different interpreters (CPython, PyPy and MicroPython) on completely different platforms (Server, Desktop, Mobile and IoT). I will highlight why we chose to aim for code reuse rather than simply compatible implementations.</p>\n<p>This section will look at some of the code from this implementation and look at specific decisions which were made to improve code reusability across platforms.</p>\n<p>Run time: 10 mins</p>\n<h3>3. Code Reuse Gotchyas and Lessons Learned</h3>\n<p>We're going to be highlighting some lessons learned from the experience including:\n<em> You live or die by your test suite\n</em> Automate ALL the testing\n<em> Document inconsistent behavior whenever you see it\n</em> Get comfortable with the official docs, take note of the small things\n* Code reuse is great, but test compatability with other implementations as well</p>\n<p>Run time: 8 mins</p>\n<h3>5. Conclusion and Questions</h3>\n<p>This is the section where I leave the audience with parting thoughts, take rounds and rounds of applause, a few questions and fin.</p>\n<p>On a more serious note; this is where I present the resources where audience members can find more about the main topics, talk slides, example code and contact details. This is also where I reiterate why you should choose to double down on Python across your all your platforms. The main takeaway should be that the developer productivity gains outweigh the costs of being a bit unique.</p>\n<p>Run time: 2 mins</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T09:28:30.462Z",
            "speaker": 92,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 113,
        "fields": {
            "kind": 1,
            "title": "Building AI powered Twitter Bot that guesses locations of pictures from pixels",
            "description": "Learn how we designed, built, and deployed the @WhereML Twitter bot that can identify where in the world a picture was taken using only the pixels in the image. We'll dive deep on artificial intelligence and deep learning with the MXNet framework and also talk about working with the Twitter Account Activity API.",
            "abstract": "The WhereML twitter bot is built on the LocationNet model which is trained with the Berkley Multimedia Commons public dataset of 33.9 million geotagged images from Flickr. The model is based on a ResNet-101 architecture and adds a classification layer that splits the earth into ~15000 cells created with Google's S2 spherical geometry library. This model is based on prior work at Berkley and Google.\r\n\r\nIn this session we'll start by describing AI in general terms then diving into deep learning and the MXNet framework. We'll describe the LocationNet model in detail and show how it is trained and created in Amazon SageMaker. Finally we'll talk about the Twitter Account Activity webhooks API and how to interact with it using an API Gateway and AWS Lambda function.\r\n\r\nAttendees are encouraged to interact with the bot in real-time at whereml.bot or on twitter at @WhereML\r\n\r\nAll code used in this project is open source and attendees are encouraged to experiment with it.",
            "abstract_html": "<p>The WhereML twitter bot is built on the LocationNet model which is trained with the Berkley Multimedia Commons public dataset of 33.9 million geotagged images from Flickr. The model is based on a ResNet-101 architecture and adds a classification layer that splits the earth into ~15000 cells created with Google's S2 spherical geometry library. This model is based on prior work at Berkley and Google.</p>\n<p>In this session we'll start by describing AI in general terms then diving into deep learning and the MXNet framework. We'll describe the LocationNet model in detail and show how it is trained and created in Amazon SageMaker. Finally we'll talk about the Twitter Account Activity webhooks API and how to interact with it using an API Gateway and AWS Lambda function.</p>\n<p>Attendees are encouraged to interact with the bot in real-time at whereml.bot or on twitter at @WhereML</p>\n<p>All code used in this project is open source and attendees are encouraged to experiment with it.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T15:46:29.047Z",
            "speaker": 93,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 114,
        "fields": {
            "kind": 1,
            "title": "Python on Windows is Okay, Actually",
            "description": "Despite more than half of Python's users being on Windows, it is generally believed that you can't do it. In this session, Steve will provide an overview of why it's not as bad as people think, and the things you can do to make sure your projects work well for ALL Python users, no matter where they work.",
            "abstract": "Packages that won't install, encodings that don't work, installers that ask too many questions, and having to own a PC are all great reasons to just ignore Windows. Or they would be, if they were true.\r\n\r\nDespite community perception, more than half of Python usage still happens on Windows, including web development, system administration, and data science, just like on Linux and Mac. And for the most part, Python works the same regardless of what operating system you happen to be using. Still, many library developers will unnecessarily exclude half of their potential audience by not even attempting to be compatible.\r\n\r\nThis session will walk through the things to be aware of when creating cross-platform libraries. From simple things like using pathlib rather than bytestrings, through to all the ways you can get builds and tests running on Windows for free, by the end of this session you will have a checklist of easy tasks for your project that will really enable the whole Python world to benefit from your work.",
            "abstract_html": "<p>Packages that won't install, encodings that don't work, installers that ask too many questions, and having to own a PC are all great reasons to just ignore Windows. Or they would be, if they were true.</p>\n<p>Despite community perception, more than half of Python usage still happens on Windows, including web development, system administration, and data science, just like on Linux and Mac. And for the most part, Python works the same regardless of what operating system you happen to be using. Still, many library developers will unnecessarily exclude half of their potential audience by not even attempting to be compatible.</p>\n<p>This session will walk through the things to be aware of when creating cross-platform libraries. From simple things like using pathlib rather than bytestrings, through to all the ways you can get builds and tests running on Windows for free, by the end of this session you will have a checklist of easy tasks for your project that will really enable the whole Python world to benefit from your work.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-11T19:21:04.873Z",
            "speaker": 94,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 116,
        "fields": {
            "kind": 1,
            "title": "Airflow on Kubernetes: Dynamically Scaling Python-based DAG Workflows",
            "description": "Over the past year, we have developed a native integration between Apache Airflow and Kubernetes that allows for dynamic allocation of DAG-based workflows and dynamic dependency management of individual tasks.",
            "abstract": "Apache Airflow is a highly popular Directed Acyclic Graphs (DAG) based workflow engine that allows users to deploy complex DAGs as python code. It is considered a natural progression of the \"code as configuration\" philosophy of DevOps and ETL. \r\n\r\nWith the addition of the native \"Kubernetes Executor\" and \"Kubernetes Operator\", we have extended Airflow's flexibility with dynamic allocation and dynamic dependency management capabilities of Kubernetes and Docker.",
            "abstract_html": "<p>Apache Airflow is a highly popular Directed Acyclic Graphs (DAG) based workflow engine that allows users to deploy complex DAGs as python code. It is considered a natural progression of the \"code as configuration\" philosophy of DevOps and ETL. </p>\n<p>With the addition of the native \"Kubernetes Executor\" and \"Kubernetes Operator\", we have extended Airflow's flexibility with dynamic allocation and dynamic dependency management capabilities of Kubernetes and Docker.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-13T23:28:06.265Z",
            "speaker": 95,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 117,
        "fields": {
            "kind": 1,
            "title": "Ten lines of Python",
            "description": "What can we do in 10 lines of Python? A fuzzy matching algorithm using nothing but the standard library. An auto-completing, syntax highlighting, history preserving REPL using just one external library. Let's unleash the power of Python.",
            "abstract": "We all know that Python is a succinct yet powerful language. Let's explore that power with two practical examples. In mere 10 lines of Python we will implement a fuzzy matching algorithm that is similar to the fuzzy open feature you'll find in SublimeText or Vim or VSCode. We will do this using nothing but the standard library of Python. The code will be idiomatic and no tricks involved. Next, we will install a CLI library and use it to build a modern REPL with auto-completion, syntax highlighting and history.",
            "abstract_html": "<p>We all know that Python is a succinct yet powerful language. Let's explore that power with two practical examples. In mere 10 lines of Python we will implement a fuzzy matching algorithm that is similar to the fuzzy open feature you'll find in SublimeText or Vim or VSCode. We will do this using nothing but the standard library of Python. The code will be idiomatic and no tricks involved. Next, we will install a CLI library and use it to build a modern REPL with auto-completion, syntax highlighting and history.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-14T05:40:49.056Z",
            "speaker": 96,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 118,
        "fields": {
            "kind": 1,
            "title": "ads",
            "description": "ads",
            "abstract": "ads",
            "abstract_html": "<p>ads</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-14T17:35:01.825Z",
            "speaker": 35,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 119,
        "fields": {
            "kind": 1,
            "title": "Agile, for Engineers",
            "description": "Project management is incredibly useful to keep projects moving and teams happy. It's also a very different mindset than hands on keyboard coding. You'll learn the roles, meetings, and artifacts that comprise Agile development, common implementations, and how to make agile work for you.",
            "abstract": "Large, complex projects are fulfilling, but also difficult to build. Agile development is a time-honed process for keeping projects moving, and teams happy. \r\n\r\nAs a rare software engineer & scrum master, I've learned how to guide engineers through the agile process. Additionally, through my experience leading Machine Learning projects at a large company, I've picked up practical tips and insights that keep teammates challenged & productive.\r\n\r\nDuring this talk, you'll learn the roles, meetings, and artifacts that comprise Agile development, and how to modify agile for your environment. Further, we'll learn how to communicate what you've learned to your fellow engineers, and keep everyone rollin'.",
            "abstract_html": "<p>Large, complex projects are fulfilling, but also difficult to build. Agile development is a time-honed process for keeping projects moving, and teams happy. </p>\n<p>As a rare software engineer &amp; scrum master, I've learned how to guide engineers through the agile process. Additionally, through my experience leading Machine Learning projects at a large company, I've picked up practical tips and insights that keep teammates challenged &amp; productive.</p>\n<p>During this talk, you'll learn the roles, meetings, and artifacts that comprise Agile development, and how to modify agile for your environment. Further, we'll learn how to communicate what you've learned to your fellow engineers, and keep everyone rollin'.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-17T19:17:11.957Z",
            "speaker": 97,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 120,
        "fields": {
            "kind": 1,
            "title": "Deep Learning & Data Types",
            "description": "I'm writing a library built on Keras and Pandas that can smartly & robustly handle common data types as deep learning inputs. We'll walk through why categorical, datetime, boolean, and other data types are difficult, practical ways of handling these data types, and how to make DL more approachable.",
            "abstract": "Deep learning is a powerful toolset, but I've been frustrated by lack of tools and techniques to pair it with tabular data. In response, I'm building one. \r\n\r\nYou'll learn:\r\n\r\n - Why non-numerical data types are difficult for most algorithms (one hot encoding, manual feature extraction, etc)\r\n - Practical techniques for using categorical, boolean, datetime, and geographic data (with starter code!)\r\n - How to quickly and efficiently prototype and iterate on deep learning models\r\n\r\nFeedback from this talk will also help me create and prioritize features for my work-in-progress package AutoDL, which implements these methods in Pandas and Keras.",
            "abstract_html": "<p>Deep learning is a powerful toolset, but I've been frustrated by lack of tools and techniques to pair it with tabular data. In response, I'm building one. </p>\n<p>You'll learn:</p>\n<ul>\n<li>Why non-numerical data types are difficult for most algorithms (one hot encoding, manual feature extraction, etc)</li>\n<li>Practical techniques for using categorical, boolean, datetime, and geographic data (with starter code!)</li>\n<li>How to quickly and efficiently prototype and iterate on deep learning models</li>\n</ul>\n<p>Feedback from this talk will also help me create and prioritize features for my work-in-progress package AutoDL, which implements these methods in Pandas and Keras.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-17T19:19:49.683Z",
            "speaker": 97,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 121,
        "fields": {
            "kind": 1,
            "title": "Analyzing Pwned Passwords with PySpark",
            "description": "Apache Spark aims to solve the problem of working with large scale distributed data. This talk will introduce Spark and its Python API by searching for patterns in millions of breached credentials. We will discuss tradeoffs for using different abstractions provided by the framework and leave you with the tools to get started with your own distributed data...and a password manager.",
            "abstract": "Apache Spark aims to solve the problem of working with large scale distributed data -- and with access to over 500 million leaked passwords we have a lot of data to dig through.\r\n\r\nAdvancements in the API make running Spark with Python or even SQL smoother and faster than ever. This talk will introduce you to Spark and the new way to run queries on structured, distributed data by looking at breached credentials. We'll walk through how to get started with Spark and discuss the tradeoffs for using different abstractions provided by the framework. With the help of live code, we'll find patterns in the password data and look at how you can encourage your users to be more secure. You will see how easy and fast it is to both explore and process data using Spark SQL and leave with the tools to get started with your own distributed data...and a password manager.",
            "abstract_html": "<p>Apache Spark aims to solve the problem of working with large scale distributed data -- and with access to over 500 million leaked passwords we have a lot of data to dig through.</p>\n<p>Advancements in the API make running Spark with Python or even SQL smoother and faster than ever. This talk will introduce you to Spark and the new way to run queries on structured, distributed data by looking at breached credentials. We'll walk through how to get started with Spark and discuss the tradeoffs for using different abstractions provided by the framework. With the help of live code, we'll find patterns in the password data and look at how you can encourage your users to be more secure. You will see how easy and fast it is to both explore and process data using Spark SQL and leave with the tools to get started with your own distributed data...and a password manager.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-18T05:17:53.098Z",
            "speaker": 98,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 122,
        "fields": {
            "kind": 1,
            "title": "2FA, WTF?",
            "description": "You may recognize Two-factor Authentication (2FA) as an additional safeguard for protecting accounts, but do you really know how it works? This talk will show how to implement One Time Passwords, including what's happening with those expiring tokens. You'll learn different approaches to implementing a 2FA solution and have a better understanding of the solution that's right for your application.",
            "abstract": "2017 was a banner year for cybersecurity attacks and we should all be worried. In an age when a new data breach is revealed with frightening regularity, developers have a responsibility to secure our applications' user data more than ever. But fear not, YOU have the power to deter the hackers!\r\n\r\nYou may recognize Two-factor Authentication (2FA) as an additional safeguard for protecting accounts, but do you really know how it works? This talk will show you how to implement One Time Passwords in Python (including what's happening under the hood of those expiring tokens) and even provide a legitimate use case for QR codes! You'll come away recognizing the different approaches to implementing a 2FA solution and have a better understanding of the solution that's right for your application. Together, we'll make the web a more secure place.",
            "abstract_html": "<p>2017 was a banner year for cybersecurity attacks and we should all be worried. In an age when a new data breach is revealed with frightening regularity, developers have a responsibility to secure our applications' user data more than ever. But fear not, YOU have the power to deter the hackers!</p>\n<p>You may recognize Two-factor Authentication (2FA) as an additional safeguard for protecting accounts, but do you really know how it works? This talk will show you how to implement One Time Passwords in Python (including what's happening under the hood of those expiring tokens) and even provide a legitimate use case for QR codes! You'll come away recognizing the different approaches to implementing a 2FA solution and have a better understanding of the solution that's right for your application. Together, we'll make the web a more secure place.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-18T05:24:53.351Z",
            "speaker": 98,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 123,
        "fields": {
            "kind": 1,
            "title": "asyncio: what's next",
            "description": "A quick recap of what's new in asyncio in Python 3.7. A look at what features and capabilities we can expect to have in asyncio in Python 3.8 and beyond.",
            "abstract": "There are some new advanced features in asyncio 3.7 that are worth mentioning: new buffered protocol, start TLS, sendfile, and many other enhancements. The new buffered protocol is especially interesting as it allows to implement protocols with better performance than if was possible before, with less code.\r\n\r\nWith PEP 567 we now have a new Context API in Python and asyncio. It allows to implement advanced tracing (zipkin, statsd, etc) and error reporting in your application, monitoring everything from costs of high-level application logic code to the very low-level IO performance.\r\n\r\nIn Python 3.8 we want to focus on asyncio usability and the robustness of async/await. Particularly, the cancellation logic and timeouts mechanisms need to be redesigned from scratch. Hopefully, closer to August, I'll have a better idea of what we'll be adding to asyncio to make cancellation and timeouts easier to implement and handle correctly.",
            "abstract_html": "<p>There are some new advanced features in asyncio 3.7 that are worth mentioning: new buffered protocol, start TLS, sendfile, and many other enhancements. The new buffered protocol is especially interesting as it allows to implement protocols with better performance than if was possible before, with less code.</p>\n<p>With PEP 567 we now have a new Context API in Python and asyncio. It allows to implement advanced tracing (zipkin, statsd, etc) and error reporting in your application, monitoring everything from costs of high-level application logic code to the very low-level IO performance.</p>\n<p>In Python 3.8 we want to focus on asyncio usability and the robustness of async/await. Particularly, the cancellation logic and timeouts mechanisms need to be redesigned from scratch. Hopefully, closer to August, I'll have a better idea of what we'll be adding to asyncio to make cancellation and timeouts easier to implement and handle correctly.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-19T01:45:32.366Z",
            "speaker": 99,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 124,
        "fields": {
            "kind": 1,
            "title": "EdgeDB",
            "description": "EdgeDB is a new object-relational database. In this talk we'll have a discussion on what features EdgeDB has, what its data model and query language look like, its first-class support of GraphQL.  We'll also touch upon advanced introspection APIs, automatic UIs generation, and what to expect from EdgeDB in the near future.",
            "abstract": "We will ship the first public preview of EdgeDB in the beginning of June, with a private preview available to PyCon US attendees mid-May.\r\n\r\nAbout a week ago we've shared our first blog post about EdgeDB [1], which generated a lot of interest (20K visitors over 2 days).  We're pretty confident that we'll be able to maintain the high community interest in EdgeDB so that it would be a very hot topic when PyBay happens.\r\n\r\nThe talk will cover: \r\n\r\n* data model and how it's different from relational DBs, NoSQL document DBs, graph DBS\r\n\r\n* schema definition DDL\r\n\r\n* query language (EdgeQL)\r\n\r\n* GraphQL\r\n\r\n* introspection\r\n\r\n* analytics\r\n\r\n* performance\r\n\r\n* etc\r\n\r\n[1] https://edgedb.com/blog/edgedb-a-new-beginning",
            "abstract_html": "<p>We will ship the first public preview of EdgeDB in the beginning of June, with a private preview available to PyCon US attendees mid-May.</p>\n<p>About a week ago we've shared our first blog post about EdgeDB [1], which generated a lot of interest (20K visitors over 2 days).  We're pretty confident that we'll be able to maintain the high community interest in EdgeDB so that it would be a very hot topic when PyBay happens.</p>\n<p>The talk will cover: </p>\n<ul>\n<li>\n<p>data model and how it's different from relational DBs, NoSQL document DBs, graph DBS</p>\n</li>\n<li>\n<p>schema definition DDL</p>\n</li>\n<li>\n<p>query language (EdgeQL)</p>\n</li>\n<li>\n<p>GraphQL</p>\n</li>\n<li>\n<p>introspection</p>\n</li>\n<li>\n<p>analytics</p>\n</li>\n<li>\n<p>performance</p>\n</li>\n<li>\n<p>etc</p>\n</li>\n</ul>\n<p>[1] https://edgedb.com/blog/edgedb-a-new-beginning</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-19T01:55:37.199Z",
            "speaker": 99,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 125,
        "fields": {
            "kind": 1,
            "title": "Python, or how Twitter Trained Entity Embeddings",
            "description": "In this talk we\u2019ll analyze how Twitter trains entity embeddings. We use Python for the heavy lifting in training embeddings (either via matrix factorization or TensorFlow-based skip-gram), and (thanks to Airflow) it is also the glue that keep everything working together.",
            "abstract": "Learned features, or entity embeddings, are a great tool to have in your machine learning toolbox. They can be used as building blocks for other models, and can be responsible for both increased performance and reduced computational costs.\r\nTwitter is mainly a Scala shop, and (almost) everything runs on the JVM. Yet our machine learning stack is built on Python.\r\nIn particular, we will show how Twitter uses Python to learn in-memory embeddings and discuss the two major algorithms we use: matrix factorization and TensorFlow-based skip-gram. Airflow allows us to glue together different pieces of the embeddings pipeline, even when they are very different in nature (or are written in Scala!) and to automate the whole process of learning and exporting embeddings.\r\nConsidering how fast the conversation (and sometimes even the vocabulary) on the Twitter platform changes, it is vital to have an easy way to regularly retrain such models, in order to be able to benefit from them.\r\nFinally we\u2019ll show how we those tools are made available to the rest of the company, therefore making it very easy for other teams to train and leverage embeddings.",
            "abstract_html": "<p>Learned features, or entity embeddings, are a great tool to have in your machine learning toolbox. They can be used as building blocks for other models, and can be responsible for both increased performance and reduced computational costs.\nTwitter is mainly a Scala shop, and (almost) everything runs on the JVM. Yet our machine learning stack is built on Python.\nIn particular, we will show how Twitter uses Python to learn in-memory embeddings and discuss the two major algorithms we use: matrix factorization and TensorFlow-based skip-gram. Airflow allows us to glue together different pieces of the embeddings pipeline, even when they are very different in nature (or are written in Scala!) and to automate the whole process of learning and exporting embeddings.\nConsidering how fast the conversation (and sometimes even the vocabulary) on the Twitter platform changes, it is vital to have an easy way to regularly retrain such models, in order to be able to benefit from them.\nFinally we\u2019ll show how we those tools are made available to the rest of the company, therefore making it very easy for other teams to train and leverage embeddings.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-20T00:01:45.390Z",
            "speaker": 100,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 126,
        "fields": {
            "kind": 1,
            "title": "What can systemd do for you as a python developer.",
            "description": "Done! Your shiny new application is functionally complete and ready to be deployed to production! But how exactly do you deploy properly on Linux? Wonder no more! In 40 minutes, this talk explains how you can harness the power of the init system and systemd to solve common deployment problems, including some that you didn't even know you had.",
            "abstract": "Done! Your shiny new application is functionally complete and ready to be deployed to production! But how exactly do you deploy properly on Linux? Wonder no more! In 30 minutes, this talk explains how you can harness the power of the init system and systemd to solve common deployment problems, including some that you didn't even know you had. Examples of things we will cover:\r\n\r\nHow to secure your system by having: private /tmp for your process, read-only paths so that your process can not write to them, inaccessible paths, protect users home, network access, bin directories, etc.\r\nHow to limit the resources you app can consume.\r\nHow to interact directly with systemd, so it can start transient units, start/stop services, mount disks, resolve addresses.\r\nHow to isolate your service without containers.\r\nHow to isolate your service using containers (using systemd to spawn a namespace).\r\nAll this will be covered from a Python developer's perspective.",
            "abstract_html": "<p>Done! Your shiny new application is functionally complete and ready to be deployed to production! But how exactly do you deploy properly on Linux? Wonder no more! In 30 minutes, this talk explains how you can harness the power of the init system and systemd to solve common deployment problems, including some that you didn't even know you had. Examples of things we will cover:</p>\n<p>How to secure your system by having: private /tmp for your process, read-only paths so that your process can not write to them, inaccessible paths, protect users home, network access, bin directories, etc.\nHow to limit the resources you app can consume.\nHow to interact directly with systemd, so it can start transient units, start/stop services, mount disks, resolve addresses.\nHow to isolate your service without containers.\nHow to isolate your service using containers (using systemd to spawn a namespace).\nAll this will be covered from a Python developer's perspective.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-20T04:01:50.277Z",
            "speaker": 101,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 127,
        "fields": {
            "kind": 1,
            "title": "Math-RNN: Teaching Neural Nets to solve simple number sequences",
            "description": "Teach Neural Networks to solve math sequences problems",
            "abstract": "A friend of mine recently asked me if we Neural Nets can do digit subtraction.  So coded up a small gist for him - \r\nhttps://gist.github.com/sampathweb/7d53b41b25f5aa9890cd520fd2cf48f5\r\n\r\nIn this talk, I will walk through the various steps of creating this Math-RNN using PyTorch.  We will also talk about how we can use this pattern to solve other number sequence problems.",
            "abstract_html": "<p>A friend of mine recently asked me if we Neural Nets can do digit subtraction.  So coded up a small gist for him - \nhttps://gist.github.com/sampathweb/7d53b41b25f5aa9890cd520fd2cf48f5</p>\n<p>In this talk, I will walk through the various steps of creating this Math-RNN using PyTorch.  We will also talk about how we can use this pattern to solve other number sequence problems.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-20T20:12:08.278Z",
            "speaker": 102,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 128,
        "fields": {
            "kind": 1,
            "title": "Amusing Algorithms",
            "description": "Learn how to answer real life questions with algorithms written in Python!",
            "abstract": "Merge sort, quick sort, binary search. Yawn! Algorithms can be fun, but the way they\u2019re taught usually is not\u2026\r\n\r\nAt a fundamental level an algorithm is just a recipe. A step-by-step guide for how to do something. But the recipe is often hidden behind complex math and opaque proofs. And it\u2019s usually applied to a seemingly narrow and uninteresting problem. \r\n\r\nIn \u2018Amusing Algorithms\u2019 we\u2019ll cut through the math and try to understand the mechanics of a few interesting and useful algorithms. We\u2019ll use Jupyter to expose data structures, intermediate steps, and simulations of various algorithms. And we\u2019ll try to use algorithms to answer real life questions like how to find love in a crowded bar, how to buy the best scalpers tickets at a baseball game, and how to figure out when you should leave your job.",
            "abstract_html": "<p>Merge sort, quick sort, binary search. Yawn! Algorithms can be fun, but the way they\u2019re taught usually is not\u2026</p>\n<p>At a fundamental level an algorithm is just a recipe. A step-by-step guide for how to do something. But the recipe is often hidden behind complex math and opaque proofs. And it\u2019s usually applied to a seemingly narrow and uninteresting problem. </p>\n<p>In \u2018Amusing Algorithms\u2019 we\u2019ll cut through the math and try to understand the mechanics of a few interesting and useful algorithms. We\u2019ll use Jupyter to expose data structures, intermediate steps, and simulations of various algorithms. And we\u2019ll try to use algorithms to answer real life questions like how to find love in a crowded bar, how to buy the best scalpers tickets at a baseball game, and how to figure out when you should leave your job.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-20T21:11:38.283Z",
            "speaker": 103,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 129,
        "fields": {
            "kind": 1,
            "title": "1 == True and True is 1 or \u2018Python Variable Assignment Can Be Very Interesting\u201d",
            "description": "1 == True and 1 is True. It\u2019s a legal python statement. What does it mean? Why does it matter? \r\n\r\nThis talk is an entertaining demonstration of how python variables works under the hood. We\u2019ll cover variable assignment, passing, the local and global name space, and python facilities for managing them. Recommended for beginning and intermediate programmers.",
            "abstract": "Python variables are virtually intuitive. Unlike many other languages we can create and destroy variables without a thought, usually with a blissful unawareness of what is happening behind the scenes.\r\n\r\nSo what are we going to talk about? Here\u2019s an example:\r\n\r\nHow many objects is the python interpreter creating for this code?\r\n\r\nx = 1\r\ny = 1\r\n\r\na) 2  \r\nb) 3\r\nc) 4\r\nd) 8\r\n\r\nIn this talk I will demonstrate with examples some interesting puzzles about python variable handling.  Topics will include variable passing, global and local scopes, mutable and immutable variables and variable identification. This is a fun presentation that covers some subtle but important characteristics of the language.\r\n\r\nAnswer: Don't know? Come to the talk.",
            "abstract_html": "<p>Python variables are virtually intuitive. Unlike many other languages we can create and destroy variables without a thought, usually with a blissful unawareness of what is happening behind the scenes.</p>\n<p>So what are we going to talk about? Here\u2019s an example:</p>\n<p>How many objects is the python interpreter creating for this code?</p>\n<p>x = 1\ny = 1</p>\n<p>a) 2<br />\nb) 3\nc) 4\nd) 8</p>\n<p>In this talk I will demonstrate with examples some interesting puzzles about python variable handling.  Topics will include variable passing, global and local scopes, mutable and immutable variables and variable identification. This is a fun presentation that covers some subtle but important characteristics of the language.</p>\n<p>Answer: Don't know? Come to the talk.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-20T22:05:39.183Z",
            "speaker": 104,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 130,
        "fields": {
            "kind": 1,
            "title": "An Absolute Beginner's Guide to Deep Learning with Keras",
            "description": "Keras is an amazing library that simplifies the coding of deep learning models. We'll begin with a brief intro to neural networks (NNs). Then demo the building of a convolutional neural net (CNN) layer-by-layer. By the end, you should be able to build your simple deep learning models and understand what each element of a neural network does.",
            "abstract": "Interest in deep learning and building artificial intelligence (AI) based applications has been growing in the past few years. The Keras library makes these techniques accessible by offering a high-level API capable of running on top of TensorFlow, CNTK, or Theano. Similar to Python in general, Keras puts user experience front and center by having an API designed for human beings (not machines). This makes Keras perfect for easy and fast prototyping. Keras also highly modular and easy to extend to more complex deep learning models. This talk will be an introduction to neural networks and the Keras API for building them. We'll walk step-by-step how to build a convolutional neural net (CNN).",
            "abstract_html": "<p>Interest in deep learning and building artificial intelligence (AI) based applications has been growing in the past few years. The Keras library makes these techniques accessible by offering a high-level API capable of running on top of TensorFlow, CNTK, or Theano. Similar to Python in general, Keras puts user experience front and center by having an API designed for human beings (not machines). This makes Keras perfect for easy and fast prototyping. Keras also highly modular and easy to extend to more complex deep learning models. This talk will be an introduction to neural networks and the Keras API for building them. We'll walk step-by-step how to build a convolutional neural net (CNN).</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-21T00:54:43.260Z",
            "speaker": 105,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 131,
        "fields": {
            "kind": 1,
            "title": "Clearer Code at Scale: Static Types at Zulip and Dropbox",
            "description": "Python now offers static types! Companies like Dropbox and Facebook, and open-source projects like Zulip, use static types (with PEP 484 and mypy) to make Python more productive and fun to work with \u2014 in existing codebases from 40k lines to 4 million, in Python 2 and 3, and while preserving the conciseness and flexibility that make Python a great language in the first place. I\u2019ll describe how.",
            "abstract": "Python now offers static types! Companies like Dropbox and Facebook, and open-source projects like Zulip, use static types (with PEP 484 and mypy) to make Python more productive and fun to work with \u2014 in existing codebases from 40k lines to 4 million, in Python 2 and 3, and while preserving the conciseness and flexibility that make Python a great language in the first place. I\u2019ll describe how.\r\n\r\nReading and understanding code is a huge part of what we do as software developers. If we make it easier to understand our codebases, we make everyone more productive, help each other write fewer bugs, and lower barriers for new contributors. That's why Python now features optional static types, and why Dropbox, Facebook, and Zulip use them on part or all of their Python code.\r\n\r\nIn this talk, I\u2019ll share lessons from Zulip\u2019s and Dropbox\u2019s experience \u2014 having led the mypy team at Dropbox and working now on the Zulip core team \u2014 for how you can start using static types in your own codebases, large or small. We\u2019ll discuss how to make it a seamless part of your project\u2019s tooling; what order to approach things in; and powerful new tools that make it even easier today to add static types to your Python codebase than ever before.",
            "abstract_html": "<p>Python now offers static types! Companies like Dropbox and Facebook, and open-source projects like Zulip, use static types (with PEP 484 and mypy) to make Python more productive and fun to work with \u2014 in existing codebases from 40k lines to 4 million, in Python 2 and 3, and while preserving the conciseness and flexibility that make Python a great language in the first place. I\u2019ll describe how.</p>\n<p>Reading and understanding code is a huge part of what we do as software developers. If we make it easier to understand our codebases, we make everyone more productive, help each other write fewer bugs, and lower barriers for new contributors. That's why Python now features optional static types, and why Dropbox, Facebook, and Zulip use them on part or all of their Python code.</p>\n<p>In this talk, I\u2019ll share lessons from Zulip\u2019s and Dropbox\u2019s experience \u2014 having led the mypy team at Dropbox and working now on the Zulip core team \u2014 for how you can start using static types in your own codebases, large or small. We\u2019ll discuss how to make it a seamless part of your project\u2019s tooling; what order to approach things in; and powerful new tools that make it even easier today to add static types to your Python codebase than ever before.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-21T05:53:16.890Z",
            "speaker": 106,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 132,
        "fields": {
            "kind": 1,
            "title": "Airflow: Automating ETLs for a Data Warehouse",
            "description": "Apache Airflow makes orchestrating complex workflows a breeze. Using Python to define directed acyclic graphs (DAGs) has simplified Optimizely\u2019s extract, transform and load (ETL) pipelines that power our data warehouse. In this talk, we\u2019ll share learnings of how Airflow has allowed Optimizely to scale the systems and people making up the Data Warehouse team.",
            "abstract": "Extract, transform and load (ETL) is a foundational part of data warehousing. Data warehousing is a foundational part of Optimizely. We help others make data driven decisions, so it\u2019s part of our culture to make decisions in the same way. Through a centralized data warehouse and visualization capabilities, everyone at Optimizely is able to make data driven decisions that help grow our business.\r\n\r\nThis centralized data warehouse is owned by the Data Warehouse team. We\u2019re responsible for bringing together ~25 services\u2019 and applications\u2019 data to provide business analysts and data scientists a holistic view into how customers are using our products. Optimizely\u2019s data warehouse what started out as a homegrown, in-house, and hard-to-maintain system turned to Apache Airflow to standardize and simplify the overall architecture.\r\n\r\nIn this talk, we\u2019ll share our learnings along the way so you can bring them back to your team and make your ETLs a breeze too:\r\nIntroduction to data warehousing and why it\u2019s important\r\nOverview of ETLs in theory and practice\r\nHow we used this process to build a robust data warehouse\r\nOverview of Apache Airflow in theory and practice\r\nHow we made our lives better with Airflow",
            "abstract_html": "<p>Extract, transform and load (ETL) is a foundational part of data warehousing. Data warehousing is a foundational part of Optimizely. We help others make data driven decisions, so it\u2019s part of our culture to make decisions in the same way. Through a centralized data warehouse and visualization capabilities, everyone at Optimizely is able to make data driven decisions that help grow our business.</p>\n<p>This centralized data warehouse is owned by the Data Warehouse team. We\u2019re responsible for bringing together ~25 services\u2019 and applications\u2019 data to provide business analysts and data scientists a holistic view into how customers are using our products. Optimizely\u2019s data warehouse what started out as a homegrown, in-house, and hard-to-maintain system turned to Apache Airflow to standardize and simplify the overall architecture.</p>\n<p>In this talk, we\u2019ll share our learnings along the way so you can bring them back to your team and make your ETLs a breeze too:\nIntroduction to data warehousing and why it\u2019s important\nOverview of ETLs in theory and practice\nHow we used this process to build a robust data warehouse\nOverview of Apache Airflow in theory and practice\nHow we made our lives better with Airflow</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-21T16:35:40.706Z",
            "speaker": 107,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 133,
        "fields": {
            "kind": 1,
            "title": "Detecting business chains at scale with PySpark and machine learning",
            "description": "You\u2019re driving to Tahoe for the slopes. What\u2019s the one thing more important than the snow report? Of course, where the nearest In-N-Out is!\r\n\r\nYelp has a special chain search for all your guilty pleasures, powered by our Chain Detector system that we just rearchitected\r\n\r\nLearn how we leveraged Spark on AWS and py3, moving away from Redshift+py27, for better scalability and maintainability",
            "abstract": "Chain Detector is a system at Yelp that automatically detects all chain businesses in the world. It is an important data set that helps us deliver a special user experience in search (\u201cShow me all the Philz Coffee locations in San Francisco\u201d). It also powers an API for these chains to understand what users are saying about them across all of their locations.\r\n\r\nThe application needs to make many complicated transformations and aggregations on all of our business data and put these through a machine learning classifier to come up with the final data set. It is a hard problem because you often deal with imperfect data. For example, you may have many Super Duper listings, and another named \u201cSuper Duper Burgers\u201d. Should they all be classified as the same chain?\r\n \r\nThe original system was built in Python 2.7 and relied on Redshift to execute massively parallelized queries. This wasn\u2019t fast enough and also used 3rd party Python 2.7 libraries to parallelize the Redshift queries even further.\r\n\r\nThis made the optimization logic heavily embedded with the application logic, so it was difficult to reason about the core algorithm. It also relied on too many unit tests mocking out Redshift calls, thus more prone to regressions and mocks that are tedious to maintain. Integration testing with Redshift is not ideal because you cannot readily sandbox clusters and only one developer can run tests at a time.\r\n\r\nWe explored Spark, a fast growing technology at Yelp, and after being impressed with prototypes, we rearchitected the rest of the system to use Spark instead of Redshift for all the data manipulation and machine learning. We were able to do this while keeping the application logic in tact with no regressions, and while upgrading to Python 3.6 in the process!\r\n\r\nSpark allowed us to naturally express the data manipulations and the core algorithm. And by using Spark and setting up proper file system abstractions, it also enabled us to do local stateless end-to-end testing in pytest -- no more mocking in unit tests! The application is also 4x faster.  However, we faced many challenges because some Python code is not compatible with the Spark paradigm and it can be tricky to get your dependencies setup to be compatible to run on AWS.\r\n\r\nOverall, we now have a faster, more modern and maintainable system that has solved our infrastructure issues. In addition, it has allowed us to safely add new features, such as supporting custom overrides from an admin through a UI, and ensure the final data set is still computed correctly.",
            "abstract_html": "<p>Chain Detector is a system at Yelp that automatically detects all chain businesses in the world. It is an important data set that helps us deliver a special user experience in search (\u201cShow me all the Philz Coffee locations in San Francisco\u201d). It also powers an API for these chains to understand what users are saying about them across all of their locations.</p>\n<p>The application needs to make many complicated transformations and aggregations on all of our business data and put these through a machine learning classifier to come up with the final data set. It is a hard problem because you often deal with imperfect data. For example, you may have many Super Duper listings, and another named \u201cSuper Duper Burgers\u201d. Should they all be classified as the same chain?</p>\n<p>The original system was built in Python 2.7 and relied on Redshift to execute massively parallelized queries. This wasn\u2019t fast enough and also used 3rd party Python 2.7 libraries to parallelize the Redshift queries even further.</p>\n<p>This made the optimization logic heavily embedded with the application logic, so it was difficult to reason about the core algorithm. It also relied on too many unit tests mocking out Redshift calls, thus more prone to regressions and mocks that are tedious to maintain. Integration testing with Redshift is not ideal because you cannot readily sandbox clusters and only one developer can run tests at a time.</p>\n<p>We explored Spark, a fast growing technology at Yelp, and after being impressed with prototypes, we rearchitected the rest of the system to use Spark instead of Redshift for all the data manipulation and machine learning. We were able to do this while keeping the application logic in tact with no regressions, and while upgrading to Python 3.6 in the process!</p>\n<p>Spark allowed us to naturally express the data manipulations and the core algorithm. And by using Spark and setting up proper file system abstractions, it also enabled us to do local stateless end-to-end testing in pytest -- no more mocking in unit tests! The application is also 4x faster.  However, we faced many challenges because some Python code is not compatible with the Spark paradigm and it can be tricky to get your dependencies setup to be compatible to run on AWS.</p>\n<p>Overall, we now have a faster, more modern and maintainable system that has solved our infrastructure issues. In addition, it has allowed us to safely add new features, such as supporting custom overrides from an admin through a UI, and ensure the final data set is still computed correctly.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-21T18:38:30.863Z",
            "speaker": 108,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 134,
        "fields": {
            "kind": 1,
            "title": "Xnd: An extensible framework for typed memory blocks and computational kernels",
            "description": "Xnd is a set of C libraries and Python modules for typed containers and computational kernels.\r\n\r\nThe type syntax is the concise Datashape notation that is sufficient for describing all C types relevant for scientific computing and has support for ragged arrays and optional types.\r\n\r\nThe clean separation of C libraries and Python modules facilitates memory and type access by other applications.",
            "abstract": "Python's scientific ecosystem has a long history of monolithic applications that a) define their own types and b) make it difficult to access the underlying raw memory blocks.\r\n\r\nXnd takes a fundamental approach to the problem and defines a general container that essentially consists of a memory pointer and a type.\r\n\r\nThe type, provided by libndtypes, is a standard algebraic data type of the sort that can be found in many compiler textbooks.  Additionally, the type contains the exact memory layout information.\r\n\r\nlibxnd combines the type and the memory pointer into a lightweight container and is responsible for element access, indexing and slicing.  At all stages types are preserved.\r\n\r\nlibgumath provides the computational infrastructure, whose functions operate on xnd containers.\r\n\r\n\r\nThe Python modules ndtypes, xnd and gumath are wrappers for the corresponding libraries.",
            "abstract_html": "<p>Python's scientific ecosystem has a long history of monolithic applications that a) define their own types and b) make it difficult to access the underlying raw memory blocks.</p>\n<p>Xnd takes a fundamental approach to the problem and defines a general container that essentially consists of a memory pointer and a type.</p>\n<p>The type, provided by libndtypes, is a standard algebraic data type of the sort that can be found in many compiler textbooks.  Additionally, the type contains the exact memory layout information.</p>\n<p>libxnd combines the type and the memory pointer into a lightweight container and is responsible for element access, indexing and slicing.  At all stages types are preserved.</p>\n<p>libgumath provides the computational infrastructure, whose functions operate on xnd containers.</p>\n<p>The Python modules ndtypes, xnd and gumath are wrappers for the corresponding libraries.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-21T19:11:45.525Z",
            "speaker": 109,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 135,
        "fields": {
            "kind": 1,
            "title": "Python Scripting for Noobs (or: How I Learned to Stop Shell Scripting and Love the StdLib)",
            "description": "My first assignment at Sentry was to write a tool that monitored Google Cloud resource quotas and alerted us on Slack whenever our servers reached a specific threshold. How did a self-proclaimed lover of shell scripts end up using python? What lessons about working on an engineering team did I learn in the process?",
            "abstract": "On the web console for Google Cloud Platform (GCP), users can see and manage resources. Google also provides a command line tool (gcloud) that also allows users to interact with GCP APIs. My assignment was to create an internal tool that would use information from gcloud to calculate percentages that we would otherwise have to grab from the GCP console compute engine resource quotas.\r\n\r\nFortunately, I knew shell scripting! I wrote version 1 (pure Bash with lots of sed and grep) and version 2 (jq is a great utility for parsing JSON) but experienced frustration using binaries that varied on different platforms. Finally, it was time to reach for Python and use excellent stdlib modules like argparse, json, and subprocess to create a better tool.\r\n\r\nThrough the process, I l gained some important insights about learning on the job in an engineering organization and the value of readable code.",
            "abstract_html": "<p>On the web console for Google Cloud Platform (GCP), users can see and manage resources. Google also provides a command line tool (gcloud) that also allows users to interact with GCP APIs. My assignment was to create an internal tool that would use information from gcloud to calculate percentages that we would otherwise have to grab from the GCP console compute engine resource quotas.</p>\n<p>Fortunately, I knew shell scripting! I wrote version 1 (pure Bash with lots of sed and grep) and version 2 (jq is a great utility for parsing JSON) but experienced frustration using binaries that varied on different platforms. Finally, it was time to reach for Python and use excellent stdlib modules like argparse, json, and subprocess to create a better tool.</p>\n<p>Through the process, I l gained some important insights about learning on the job in an engineering organization and the value of readable code.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-21T19:45:51.976Z",
            "speaker": 110,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 136,
        "fields": {
            "kind": 1,
            "title": "What Drove that Result? - Techniques & Tradeoffs for Interpreting ML Result",
            "description": "Interpretation of Machine Learning model results is an enduring challenge, especially when communicating results to laymen. This talk reviews the pitfalls and remedies of interpretation using more traditional and newer techniques. Both Machine Learning and Deep Learning are covered.",
            "abstract": "Interpretation of Machine Learning model results is an enduring challenge, especially when communicating results to laymen. This talk reviews the pitfalls and remedies of interpretation using more traditional and newer techniques. \r\n\r\nAnalytical techniques including use of coefficients for regression, feature importance in classification models, and the more recent Shapley Additive exPlanations are explored. Different visualization techniques are also explored as essential parts of a coherent interpretation methodology.\r\n\r\nBoth Machine Learning and Deep Learning are covered, with a focus on the SKLearn and Pytorch libraries.",
            "abstract_html": "<p>Interpretation of Machine Learning model results is an enduring challenge, especially when communicating results to laymen. This talk reviews the pitfalls and remedies of interpretation using more traditional and newer techniques. </p>\n<p>Analytical techniques including use of coefficients for regression, feature importance in classification models, and the more recent Shapley Additive exPlanations are explored. Different visualization techniques are also explored as essential parts of a coherent interpretation methodology.</p>\n<p>Both Machine Learning and Deep Learning are covered, with a focus on the SKLearn and Pytorch libraries.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-21T21:04:11.254Z",
            "speaker": 111,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 137,
        "fields": {
            "kind": 1,
            "title": "Anatomy of Open edX - a modern online learning platform serving over 26 million users",
            "description": "You may have heard of edX.org, the MOOC site created by Harvard and MIT, but did you know that the software powering this site is open source and written in Python? We\u2019ll do a technical deep dive and explore how this software is built in a scalable way to serve millions of concurrent learners, and also show you how you can create your own Open edX site to deliver online courses at scale.",
            "abstract": "What does it take to build a web application that can serve millions of concurrent users? This talk will dissect Open edX, the open source online learning platform that powers edX.org and hundreds of other sites around the world.\r\n\r\nOpen edX is written in Django but relies on a slew of other software to work at scale. We\u2019ll break down the components of the entire software architecture, and explain how each component is used. This will give attendees some insights into how they might architect their web application if they\u2019re building software to serve a large audience.\r\n\r\nWe\u2019ll discuss how Open edX utilizes both MySQL and MongoDB as data stores and why it uses MySQL for student data and MongoDB for course data. We\u2019ll describe the queuing architecture of Celery and RabbitMQ to process background jobs, and Hadoop for processing terabytes of learner analytics data. \r\n\r\nDevOps enthusiasts will not be disappointed as we\u2019ll go into the use of Ansible and Terraform for deploying the entire suite of software in a consistent and repeatable way up to popular cloud providers such as AWS, Google Cloud and Azure.",
            "abstract_html": "<p>What does it take to build a web application that can serve millions of concurrent users? This talk will dissect Open edX, the open source online learning platform that powers edX.org and hundreds of other sites around the world.</p>\n<p>Open edX is written in Django but relies on a slew of other software to work at scale. We\u2019ll break down the components of the entire software architecture, and explain how each component is used. This will give attendees some insights into how they might architect their web application if they\u2019re building software to serve a large audience.</p>\n<p>We\u2019ll discuss how Open edX utilizes both MySQL and MongoDB as data stores and why it uses MySQL for student data and MongoDB for course data. We\u2019ll describe the queuing architecture of Celery and RabbitMQ to process background jobs, and Hadoop for processing terabytes of learner analytics data. </p>\n<p>DevOps enthusiasts will not be disappointed as we\u2019ll go into the use of Ansible and Terraform for deploying the entire suite of software in a consistent and repeatable way up to popular cloud providers such as AWS, Google Cloud and Azure.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-21T21:08:18.945Z",
            "speaker": 112,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 138,
        "fields": {
            "kind": 1,
            "title": "How can software combat climate change?",
            "description": "Have you ever wanted to know how climate change actually works and what you, as a software engineer, can do about it? In this talk, I will describe how climate change works and how software is making possible the largest global infrastructure transition of our lifetimes.",
            "abstract": "Energy is big. Really big. You just won't believe how vastly, hugely, mindbogglingly big it is. Every year we use over 520 Quadrillion BTUs of energy, and over 80% of it comes from fossil fuels. We are currently switching all of that (>450 QBTUs) to carbon-free sources. Bloomberg calls it a $10 trillion dollar opportunity in the next 25 years. That's faster growth than the Internet.\r\n\r\nBut it's not all hardware like solar panels and LED lights. Half of the energy transition will be due to software. Did you know that less than half of the cost of installing distributed solar is hardware and labor? The majority is actually soft-costs like customer acquisition, engineering, permitting, and operations. Those can be solved with software! The same is true with smart grids, automated electric vehicles, public transit, energy efficiency, and many more new energy technologies.\r\n\r\nIn this talk, I will go through how climate change works and what we will experience, from an engineer's point of view. Then, I will talk about the software technologies that are currently in place, what we are doing now, and where we are going in the next 25 years. Half of the talk will be open for questions, so if you have every wanted to ask something about climate change or clean energy, now is your chance!",
            "abstract_html": "<p>Energy is big. Really big. You just won't believe how vastly, hugely, mindbogglingly big it is. Every year we use over 520 Quadrillion BTUs of energy, and over 80% of it comes from fossil fuels. We are currently switching all of that (&gt;450 QBTUs) to carbon-free sources. Bloomberg calls it a $10 trillion dollar opportunity in the next 25 years. That's faster growth than the Internet.</p>\n<p>But it's not all hardware like solar panels and LED lights. Half of the energy transition will be due to software. Did you know that less than half of the cost of installing distributed solar is hardware and labor? The majority is actually soft-costs like customer acquisition, engineering, permitting, and operations. Those can be solved with software! The same is true with smart grids, automated electric vehicles, public transit, energy efficiency, and many more new energy technologies.</p>\n<p>In this talk, I will go through how climate change works and what we will experience, from an engineer's point of view. Then, I will talk about the software technologies that are currently in place, what we are doing now, and where we are going in the next 25 years. Half of the talk will be open for questions, so if you have every wanted to ask something about climate change or clean energy, now is your chance!</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-21T21:23:55.375Z",
            "speaker": 113,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 139,
        "fields": {
            "kind": 1,
            "title": "Allofplos, get, manage and analyse all PLOS papers",
            "description": "More than 200,000 scientific articles are available on the Public Library of Science website. For those who want to do text mining on the PLOS corpus we made all articles available with a python library alongside with tools to process them.\r\nWith the methods included in this library, researchers will be able to extract selected parts of articles and also make queries not possible with other tools.",
            "abstract": "PLOS (Public Library of Science) is the largest open access science, technology and medicine publisher. \r\nThis library comes with a small sample corpus (in XML files) that can be used to learn how to use allofplos classes as an API to process article contents without XML parsing.\r\nIt also has a method to download the whole corpus.\r\nIt included a sample SQLite database with the sample corpus and instructions on how to generate a custom database. With the DB in place, the end user could make customs SQL queries.\r\nThese tools can be handy for data mining, text analysis, metascience, and more.",
            "abstract_html": "<p>PLOS (Public Library of Science) is the largest open access science, technology and medicine publisher. \nThis library comes with a small sample corpus (in XML files) that can be used to learn how to use allofplos classes as an API to process article contents without XML parsing.\nIt also has a method to download the whole corpus.\nIt included a sample SQLite database with the sample corpus and instructions on how to generate a custom database. With the DB in place, the end user could make customs SQL queries.\nThese tools can be handy for data mining, text analysis, metascience, and more.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-21T23:03:45.185Z",
            "speaker": 114,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 140,
        "fields": {
            "kind": 1,
            "title": "Monitoring Anything and Everything with Python + Grafana",
            "description": "Data processing pipeline is now common architecture for all infrastructure. In order to serve end users and applications that using these infrastructure, the monitoring & alerting is required. So, you will need a single platform to monitor your all the components.That\u2019s where Grafana comes into play. Grafana has a pluggable architecture and in built alert mechanism.",
            "abstract": "We will demo attendees how to create a basic monitoring & alerting dashboard using python & grafana. We will also add default metrics template for different big data components such as  Kafka, Elasticsearch, Kubernetes, MongoDB, HDFS and 3rd party HTTP API's. With that, user do not need to configure anything on platform side for these components and can visualize the infrastructure just by providing the endpoint information.",
            "abstract_html": "<p>We will demo attendees how to create a basic monitoring &amp; alerting dashboard using python &amp; grafana. We will also add default metrics template for different big data components such as  Kafka, Elasticsearch, Kubernetes, MongoDB, HDFS and 3rd party HTTP API's. With that, user do not need to configure anything on platform side for these components and can visualize the infrastructure just by providing the endpoint information.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-21T23:06:06.578Z",
            "speaker": 115,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 141,
        "fields": {
            "kind": 1,
            "title": "Communication Ethics and Comedy",
            "description": "PMs at Heroku, Lever, Checkr, Google, and Facebook have given the same answer in informational interviews: Communication is one of the most important skills an engineer can have, and yet it is often an area that needs improvement. With the challenges facing engineers in 2018--from ethics to diversity--it is a valuable skill to build. And why not build it with comedy?",
            "abstract": "Meaningful conversations that shape the workplace culture are difficult, especially in a time of ethical scrutiny on the tech industry for privacy violation, forced arbitration, and the alienation of women and people of color. How do we engage in those conversations effectively?\r\n\r\nThis talk aims to unpack the complex pain points of meaningful communication within teams, with managers and mentors, and in pair programming. Specifically, it addresses:\r\n\r\n- empathetic work environment, and its advantages\r\n- how we foster ethical behavior\r\n- common gaffes and false equivalencies\r\n- the communication burden for minorities in tech\r\n\r\nTo break these academic concepts down, the talk will introduce the underlying principles of improv comedy, bringing the stage mantra of \u201cI\u2019ve got your back\u201d into a context for allies and coworkers. \r\nThese principles can help guide professional communication, and will discuss punching up, character bias, and the classic \u201cyes, and.\u201d",
            "abstract_html": "<p>Meaningful conversations that shape the workplace culture are difficult, especially in a time of ethical scrutiny on the tech industry for privacy violation, forced arbitration, and the alienation of women and people of color. How do we engage in those conversations effectively?</p>\n<p>This talk aims to unpack the complex pain points of meaningful communication within teams, with managers and mentors, and in pair programming. Specifically, it addresses:</p>\n<ul>\n<li>empathetic work environment, and its advantages</li>\n<li>how we foster ethical behavior</li>\n<li>common gaffes and false equivalencies</li>\n<li>the communication burden for minorities in tech</li>\n</ul>\n<p>To break these academic concepts down, the talk will introduce the underlying principles of improv comedy, bringing the stage mantra of \u201cI\u2019ve got your back\u201d into a context for allies and coworkers. \nThese principles can help guide professional communication, and will discuss punching up, character bias, and the classic \u201cyes, and.\u201d</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T00:37:40.161Z",
            "speaker": 116,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 142,
        "fields": {
            "kind": 1,
            "title": "Production ML",
            "description": "There\u2019s increasing demand for querying ML models as part of a live product, but many questions specific to this task remain unanswered. How can new models be quickly validated? What is the role of online and offline metrics in the production model lifecycle? What is a production model lifecycle?",
            "abstract": "In this talk, I\u2019ll cover some of the problems I\u2019ve faced bringing ML into production products.\r\nI cover design considerations for the model, product, and server that keep things simple.\r\nI discuss how to quickly validate model and feature changes to meet the need for fast turn-around time.\r\nI address common questions around offline and online metrics, reporting, and analytics.\r\n \r\nThis talk is best enjoyed with a basic understanding of Machine Learning and of server development.",
            "abstract_html": "<p>In this talk, I\u2019ll cover some of the problems I\u2019ve faced bringing ML into production products.\nI cover design considerations for the model, product, and server that keep things simple.\nI discuss how to quickly validate model and feature changes to meet the need for fast turn-around time.\nI address common questions around offline and online metrics, reporting, and analytics.</p>\n<p>This talk is best enjoyed with a basic understanding of Machine Learning and of server development.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T02:06:27.456Z",
            "speaker": 117,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 143,
        "fields": {
            "kind": 1,
            "title": "Community - from the inside out",
            "description": "Build your own team and community skills as intentionally and specifically as you would programming language skills.  Why \"come for the language, but stay for the community\" - without intentionally maintaining and building the well functioning aspects of the community?  You can, and we'll show some fundamentals of how.",
            "abstract": "Awareness of a few aspects neuro-biology, cognition, sensory I/O, and storage formats will help set the stage.  Competence and openness makes for the most effective teams, and we'll look at why:  how this works, and how to intentionally put it to work.  We'll talk about concepts of cognitive encapsulation, variations of concept storage, data structures of team communication, and some transcription issues.",
            "abstract_html": "<p>Awareness of a few aspects neuro-biology, cognition, sensory I/O, and storage formats will help set the stage.  Competence and openness makes for the most effective teams, and we'll look at why:  how this works, and how to intentionally put it to work.  We'll talk about concepts of cognitive encapsulation, variations of concept storage, data structures of team communication, and some transcription issues.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T02:40:06.302Z",
            "speaker": 118,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 144,
        "fields": {
            "kind": 1,
            "title": "Reproducible performance - profiling all the code, all the time, for free.",
            "description": "This talk will teach you more than you ever wanted to know about measuring and improving the performance of your Python code - even if your program is actually spending most of its time somewhere else. We'll cover a set of improvements in VMProf that make it possible to measure your code all the way down to the C level with next to no overhead - so that you can profile all the code, all the time.",
            "abstract": "How would you diagnose a performance issue after is had already happened? How can you tell if your code is about to explode before it actually does? How to tell if your latest slowdown around NumPy is happening at the Python level, or at the C level? Does the Django ORM hate your guts, or is it just Postgres being slow on the first query of the last Friday of the month?\r\n\r\nIn this talk you\u2019ll be equipped with the ability to answer all of these questions - and more! - within two clicks of the mouse\u2026 and if that\u2019s not possible, you\u2019ll learn what your options are.\r\n\r\nWe\u2019ll explore the radical idea of always running with a profiler enabled, and the far-reaching implications of actually being able to do that in production, with almost no run-time overhead.",
            "abstract_html": "<p>How would you diagnose a performance issue after is had already happened? How can you tell if your code is about to explode before it actually does? How to tell if your latest slowdown around NumPy is happening at the Python level, or at the C level? Does the Django ORM hate your guts, or is it just Postgres being slow on the first query of the last Friday of the month?</p>\n<p>In this talk you\u2019ll be equipped with the ability to answer all of these questions - and more! - within two clicks of the mouse\u2026 and if that\u2019s not possible, you\u2019ll learn what your options are.</p>\n<p>We\u2019ll explore the radical idea of always running with a profiler enabled, and the far-reaching implications of actually being able to do that in production, with almost no run-time overhead.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T02:52:44.625Z",
            "speaker": 119,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 145,
        "fields": {
            "kind": 1,
            "title": "Parsing as a Programming Pattern: From Practice to Principle",
            "description": "Problems are easiest to solve when you put them in your terms. While the right abstractions may not be easy to find at first, without them, software will not scale. Enough digging ditches with spoons. We're engineers. We can do better.\r\n\r\nCome learn how senior engineers translate the 1s and 0s of a bytestream into high-level objects, modeling solutions to a wide-range of modern software problems.",
            "abstract": "From grammars to state machines, the patterns used in parsing generalize to all manner of problem a Python programmer encounters in day-to-day development. Most of these architectural tools are already in use all around us, we just need to learn to see them. From decoding bytestrings to calling json.loads(), we are constantly parsing our way to richer semantics and more usable development environments.\r\n\r\nWe'll start with these familiar examples of parsing, explaining how they work underneath the hood, then talk about how they flow together to create a reliable data pipeline. From there we'll be able to see where more-experienced developers continue those robust patterns to write scalable software. We'll look at examples from industry and open-source, including schema, attrs, parsley, and others to unlock the potential that parsing can bring your applications.",
            "abstract_html": "<p>From grammars to state machines, the patterns used in parsing generalize to all manner of problem a Python programmer encounters in day-to-day development. Most of these architectural tools are already in use all around us, we just need to learn to see them. From decoding bytestrings to calling json.loads(), we are constantly parsing our way to richer semantics and more usable development environments.</p>\n<p>We'll start with these familiar examples of parsing, explaining how they work underneath the hood, then talk about how they flow together to create a reliable data pipeline. From there we'll be able to see where more-experienced developers continue those robust patterns to write scalable software. We'll look at examples from industry and open-source, including schema, attrs, parsley, and others to unlock the potential that parsing can bring your applications.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T02:59:04.442Z",
            "speaker": 78,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 146,
        "fields": {
            "kind": 1,
            "title": "Bluring the lines between virtual and real world data for CNNs",
            "description": "A walk through of 2D virtual data generation using for analog scene interpretation using a CNN.",
            "abstract": "Virtual data generation is a powerful tool for creating data sets which would otherwise be prohibitively expensive to generate and label.\r\n\r\nThis talk will focus on how to set up basic virtual data generation and labeling for a camera vision system which produces analog descriptive outputs.\r\n\r\nWe'll look at how a network trained on this data performs in the real world and then look at techniques to increase data variability and combat over-fitting.",
            "abstract_html": "<p>Virtual data generation is a powerful tool for creating data sets which would otherwise be prohibitively expensive to generate and label.</p>\n<p>This talk will focus on how to set up basic virtual data generation and labeling for a camera vision system which produces analog descriptive outputs.</p>\n<p>We'll look at how a network trained on this data performs in the real world and then look at techniques to increase data variability and combat over-fitting.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T03:01:39.978Z",
            "speaker": 120,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 147,
        "fields": {
            "kind": 1,
            "title": "In the (__name__) of Flask Blueprints",
            "description": "Flask is often overlooked when you are trying to build out a modular application that can scale, but Flask Blueprints make modularity easy and elegant to achieve. This talk will focus on how Blueprints can be used to create a dynamic, componentized architecture that will help you organize logic and mitigate configuration issues.",
            "abstract": "When first learning the Flask microframework, many developers conclude that Flask is a simple way to build smaller applications but would not consider it for anything with multiple application components. While frameworks like Django and Pyramid have their own methods to extend a single application, Flask Blueprints should not be undervalued for their simplicity in creating and maintaining a componentized architecture. \r\n\r\nSmaller Flask applications are often built using a single server file in which routes and view functions are defined and the application is instantiated and run. While this design makes sense for simpler projects, it forces the developer to register all routes on the main Flask application object, which can make the server file extremely long and code difficult to organize as more routes and features are added. Flask Blueprints were included to allow developers the ability to organize their logic and routes into segmented components which have the flexibility to use common or unique configurations. By registering a Blueprint object on a Flask application object, the Blueprint will often be configured to inherit traits from the main app, but can have separate views, templates, models, and static files depending on the folder structure. This talk will delve into best practices when structuring a scalable Flask app with Blueprints and cover the common pitfalls of working with componentized Flask.",
            "abstract_html": "<p>When first learning the Flask microframework, many developers conclude that Flask is a simple way to build smaller applications but would not consider it for anything with multiple application components. While frameworks like Django and Pyramid have their own methods to extend a single application, Flask Blueprints should not be undervalued for their simplicity in creating and maintaining a componentized architecture. </p>\n<p>Smaller Flask applications are often built using a single server file in which routes and view functions are defined and the application is instantiated and run. While this design makes sense for simpler projects, it forces the developer to register all routes on the main Flask application object, which can make the server file extremely long and code difficult to organize as more routes and features are added. Flask Blueprints were included to allow developers the ability to organize their logic and routes into segmented components which have the flexibility to use common or unique configurations. By registering a Blueprint object on a Flask application object, the Blueprint will often be configured to inherit traits from the main app, but can have separate views, templates, models, and static files depending on the folder structure. This talk will delve into best practices when structuring a scalable Flask app with Blueprints and cover the common pitfalls of working with componentized Flask.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T03:04:38.967Z",
            "speaker": 121,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 148,
        "fields": {
            "kind": 1,
            "title": "Service Testing with Apache Airflow",
            "description": "End-to-end (E2E) testing is a critical step in the testing pyramid to ensure features powered by multiple services are shipped with high quality. To make it easier at Optimizely, we\u2019ve turned to Apache Airflow\u2014a framework that makes it easy to manage complex workflows. In this talk, we\u2019ll share a pattern we use to test our services end-to-end on top of Airflow and how you can easily adopt it!",
            "abstract": "Shipping quality software is hard for so many reasons. As engineers, we try our best to ship the best code we can and use frameworks like the testing pyramid to increase our confidence that we are. While unit tests are usually easy to write and integration tests are easy-ish to write, the final frontier of end-to-end (E2E) tests is much more difficult to develop. Despite their difficulty, E2E tests are critical to ensuring features are built in a way that meets users\u2019 needs at launch and during its lifetime.\r\n\r\nIn a services-oriented architecture, E2E tests become even more difficult to write because services tend to be owned by different teams. We reached this problem at Optimizely when we wanted to have rock-solid quality over counting impressions, which we use for billing our customers\u2014so it\u2019s pretty important we get that right and have confidence that it\u2019s always right.\r\n\r\nThe services we relied on surfaced APIs, and in the end, it\u2019s all data, so we thought: what if we use Apache Airflow to make this easy to write, understand, monitor and schedule? Our intuition was accurate and today we use this system to ensure changes to our counting logic do not cause regressions and provide a bad experience for our customers.\r\n\r\nAirflow defines workflows as directed acyclic graphs (DAGs). By defining a test step as a node in the DAG, we\u2019re able to more easily debug independent failures and can share these steps across teams to reduce maintenance and communication overhead\r\n\r\nWe\u2019re excited by this pattern we\u2019ve been using for more than 6 months because it has proven itself invaluable for us by helping catch critical bugs before they happen. More so, we can extend the same test as a monitor by running on a timer post-production. We hope this simple pattern proves useful and makes E2E testing easier for you too!\r\n\r\nAs an outline, this talk will cover:\r\n0 - 2 min: Why E2E testing is important but hard to implement\r\n2 - 4 min: An introduction to Apache Airflow\r\n4 - 5 min: How Apache Airflow workflows map to E2E tests\r\n5 - 8 min: How to write a simple end-to-test using Airflow\r\n8 - 10 min: The benefits of writing E2E tests this way for the debugger and the tester\r\n10 -16 min: Scaling this pattern across teams by sharing the DAG, not the code\r\n16 - 18 min: Using the same E2E test to also monitor in production\r\n18 - 20 min: Conclusion and wrap-up\r\n20 - 25 min: Open for questions",
            "abstract_html": "<p>Shipping quality software is hard for so many reasons. As engineers, we try our best to ship the best code we can and use frameworks like the testing pyramid to increase our confidence that we are. While unit tests are usually easy to write and integration tests are easy-ish to write, the final frontier of end-to-end (E2E) tests is much more difficult to develop. Despite their difficulty, E2E tests are critical to ensuring features are built in a way that meets users\u2019 needs at launch and during its lifetime.</p>\n<p>In a services-oriented architecture, E2E tests become even more difficult to write because services tend to be owned by different teams. We reached this problem at Optimizely when we wanted to have rock-solid quality over counting impressions, which we use for billing our customers\u2014so it\u2019s pretty important we get that right and have confidence that it\u2019s always right.</p>\n<p>The services we relied on surfaced APIs, and in the end, it\u2019s all data, so we thought: what if we use Apache Airflow to make this easy to write, understand, monitor and schedule? Our intuition was accurate and today we use this system to ensure changes to our counting logic do not cause regressions and provide a bad experience for our customers.</p>\n<p>Airflow defines workflows as directed acyclic graphs (DAGs). By defining a test step as a node in the DAG, we\u2019re able to more easily debug independent failures and can share these steps across teams to reduce maintenance and communication overhead</p>\n<p>We\u2019re excited by this pattern we\u2019ve been using for more than 6 months because it has proven itself invaluable for us by helping catch critical bugs before they happen. More so, we can extend the same test as a monitor by running on a timer post-production. We hope this simple pattern proves useful and makes E2E testing easier for you too!</p>\n<p>As an outline, this talk will cover:\n0 - 2 min: Why E2E testing is important but hard to implement\n2 - 4 min: An introduction to Apache Airflow\n4 - 5 min: How Apache Airflow workflows map to E2E tests\n5 - 8 min: How to write a simple end-to-test using Airflow\n8 - 10 min: The benefits of writing E2E tests this way for the debugger and the tester\n10 -16 min: Scaling this pattern across teams by sharing the DAG, not the code\n16 - 18 min: Using the same E2E test to also monitor in production\n18 - 20 min: Conclusion and wrap-up\n20 - 25 min: Open for questions</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T03:29:54.847Z",
            "speaker": 122,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 149,
        "fields": {
            "kind": 1,
            "title": "The bots are coming! Writing chatbots with Python",
            "description": "With the popularity of Slack & similar corporate messaging platforms, there's been a recent rise in chatbots, simple applications operating as \"service workers\" in chat rooms. With bots, users can request information & automate tasks, all from user messages. In this session, you'll learn how to create chatbots using Python for both Slack & Google's Hangouts Chat service available for G Suite users",
            "abstract": "Chatbots are simple computer applications or small artificial intelligence entities that interact with humans or other bots in messaging platforms. An evolution of simplistic instant messaging platforms, newer, more intelligent platforms such as Slack, Atlassian Stride, Cisco Spark, Microsoft Teams, and Hangouts Chat from Google represent a new breed of tools focused on organizations and work tasks.\r\n\r\nSlack has been leading the workplace communication landscape since it launched in 2013 and quicly rose to prominence. More than just a messaging app, Slack differentiates itself from the rest of the field by supporting attachments, search, separate workspaces, and a 3rd-party app development platform. Earlier this year, Google's G Suite team launched their own collaboration, Hangouts Chat, to general availability, including its bot framework and API. Developers can create bot integrations to streamline work, automate tasks, or give users new ways to connect with G Suite apps or data.\r\n\r\nChatbots can cover a variety of applications, from e-commerce to automation to information requests to out-of-band alerts and notifications. With a simple message in a chat room, users can query for information, find relevant customer documents, or perform other heavy-lifting that would otherwise require humans to interact with N systems and requiring manual collation. This session gives attendees an overview of chatbots and demonstrates building simple bots on both platforms. Whether for your organization or your customers, chatbots bring to life the next generation intelligent collaboration platform. Get started today!",
            "abstract_html": "<p>Chatbots are simple computer applications or small artificial intelligence entities that interact with humans or other bots in messaging platforms. An evolution of simplistic instant messaging platforms, newer, more intelligent platforms such as Slack, Atlassian Stride, Cisco Spark, Microsoft Teams, and Hangouts Chat from Google represent a new breed of tools focused on organizations and work tasks.</p>\n<p>Slack has been leading the workplace communication landscape since it launched in 2013 and quicly rose to prominence. More than just a messaging app, Slack differentiates itself from the rest of the field by supporting attachments, search, separate workspaces, and a 3rd-party app development platform. Earlier this year, Google's G Suite team launched their own collaboration, Hangouts Chat, to general availability, including its bot framework and API. Developers can create bot integrations to streamline work, automate tasks, or give users new ways to connect with G Suite apps or data.</p>\n<p>Chatbots can cover a variety of applications, from e-commerce to automation to information requests to out-of-band alerts and notifications. With a simple message in a chat room, users can query for information, find relevant customer documents, or perform other heavy-lifting that would otherwise require humans to interact with N systems and requiring manual collation. This session gives attendees an overview of chatbots and demonstrates building simple bots on both platforms. Whether for your organization or your customers, chatbots bring to life the next generation intelligent collaboration platform. Get started today!</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T05:13:47.445Z",
            "speaker": 123,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 150,
        "fields": {
            "kind": 1,
            "title": "Python 3: The Next Generation (is here already)",
            "description": "Python 3 is now 9 years old. When 3.0 launched at the end of 2008, I forecast it would take a decade for the world to migrate because of its incompatibility w/2.x. According to the Python 3 Wall of Superpowers, 94.5% of all major packages have migrated, so we're well on-track. But what about those still on 2.x? This talk covers key 2.x & 3.x differences & migration tips to get you to 3.x tomorrow!",
            "abstract": "In this talk I will discuss the evolution of Python & answer common FAQs. There are those who worry that Python 3 is backwards-incompatible to Python 2. We address that issue, discuss what the main differences are, talk migration, the roles of 2.6/2.7 & other transition tools.\r\n\r\nPython is currently at a crossroads: Python 2 has taken it from a quiet word-of-mouth language to primetime, with many companies around the world using it and an ever-increasing global marketshare of the programming world. But now comes Python 3, the first version of the language that is not backwards compatible with previous releases. This large change brings up many questions that have become FAQs:\r\n\r\n* What does this mean?\r\n* Are all my Python programs going to break?\r\n* Will I have to rewrite everything?\r\n* How much time do I have?\r\n* When is Python 2 going to be EOL'd?\r\n* Is the language undergoing a complete rewrite and will I even recognize it?\r\n* What are the changes between Python 2 and 3 anyway?\r\n* Are migration plans or transition tools available?\r\n* If I want to start learning Python, should I do Python 2 or Python 3?\r\n* Are all Python 2 books obsolete?\r\n\r\nWe will attempt to answer all of these questions and more. With many users and organizations having already migrated, it's time the rest of you Join us too!",
            "abstract_html": "<p>In this talk I will discuss the evolution of Python &amp; answer common FAQs. There are those who worry that Python 3 is backwards-incompatible to Python 2. We address that issue, discuss what the main differences are, talk migration, the roles of 2.6/2.7 &amp; other transition tools.</p>\n<p>Python is currently at a crossroads: Python 2 has taken it from a quiet word-of-mouth language to primetime, with many companies around the world using it and an ever-increasing global marketshare of the programming world. But now comes Python 3, the first version of the language that is not backwards compatible with previous releases. This large change brings up many questions that have become FAQs:</p>\n<ul>\n<li>What does this mean?</li>\n<li>Are all my Python programs going to break?</li>\n<li>Will I have to rewrite everything?</li>\n<li>How much time do I have?</li>\n<li>When is Python 2 going to be EOL'd?</li>\n<li>Is the language undergoing a complete rewrite and will I even recognize it?</li>\n<li>What are the changes between Python 2 and 3 anyway?</li>\n<li>Are migration plans or transition tools available?</li>\n<li>If I want to start learning Python, should I do Python 2 or Python 3?</li>\n<li>Are all Python 2 books obsolete?</li>\n</ul>\n<p>We will attempt to answer all of these questions and more. With many users and organizations having already migrated, it's time the rest of you Join us too!</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T05:28:45.022Z",
            "speaker": 123,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 151,
        "fields": {
            "kind": 1,
            "title": "Automated responses to questions about your health",
            "description": "Have you ever searched online for some healthy issue? With Tensorflow models built from freely available data, and utilizing a domain specific ontology, learn how to bootstrapped a solution for automatically responding to the types questions you might ask on Google but need a more specific medical response.",
            "abstract": "Have you ever searched online for some healthy issue? Chances are you gained little meaningful information from your search and perhaps made your doctor\u2019s life that much harder when you went into their office with your newfound \u201cknowledge\u201d. With Tensorflow models built from freely available data, and utilizing a domain specific ontology, attendees will learn how to bootstrapped a solution for automatically responding to the types questions you might ask on Google but need a more specific medical response.\r\n\r\nUsing the open source Python library Tensorflow, I will go over the steps I took to create a model that generates medical answers to healthcare questions. The talk will discuss how to frame your machine learning project so that your time and efforts are rewarded with valuable insights that you can report to your team. Technical details such as model performance, choice of deep learning library and design as well as the basics of acquiring good data and cleaning it will be covered. Also covered will be healthcare industry specific challenges in natural language processing and what effort is necessary to create a question/answer system that produces meaningful results\r\n\r\nMore specifically it will touch on:\r\n- Entity extraction\r\n- Automated ontology generation\r\n- Building training sets locally and on cloud\r\n- Speaker recognition\r\n- Sentiment analysis\r\n- Speed comparisons between Tensorflow, Keras and PyTorch",
            "abstract_html": "<p>Have you ever searched online for some healthy issue? Chances are you gained little meaningful information from your search and perhaps made your doctor\u2019s life that much harder when you went into their office with your newfound \u201cknowledge\u201d. With Tensorflow models built from freely available data, and utilizing a domain specific ontology, attendees will learn how to bootstrapped a solution for automatically responding to the types questions you might ask on Google but need a more specific medical response.</p>\n<p>Using the open source Python library Tensorflow, I will go over the steps I took to create a model that generates medical answers to healthcare questions. The talk will discuss how to frame your machine learning project so that your time and efforts are rewarded with valuable insights that you can report to your team. Technical details such as model performance, choice of deep learning library and design as well as the basics of acquiring good data and cleaning it will be covered. Also covered will be healthcare industry specific challenges in natural language processing and what effort is necessary to create a question/answer system that produces meaningful results</p>\n<p>More specifically it will touch on:\n- Entity extraction\n- Automated ontology generation\n- Building training sets locally and on cloud\n- Speaker recognition\n- Sentiment analysis\n- Speed comparisons between Tensorflow, Keras and PyTorch</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T05:37:58.029Z",
            "speaker": 124,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 152,
        "fields": {
            "kind": 1,
            "title": "Documentation for Python made simple with Sphinx",
            "description": "Documentation is one of those things no-one likes to write, but it can be the difference-maker on making the project successful. Thankfully, there's tools like Sphinx that can make the documentation process less-painful.",
            "abstract": "Documentation is one of those things no-one likes to write, but it can be the difference-maker on making the project successful. Sphinx is tool that makes it easy to create readable documentation for your projects. This talk session is going to show how to create great documentation using all of the features of Sphinx!",
            "abstract_html": "<p>Documentation is one of those things no-one likes to write, but it can be the difference-maker on making the project successful. Sphinx is tool that makes it easy to create readable documentation for your projects. This talk session is going to show how to create great documentation using all of the features of Sphinx!</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T05:45:47.671Z",
            "speaker": 25,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 153,
        "fields": {
            "kind": 1,
            "title": "Automated responses to questions about your health",
            "description": "Have you ever searched online for some healthy issue? With Tensorflow models built from freely available data, and utilizing a domain specific ontology, learn how to bootstrapped a solution for automatically responding to the types questions you might ask on Google but need a more specific medical response.",
            "abstract": "Have you ever searched online for some healthy issue? Chances are you gained little meaningful information from your search and perhaps made your doctor\u2019s life that much harder when you went into their office with your newfound \u201cknowledge\u201d. With Tensorflow models built from freely available data, and utilizing a domain specific ontology, learn how to bootstrapped a solution for automatically responding to the types questions you might ask on Google but need a more specific medical response.\r\n\r\nUsing the open source Python library Tensorflow, I will go over the steps I took to create a model that generates medical answers to healthcare questions. It will discuss how to frame your machine learning project so that your time and efforts are rewarded with valuable insights that you can report to your team. Technical details such as model performance, choice of deep learning library and design as well as the basics of acquiring good data and cleaning it will be covered. Also covered will be healthcare industry specific challenges in natural language processing and what effort is necessary to create a question/answer system that produces meaningful results\r\n\r\nMore specifically it will touch on:\r\n- Entity extraction\r\n- Automated ontology generation\r\n- Building training sets locally and on cloud\r\n- Speaker recognition\r\n- Sentiment analysis\r\n- Speed comparisons between Tensorflow, Keras and PyTorch",
            "abstract_html": "<p>Have you ever searched online for some healthy issue? Chances are you gained little meaningful information from your search and perhaps made your doctor\u2019s life that much harder when you went into their office with your newfound \u201cknowledge\u201d. With Tensorflow models built from freely available data, and utilizing a domain specific ontology, learn how to bootstrapped a solution for automatically responding to the types questions you might ask on Google but need a more specific medical response.</p>\n<p>Using the open source Python library Tensorflow, I will go over the steps I took to create a model that generates medical answers to healthcare questions. It will discuss how to frame your machine learning project so that your time and efforts are rewarded with valuable insights that you can report to your team. Technical details such as model performance, choice of deep learning library and design as well as the basics of acquiring good data and cleaning it will be covered. Also covered will be healthcare industry specific challenges in natural language processing and what effort is necessary to create a question/answer system that produces meaningful results</p>\n<p>More specifically it will touch on:\n- Entity extraction\n- Automated ontology generation\n- Building training sets locally and on cloud\n- Speaker recognition\n- Sentiment analysis\n- Speed comparisons between Tensorflow, Keras and PyTorch</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T05:49:13.304Z",
            "speaker": 124,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 154,
        "fields": {
            "kind": 1,
            "title": "What do I do when my model doesn't learn?",
            "description": "MORE details TBD. If my model doesn't learn should I add should I add a layer to my neural net or should I be turning up the dropout? Should I be discretizing my input features or calibrating my output predictions?\r\n\r\nIn this talk I will use examples to show how to develop an intuition around common roadblocks and obstacles encountered when building an ML model.",
            "abstract": "TBD",
            "abstract_html": "<p>TBD</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T05:52:10.848Z",
            "speaker": 125,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 155,
        "fields": {
            "kind": 1,
            "title": "How to Easily Configure and Use Logging via Balsa",
            "description": "This talk will describe Python's logging module and 'Balsa', an open source package that does much of the repetitive work required to set up logging.  With just a few lines of code, Balsa provides logging for a variety of Python applications, including command line, GUI, and web apps.",
            "abstract": "While virtually all production apps require some sort of logging, most developers approach it with the level of excitement and enthusiasm usually reserved for flossing their teeth.  The Python logging module is awesome, but after writing many Python applications I realized I was copying and pasting similar logging code for every new project.  To remedy this, I created the Balsa package that contains logging code common across many different kinds of applications.   Balsa works great for virtually any Python program, be it a command line, GUI or web app.  This talk will cover the Python logging module, uses of logging, good log formatting, appropriate routing of log messages to files/cloud/user based on severity, and easily connecting to cloud logging/exception services.",
            "abstract_html": "<p>While virtually all production apps require some sort of logging, most developers approach it with the level of excitement and enthusiasm usually reserved for flossing their teeth.  The Python logging module is awesome, but after writing many Python applications I realized I was copying and pasting similar logging code for every new project.  To remedy this, I created the Balsa package that contains logging code common across many different kinds of applications.   Balsa works great for virtually any Python program, be it a command line, GUI or web app.  This talk will cover the Python logging module, uses of logging, good log formatting, appropriate routing of log messages to files/cloud/user based on severity, and easily connecting to cloud logging/exception services.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T05:57:03.579Z",
            "speaker": 126,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 156,
        "fields": {
            "kind": 1,
            "title": "Ask Alexa: How do I create my first Alexa skill?",
            "description": "Have you ever asked Alexa to play a song, asked Siri for a restaurant recommendation, or asked OK Google for a stock price?  Digital personal assistants are becoming increasingly ubiquitous.  If you\u2019ve ever wondered how you can get started on customizing your digital personal assistant, this talk is for you!  We will walk through how to create a basic Alexa skill, test it, and publish it.",
            "abstract": "We will walk through how to create a basic Alexa skill.  We will walk through the process of using the Alexa Skills Kit to build a voice user interface (VUI) and creating a backend AWS Lambda function in Python.  We will also talk about how to test and publish your skill.",
            "abstract_html": "<p>We will walk through how to create a basic Alexa skill.  We will walk through the process of using the Alexa Skills Kit to build a voice user interface (VUI) and creating a backend AWS Lambda function in Python.  We will also talk about how to test and publish your skill.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T06:00:44.614Z",
            "speaker": 127,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 157,
        "fields": {
            "kind": 1,
            "title": "Debugging in production - stop reproducing bugs, start fixing them.",
            "description": "Given enough context, all bugs are shallow... but how do we preserve context in the face of adversity? This talk will give you a set of tools for failing early, failing rarely, and failing usefully. We'll cover Sentry, remote debugging with PyCharm, and most importantly - setting up your system so that a debugger with a stack trace is always available. You'll never have to reproduce a bug again.",
            "abstract": "This talk is designed give you debugging superpowers.\r\n\r\nHave you ever spent five hours trying to reproduce a bug, only to fix it in five minutes flat? Did you ever wish you could see all the variables in your stack trace? What do you do if your program has so much internal state that it seems impossible to log? Wouldn't it be nice to go back in time, and just start debugging your latest issue on the spot? Can you examine everything that led to an exception under a real debugger at your leisure, while still failing gracefully and immediately?\r\n \r\nThe talk consists of three parts:\r\n- Post-mortem debugging with Sentry.\r\n- Pre-mortem debugging with PyCharm / pydevd, on production, with full context, without stopping the world.\r\n- The wonderful usefulness, the necessary supporting incantations, and the terrible implications of using os.fork() inside a Python program.",
            "abstract_html": "<p>This talk is designed give you debugging superpowers.</p>\n<p>Have you ever spent five hours trying to reproduce a bug, only to fix it in five minutes flat? Did you ever wish you could see all the variables in your stack trace? What do you do if your program has so much internal state that it seems impossible to log? Wouldn't it be nice to go back in time, and just start debugging your latest issue on the spot? Can you examine everything that led to an exception under a real debugger at your leisure, while still failing gracefully and immediately?</p>\n<p>The talk consists of three parts:\n- Post-mortem debugging with Sentry.\n- Pre-mortem debugging with PyCharm / pydevd, on production, with full context, without stopping the world.\n- The wonderful usefulness, the necessary supporting incantations, and the terrible implications of using os.fork() inside a Python program.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T06:37:19.291Z",
            "speaker": 119,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 158,
        "fields": {
            "kind": 1,
            "title": "Observability in a Python World",
            "description": "Observability is at the front of everyone's mind right now, but what's the best way to observe the behavior of Python services? This talk will deep dive into instrumentation and observability of your next Python application",
            "abstract": "Understanding how your application is operating in production is crucial to those supporting it. Having the right observability around the application is something that can be forgotten until it's too late. This talk will look at the best-practices around writing observable code, ensuring that you have useful triage information during incidents and creating sustainable monitoring patterns for your application(s).",
            "abstract_html": "<p>Understanding how your application is operating in production is crucial to those supporting it. Having the right observability around the application is something that can be forgotten until it's too late. This talk will look at the best-practices around writing observable code, ensuring that you have useful triage information during incidents and creating sustainable monitoring patterns for your application(s).</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T06:38:09.379Z",
            "speaker": 25,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 159,
        "fields": {
            "kind": 1,
            "title": "Building for Simplicity",
            "description": "When dealing with ambiguous requirements & external dependencies that are out of your control, it's easy to take the easy road in overengineering a system. But why take time in designing, isn't an ok design good enough? Good enough design though fails when it's inflexible to build upon. In this talk, we'll explore how we can spot an overengineered design, take it apart and build something simple",
            "abstract": "Abstract:\r\n\tBuilding for simplicity is hard. It takes multiple tries to get it right but often times, it pays off to ensure that you have met all of the requirements and that you've increased code maintainability. By building for simplicity, you can achieve \r\n\t   - minimal technical debt \r\n\t   - decreased time in onboarding engineers unfamiliar with the system\r\n\t   - lessened dependencies amongst internal systems\r\n    It's hard to achieve because you have to capture the right abstractions and to do that - you have to ensure that you really know what you need to build. By gathering all of the requirements, you're forced to know what you're building in the first place with no holes for ambiguity. After gathering the functional requirements, you can start designing the higher level abstractions to minimize coupling between internal systems. In this talk - we\u2019ll explore how our team had managed to design a simple notification system that are able to handle asynchronous external events from an initial over engineered design to something simple and elegant that takes a couple of lines of code and drop it on things we want to trigger a notification.",
            "abstract_html": "<p>Abstract:\n    Building for simplicity is hard. It takes multiple tries to get it right but often times, it pays off to ensure that you have met all of the requirements and that you've increased code maintainability. By building for simplicity, you can achieve \n       - minimal technical debt \n       - decreased time in onboarding engineers unfamiliar with the system\n       - lessened dependencies amongst internal systems\n    It's hard to achieve because you have to capture the right abstractions and to do that - you have to ensure that you really know what you need to build. By gathering all of the requirements, you're forced to know what you're building in the first place with no holes for ambiguity. After gathering the functional requirements, you can start designing the higher level abstractions to minimize coupling between internal systems. In this talk - we\u2019ll explore how our team had managed to design a simple notification system that are able to handle asynchronous external events from an initial over engineered design to something simple and elegant that takes a couple of lines of code and drop it on things we want to trigger a notification.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T06:48:23.508Z",
            "speaker": 128,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 160,
        "fields": {
            "kind": 1,
            "title": "From our functional friends",
            "description": "Exploring features of functional programming available in python and bringing in more features and design patterns into python.",
            "abstract": "In their journey through python, functional features like comprehensions, map, filter, become part of every pythonista's toolbox - knowingly or unknowingly. However, python offers more functional features and lacks even more of them. In this talk, you'll learn briefly about the less popular functional features that python provides and how you can improve your functional toolbox by augmenting the default functional toolbox with features like tail recursion as well as by using popular design patterns from functional programming languages like pipelining to improve readability and understanding.",
            "abstract_html": "<p>In their journey through python, functional features like comprehensions, map, filter, become part of every pythonista's toolbox - knowingly or unknowingly. However, python offers more functional features and lacks even more of them. In this talk, you'll learn briefly about the less popular functional features that python provides and how you can improve your functional toolbox by augmenting the default functional toolbox with features like tail recursion as well as by using popular design patterns from functional programming languages like pipelining to improve readability and understanding.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T06:58:07.724Z",
            "speaker": 129,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 161,
        "fields": {
            "kind": 1,
            "title": "Power your apps with Gmail, Drive, Calendar, Sheets, Slides & more",
            "description": "Ever want to integrate Google technologies into your web+mobile apps? Most G Suite applications like Gmail, Google Drive, Calendar, Sheets, and Slides have a developer API. In this session, we explore each G Suite productivity tool & give you an idea of how to use its API. To keep things interesting, we'll even try some live demos so you can integrate leverage the power of G Suite in your own apps",
            "abstract": "This talk covers everything you need to know in order to use Google APIs from Python. We'll start with the Google APIs Client Library, which you can use with *any* of the languages supported, not just Python. We'll discuss how to install the open source Client Library for Python, then describe & show the steps necessary to create new projects & enable Google APIs from the Developers Console. Next we discuss how to create & access the necessary credentials. That's just the setup.\r\n\r\nThe second part of the talk walks you through the necessary boilerplate code that you need to get started. This code will stay fairly consistent for your first, second, or 20th app, and whether you using 1, 2, or 20 Google APIs. We'll conclude demonstrating some of these steps, and time-permitting, walk through some some simple code (in Python) that accesses Google APIs, mainly those found in G Suite.",
            "abstract_html": "<p>This talk covers everything you need to know in order to use Google APIs from Python. We'll start with the Google APIs Client Library, which you can use with <em>any</em> of the languages supported, not just Python. We'll discuss how to install the open source Client Library for Python, then describe &amp; show the steps necessary to create new projects &amp; enable Google APIs from the Developers Console. Next we discuss how to create &amp; access the necessary credentials. That's just the setup.</p>\n<p>The second part of the talk walks you through the necessary boilerplate code that you need to get started. This code will stay fairly consistent for your first, second, or 20th app, and whether you using 1, 2, or 20 Google APIs. We'll conclude demonstrating some of these steps, and time-permitting, walk through some some simple code (in Python) that accesses Google APIs, mainly those found in G Suite.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T07:13:47.871Z",
            "speaker": 123,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 162,
        "fields": {
            "kind": 1,
            "title": "Facial Recognition - Make sure that you are you",
            "description": "MacFace - An app made from Open CV module in Python3 integrated with Apple Script to unlock the computer with Face ID.",
            "abstract": "Python3\u2019s OpenCV is an awesome library of methods that are aimed at real time computer vision. One of its application areas include Facial Recognition System. I used the Facial Recognition system methods along with Apple script to create an application that unlocks your Mac using your password. When you first install the application, it takes training data (pictures) of the person. Then, the  application runs in the background and when you click on the log in icon on the laptop,  the application activates the camera in your Mac, takes a picture and runs it through the script to match the picture with a training data. When the python script returns the confirmed identity, the apple script opens unlocks the laptop with the password that the user provided. \r\nIn this talk, I will take one aspect of the app, which is using the training data which is pictures and the associated name and matching a new picture with the name of the person.  It is an easy implementation of Open CV which will be enjoyed by the audience.",
            "abstract_html": "<p>Python3\u2019s OpenCV is an awesome library of methods that are aimed at real time computer vision. One of its application areas include Facial Recognition System. I used the Facial Recognition system methods along with Apple script to create an application that unlocks your Mac using your password. When you first install the application, it takes training data (pictures) of the person. Then, the  application runs in the background and when you click on the log in icon on the laptop,  the application activates the camera in your Mac, takes a picture and runs it through the script to match the picture with a training data. When the python script returns the confirmed identity, the apple script opens unlocks the laptop with the password that the user provided. \nIn this talk, I will take one aspect of the app, which is using the training data which is pictures and the associated name and matching a new picture with the name of the person.  It is an easy implementation of Open CV which will be enjoyed by the audience.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T09:38:53.163Z",
            "speaker": 130,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 163,
        "fields": {
            "kind": 1,
            "title": "An Import Loop and a Fiery Reentry",
            "description": "The Skyfield astronomy library generates planet and satellite positions\r\nfor Python programmers.  With it, Brandon was able to schedule a final\r\nglimpse of Tiangong-1 hours before it burned up in our planet\u2019s\r\natmosphere.  But building a beautiful API always involves compromises,\r\nand Brandon will discuss the problem that import loops pose for Python\r\nAPIs based on method chains.",
            "abstract": "I will start by describing how a library can leverage NumPy and the\r\nJupyter Notebook to avoid reinventing wheels as it brings numerical\r\ncalculations and powerful visualizations to Python programmers.  Using\r\nmy astronomy library Skyfield as an example, I\u2019ll show how I was able to\r\npredict the final few passages of space station Tiangong-1 over my house\r\nbefore its fiery plunge into Earth\u2019s atmosphere in early April.\r\n\r\nBut Python is not a language that\u2019s in every way perfect.  I\u2019ll examine\r\none particular style of API that I used when building Skyfield: the\r\nmethod chain, where objects suggest to the user which operations are\r\npossible next by which methods they support.  This often requires\r\nobjects to be able to instantiate other objects \u2014 in the case of\r\nSkyfield, creating an import loop once the collection of features became\r\nrich enough.  I\u2019ll look at the options for resolving import loops and\r\nwhat the solution does to the shape of a library like Skyfield.",
            "abstract_html": "<p>I will start by describing how a library can leverage NumPy and the\nJupyter Notebook to avoid reinventing wheels as it brings numerical\ncalculations and powerful visualizations to Python programmers.  Using\nmy astronomy library Skyfield as an example, I\u2019ll show how I was able to\npredict the final few passages of space station Tiangong-1 over my house\nbefore its fiery plunge into Earth\u2019s atmosphere in early April.</p>\n<p>But Python is not a language that\u2019s in every way perfect.  I\u2019ll examine\none particular style of API that I used when building Skyfield: the\nmethod chain, where objects suggest to the user which operations are\npossible next by which methods they support.  This often requires\nobjects to be able to instantiate other objects \u2014 in the case of\nSkyfield, creating an import loop once the collection of features became\nrich enough.  I\u2019ll look at the options for resolving import loops and\nwhat the solution does to the shape of a library like Skyfield.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-22T11:52:41.259Z",
            "speaker": 131,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 164,
        "fields": {
            "kind": 1,
            "title": "ipsum",
            "description": "foo",
            "abstract": "foobar",
            "abstract_html": "<p>foobar</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-23T02:46:33.254Z",
            "speaker": 132,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 165,
        "fields": {
            "kind": 1,
            "title": "State Machines are for Humans",
            "description": "This talk aims to explore how designing our software in this way helps us to write code that is easier to understand and less likely to produce errors at runtime.\r\n\r\nIn particular, we're going to study three aspects of state machines that make them valuable tools for us as engineers:\r\n\r\n1. Readability\r\n2. Testability\r\n3. Visualization (of logic/control flow)",
            "abstract": "This talk is going to start by spending some time at the beginning talking a little about the formal definition of a state machine and how a problem solved using a nested if/elif block compares to a solution that uses a state machine.\r\n\r\nAfterwards, there will be a demo that shows how to use the aptly named [transitions library](https://github.com/pytransitions/transitions) to write state machines.\r\n\r\nThis demo will show us a concrete example of what a state machine implementation looks like in Python and how it helps us to better understand and test our code.",
            "abstract_html": "<p>This talk is going to start by spending some time at the beginning talking a little about the formal definition of a state machine and how a problem solved using a nested if/elif block compares to a solution that uses a state machine.</p>\n<p>Afterwards, there will be a demo that shows how to use the aptly named <a href=\"https://github.com/pytransitions/transitions\">transitions library</a> to write state machines.</p>\n<p>This demo will show us a concrete example of what a state machine implementation looks like in Python and how it helps us to better understand and test our code.</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-23T04:30:58.586Z",
            "speaker": 133,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 166,
        "fields": {
            "kind": 1,
            "title": "How to learn Python (or any other language) as a mid-career professional adult",
            "description": "There are many resources to teach yourself Python fundamentals, but how do you go beyond the basics and dig deeper into more advanced concepts and language internals? I will offer opinionated advice drawn from experience as a self-taught Python programmer who started to code 4 years ago and have been teaching Python for the past 2 years to other adult learners.",
            "abstract": "Many new programmers learn Python through building simple applications by following guidelines, but Python is not only a tool for building but also a laboratory for exploration. I propose that a way to accelerate learning is to probe the edges of the language and attempt perhaps unintended uses. I will offer several case studies to illustrate this approach:\r\n\r\n* making a list that contains itself to clarify Python's memory model\r\n\r\n* changing an instance's type to show how classes are dynamic data structures\r\n\r\n* taking a function and repurposing it to belong to another module to see how scopes are implemented\r\n\r\n* an example from C: how to make the \"main\" function recursive to convince yourself that it is in fact like any other function",
            "abstract_html": "<p>Many new programmers learn Python through building simple applications by following guidelines, but Python is not only a tool for building but also a laboratory for exploration. I propose that a way to accelerate learning is to probe the edges of the language and attempt perhaps unintended uses. I will offer several case studies to illustrate this approach:</p>\n<ul>\n<li>\n<p>making a list that contains itself to clarify Python's memory model</p>\n</li>\n<li>\n<p>changing an instance's type to show how classes are dynamic data structures</p>\n</li>\n<li>\n<p>taking a function and repurposing it to belong to another module to see how scopes are implemented</p>\n</li>\n<li>\n<p>an example from C: how to make the \"main\" function recursive to convince yourself that it is in fact like any other function</p>\n</li>\n</ul>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-24T05:07:31.675Z",
            "speaker": 134,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 167,
        "fields": {
            "kind": 1,
            "title": "Good Old C: A Hard Language For Easy Wins",
            "description": "A year ago, we decided to tackle the hotspots in Yelp\u2019s codebase by rewriting them in good old C, saving ourselves a couple hundred thousand dollars a year with just a few days of engineering. This talk is about how we supercharged our entire infrastructure with C, Cython, and a bunch of other things that start with C - you\u2019ll see.",
            "abstract": "Python is nice, but when you find yourself calling the same function thousands of times at every second, you sometimes wish you could have something better. What about our good old friend C? At Yelp, we used to spend hundreds of milliseconds at each request just doing cryptographic operations. So we rewrote the interface to OpenSSL in pure C and sped up everything 20x.\r\n\r\nThen we realized that another of our internal packages was eating up a lot of time. So we put it through Cython, just like that, and cut its execution time in half. This set us on a mission to find all of the hotspots and the easy wins around Yelp\u2019s codebase. Now, the running joke around the Performance team has become: \u201cThis is slow? Rewrite it in C!\u201d",
            "abstract_html": "<p>Python is nice, but when you find yourself calling the same function thousands of times at every second, you sometimes wish you could have something better. What about our good old friend C? At Yelp, we used to spend hundreds of milliseconds at each request just doing cryptographic operations. So we rewrote the interface to OpenSSL in pure C and sped up everything 20x.</p>\n<p>Then we realized that another of our internal packages was eating up a lot of time. So we put it through Cython, just like that, and cut its execution time in half. This set us on a mission to find all of the hotspots and the easy wins around Yelp\u2019s codebase. Now, the running joke around the Performance team has become: \u201cThis is slow? Rewrite it in C!\u201d</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-25T21:36:56.654Z",
            "speaker": 135,
            "cancelled": false
        }
    },
    {
        "model": "symposion_proposals.proposalbase",
        "pk": 168,
        "fields": {
            "kind": 1,
            "title": "From Batching to Streaming: A challenging migration tale",
            "description": "Making decisions around data infrastructure investments are never easy. We faced similar challenges at Yelp when we decided to replace our batch ETL system with our streaming Data Pipeline. In this session, we\u2019ll discuss our decision-making process and share lessons learned along the way so that you can apply them when confronted with similar challenges.",
            "abstract": "In 2011, we wrote an extract-transform-load system (ETL) to move data to our then newly created Data Warehouse. The system worked very well until about 2015 when it failed to handle our rapidly increasing datasets and our growing business needs. We realized that our data infrastructure needed to change -- and fast. This talk will walk you through the challenges we faced and provide a retrospective take on the lessons we learned along the way, covering questions like:\r\nAt what point should you seriously start thinking about the architecture of your data infrastructure?\r\nHow do you decide which technology investments are the right investments for you?\r\nHow do you get organizational buy-in on your new approach?\r\nTo open source or not to open source?\r\nHow do you balance rolling out the new system, while continuing to manage the old system?",
            "abstract_html": "<p>In 2011, we wrote an extract-transform-load system (ETL) to move data to our then newly created Data Warehouse. The system worked very well until about 2015 when it failed to handle our rapidly increasing datasets and our growing business needs. We realized that our data infrastructure needed to change -- and fast. This talk will walk you through the challenges we faced and provide a retrospective take on the lessons we learned along the way, covering questions like:\nAt what point should you seriously start thinking about the architecture of your data infrastructure?\nHow do you decide which technology investments are the right investments for you?\nHow do you get organizational buy-in on your new approach?\nTo open source or not to open source?\nHow do you balance rolling out the new system, while continuing to manage the old system?</p>",
            "additional_notes": "",
            "additional_notes_html": "",
            "submitted": "2018-04-27T01:01:51.906Z",
            "speaker": 136,
            "cancelled": false
        }
    }
]
